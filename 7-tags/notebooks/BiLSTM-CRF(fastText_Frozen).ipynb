{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-27T14:44:02.920722Z",
     "iopub.status.busy": "2025-01-27T14:44:02.920396Z",
     "iopub.status.idle": "2025-01-27T14:44:09.500201Z",
     "shell.execute_reply": "2025-01-27T14:44:09.499414Z",
     "shell.execute_reply.started": "2025-01-27T14:44:02.920698Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/kmkurn/pytorch-crf.git\n",
      "  Cloning https://github.com/kmkurn/pytorch-crf.git to /tmp/pip-req-build-n3lnukj1\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/kmkurn/pytorch-crf.git /tmp/pip-req-build-n3lnukj1\n",
      "  Resolved https://github.com/kmkurn/pytorch-crf.git to commit 623e3402d00a2728e99d6e8486010d67c754267b\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: pytorch-crf\n",
      "  Building wheel for pytorch-crf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pytorch-crf: filename=pytorch_crf-0.7.2-py3-none-any.whl size=6410 sha256=1c34c717aaeda2f41e418db9966a6e95725928e6ed0748797631650c4537fd43\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-dhxzrify/wheels/39/5f/f6/4b48b35895d914f4f5fff5b600f87658c11693e37b6a4f118e\n",
      "Successfully built pytorch-crf\n",
      "Installing collected packages: pytorch-crf\n",
      "Successfully installed pytorch-crf-0.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/kmkurn/pytorch-crf.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T13:38:14.556038Z",
     "iopub.status.busy": "2025-01-27T13:38:14.555689Z",
     "iopub.status.idle": "2025-01-27T13:38:14.559931Z",
     "shell.execute_reply": "2025-01-27T13:38:14.559038Z",
     "shell.execute_reply.started": "2025-01-27T13:38:14.556004Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Experiment: Named Entity Recognition (NER) using BiLSTM-CRF with FastText Embeddings(Frozen)\n",
    "# Configuration Settings:\n",
    "# - Embedding Dimension: 300 (FastText)\n",
    "# - Hidden Dimension: 256/128 (BiLSTM)\n",
    "# - Batch Size: 64/32\n",
    "# - Model: BiLSTM-CRF (Single NER MOdel/Joint Model for POS and NER)\n",
    "# - Optimizer: Adam with L2 regularization (weight_decay=1e-5)\n",
    "# - Learning Rate: 0.001\n",
    "# - Dropout: 0.5\n",
    "# - Early Stopping: Patience of 3 epochs\n",
    "# - Training Epochs: up to 30\n",
    "# - Dataset: Pre-split CoNLL format (train_v5.conll, val_v5.conll, test_v5.conll)\n",
    "# - FastText Model: cc.my.300.bin (pre-trained FastText embeddings)\n",
    "# - Evaluation Metrics: Precision, Recall, F1-Score for both POS and NER tasks\n",
    "# - Hardware: NVIDIA Tesla T4 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T14:45:02.080140Z",
     "iopub.status.busy": "2025-01-27T14:45:02.079819Z",
     "iopub.status.idle": "2025-01-27T14:45:19.108640Z",
     "shell.execute_reply": "2025-01-27T14:45:19.107975Z",
     "shell.execute_reply.started": "2025-01-27T14:45:02.080113Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchcrf import CRF\n",
    "from sklearn.metrics import classification_report\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T13:38:25.324636Z",
     "iopub.status.busy": "2025-01-27T13:38:25.323265Z",
     "iopub.status.idle": "2025-01-27T13:45:01.888982Z",
     "shell.execute_reply": "2025-01-27T13:45:01.888039Z",
     "shell.execute_reply.started": "2025-01-27T13:38:25.324608Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 341179 words from FastText binary model.\n",
      "Epoch 1/30: Train Loss = 128557.3785, Val Loss = 6385.1956\n",
      "Epoch 2/30: Train Loss = 46451.4199, Val Loss = 5261.7667\n",
      "Epoch 3/30: Train Loss = 36191.9777, Val Loss = 3552.7402\n",
      "Epoch 4/30: Train Loss = 31046.8064, Val Loss = 3619.1624\n",
      "Epoch 5/30: Train Loss = 27484.7393, Val Loss = 2780.2071\n",
      "Epoch 6/30: Train Loss = 25561.2284, Val Loss = 2807.4914\n",
      "Epoch 7/30: Train Loss = 23871.3128, Val Loss = 2489.1845\n",
      "Epoch 8/30: Train Loss = 21958.7037, Val Loss = 2266.7023\n",
      "Epoch 9/30: Train Loss = 20439.4832, Val Loss = 2206.0556\n",
      "Epoch 10/30: Train Loss = 19394.0990, Val Loss = 2064.2375\n",
      "Epoch 11/30: Train Loss = 18439.5933, Val Loss = 2156.3890\n",
      "Epoch 12/30: Train Loss = 17787.5161, Val Loss = 1975.4298\n",
      "Epoch 13/30: Train Loss = 17017.7447, Val Loss = 1882.4904\n",
      "Epoch 14/30: Train Loss = 16050.4395, Val Loss = 1845.0240\n",
      "Epoch 15/30: Train Loss = 15350.2478, Val Loss = 1766.6904\n",
      "Epoch 16/30: Train Loss = 15010.3573, Val Loss = 1788.3697\n",
      "Epoch 17/30: Train Loss = 14590.0048, Val Loss = 1791.2532\n",
      "Epoch 18/30: Train Loss = 13727.5885, Val Loss = 1690.2072\n",
      "Epoch 19/30: Train Loss = 13688.5619, Val Loss = 1623.8210\n",
      "Epoch 20/30: Train Loss = 13542.6924, Val Loss = 1674.3200\n",
      "Epoch 21/30: Train Loss = 12790.0585, Val Loss = 1657.9308\n",
      "Epoch 22/30: Train Loss = 12460.6162, Val Loss = 1611.2504\n",
      "Epoch 23/30: Train Loss = 11947.2550, Val Loss = 1582.7485\n",
      "Epoch 24/30: Train Loss = 11717.2881, Val Loss = 1548.2051\n",
      "Epoch 25/30: Train Loss = 11171.2739, Val Loss = 1517.2975\n",
      "Epoch 26/30: Train Loss = 10969.0784, Val Loss = 1582.3544\n",
      "Epoch 27/30: Train Loss = 10714.1414, Val Loss = 1489.6940\n",
      "Epoch 28/30: Train Loss = 11202.9367, Val Loss = 1526.7231\n",
      "Epoch 29/30: Train Loss = 10450.8538, Val Loss = 1508.3701\n",
      "Epoch 30/30: Train Loss = 9940.3196, Val Loss = 1605.7012\n",
      "Early stopping triggered!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchcrf/__init__.py:308: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:530.)\n",
      "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-DATE     0.7887    0.8485    0.8175        66\n",
      "       B-LOC     0.9793    0.9619    0.9706      1182\n",
      "       B-NUM     1.0000    0.1333    0.2353        15\n",
      "       B-ORG     0.5000    0.2917    0.3684        48\n",
      "       B-PER     0.8182    0.7941    0.8060        34\n",
      "      B-TIME     0.8889    0.8889    0.8889         9\n",
      "      E-DATE     0.7606    0.8182    0.7883        66\n",
      "       E-LOC     0.9793    0.9619    0.9706      1182\n",
      "       E-NUM     1.0000    0.1333    0.2353        15\n",
      "       E-ORG     0.5000    0.2917    0.3684        48\n",
      "       E-PER     0.8182    0.7941    0.8060        34\n",
      "      E-TIME     0.8889    0.8889    0.8889         9\n",
      "      I-DATE     0.7037    0.5000    0.5846        38\n",
      "       I-LOC     0.9918    0.9642    0.9778       503\n",
      "       I-ORG     0.3667    0.2821    0.3188        39\n",
      "           O     0.9836    0.9914    0.9875     21324\n",
      "      S-DATE     1.0000    0.8523    0.9202        88\n",
      "       S-LOC     0.7241    0.3387    0.4615       124\n",
      "       S-NUM     0.9488    0.9747    0.9616       475\n",
      "       S-ORG     0.0000    0.0000    0.0000        21\n",
      "       S-PER     0.6519    0.7892    0.7140       223\n",
      "      S-TIME     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.9747     25543\n",
      "   macro avg     0.7406    0.6136    0.6396     25543\n",
      "weighted avg     0.9730    0.9747    0.9729     25543\n",
      "\n",
      "\n",
      "Sample 583:\n",
      "Sentence:     ကြယ်စင်ကျော် သည် ငှက် ကြည့် ခြင်း နှင့် ရှားပါး မျိုးစိတ် များ ကို ဓာတ်ပုံ ရိုက် ခြင်း ကို နှစ်သက် သည် ။\n",
      "True NER:     S-PER O O O O O O O O O O O O O O O O\n",
      "Predicted NER: S-PER O O O O O O O O O O O O O O O O\n",
      "\n",
      "Sample 1267:\n",
      "Sentence:     ဒီ က နေ ငါး ဒေါ်လာ ယူ လိုက် ပါ ။\n",
      "True NER:     O O O O O O O O O\n",
      "Predicted NER: O O O O O O O O O\n",
      "\n",
      "Sample 1458:\n",
      "Sentence:     ဆံပင် ကောက် သည် ။\n",
      "True NER:     O O O O\n",
      "Predicted NER: O O O O\n",
      "\n",
      "Sample 1200:\n",
      "Sentence:     ခရီးပန်းတိုင် သို့ ဆိုက်ရောက် သည် ။\n",
      "True NER:     O O O O O\n",
      "Predicted NER: O O O O O\n",
      "\n",
      "Sample 1333:\n",
      "Sentence:     စောင့် ပါ ဦး ။\n",
      "True NER:     O O O O\n",
      "Predicted NER: O O O O\n"
     ]
    }
   ],
   "source": [
    "#embedding300/hiddendim256/batch64(Single NER Model)\n",
    "# Load FastText binary model\n",
    "fasttext_bin_file = \"/kaggle/input/glove-100d/cc.my.300.bin\"  \n",
    "fasttext_model = load_facebook_model(fasttext_bin_file)\n",
    "\n",
    "# Extract the word vectors\n",
    "fasttext_vectors = fasttext_model.wv\n",
    "print(f\"Loaded {len(fasttext_vectors)} words from FastText binary model.\")\n",
    "\n",
    "# Define Dataset Class (NER-only)\n",
    "class CoNLLDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.sentences, self.ner_tags = self.load_data(file_path)\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        sentences, ner_tags = [], []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            sentence, ner_tag = [], []\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    word, _, ner = line.strip().split(\"\\t\")  # Ignore POS tags\n",
    "                    sentence.append(word)\n",
    "                    ner_tag.append(ner)\n",
    "                else:\n",
    "                    if sentence:\n",
    "                        sentences.append(sentence)\n",
    "                        ner_tags.append(ner_tag)\n",
    "                    sentence, ner_tag = [], []\n",
    "            if sentence:\n",
    "                sentences.append(sentence)\n",
    "                ner_tags.append(ner_tag)\n",
    "        return sentences, ner_tags\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.ner_tags[idx]\n",
    "\n",
    "# Collate function for dynamic padding (NER-only)\n",
    "def collate_fn(batch):\n",
    "    sentences, ner_tags = zip(*batch)\n",
    "    max_len = max(len(s) for s in sentences)\n",
    "\n",
    "    sentence_tensors = []\n",
    "    ner_tensors = []\n",
    "\n",
    "    for s, n in zip(sentences, ner_tags):\n",
    "        padded_sentence = s + [\"<PAD>\"] * (max_len - len(s))\n",
    "        padded_ner = n + [\"<PAD>\"] * (max_len - len(n))\n",
    "\n",
    "        sentence_tensors.append(torch.tensor([vocab.get(word, vocab[\"<UNK>\"]) for word in padded_sentence], dtype=torch.long))\n",
    "        ner_tensors.append(torch.tensor([ner_tag_to_ix[tag] for tag in padded_ner], dtype=torch.long))\n",
    "\n",
    "    return torch.stack(sentence_tensors), torch.stack(ner_tensors)\n",
    "\n",
    "# Define BiLSTM-CRF Model for NER-only\n",
    "class BiLSTMCRF_NER(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_ner_tags, fasttext_embeddings):\n",
    "        super(BiLSTMCRF_NER, self).__init__()\n",
    "        # Initialize embedding layer with FastText embeddings (frozen)\n",
    "        self.embedding = nn.Embedding.from_pretrained(fasttext_embeddings, freeze=True)  # Freeze embeddings\n",
    "        self.dropout = nn.Dropout(0.5)  # Add dropout\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.ner_fc = nn.Linear(hidden_dim * 2, num_ner_tags)\n",
    "        self.ner_crf = CRF(num_ner_tags, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x)\n",
    "        embeddings = self.dropout(embeddings)  # Apply dropout\n",
    "        lstm_out, _ = self.bilstm(embeddings)\n",
    "        lstm_out = self.dropout(lstm_out)  # Apply dropout\n",
    "        ner_logits = self.ner_fc(lstm_out)\n",
    "        return ner_logits\n",
    "\n",
    "    def compute_loss(self, x, ner_tags):\n",
    "        ner_logits = self.forward(x)\n",
    "        ner_loss = -self.ner_crf(ner_logits, ner_tags, mask=(x != vocab[\"<PAD>\"]))\n",
    "        return ner_loss\n",
    "\n",
    "    def decode(self, x):\n",
    "        ner_logits = self.forward(x)\n",
    "        ner_tags = self.ner_crf.decode(ner_logits)\n",
    "        return ner_tags\n",
    "\n",
    "# Paths to pre-split datasets\n",
    "train_file_path = \"/kaggle/input/split-fix-data/train_v5.conll\"\n",
    "val_file_path = \"/kaggle/input/split-fix-data/val_v5.conll\"\n",
    "test_file_path = \"/kaggle/input/split-fix-data/test_v5.conll\"\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = CoNLLDataset(train_file_path)\n",
    "val_dataset = CoNLLDataset(val_file_path)\n",
    "test_dataset = CoNLLDataset(test_file_path)\n",
    "\n",
    "# Create vocabulary and tag-to-index mappings\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "ner_tag_to_ix = {\"<PAD>\": 0}  # Add <PAD> to NER tags\n",
    "\n",
    "# Build vocab and tag mappings\n",
    "for dataset in [train_dataset, val_dataset, test_dataset]:\n",
    "    for sentence, ner_tags in zip(dataset.sentences, dataset.ner_tags):\n",
    "        for word in sentence:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "        for ner_tag in ner_tags:\n",
    "            if ner_tag not in ner_tag_to_ix:\n",
    "                ner_tag_to_ix[ner_tag] = len(ner_tag_to_ix)\n",
    "\n",
    "# Create embedding matrix using FastText (dimension: 300)\n",
    "embedding_dim = 300  # FastText embedding dimension\n",
    "fasttext_embeddings = torch.zeros((len(vocab), embedding_dim))  # Initialize with zeros\n",
    "\n",
    "for word, idx in vocab.items():\n",
    "    if word in fasttext_vectors:\n",
    "        fasttext_embeddings[idx] = torch.tensor(fasttext_vectors[word])  # Use full 300 dimensions\n",
    "    elif word == \"<PAD>\":\n",
    "        fasttext_embeddings[idx] = torch.zeros(embedding_dim)  # Zero vector for padding\n",
    "    else:\n",
    "        fasttext_embeddings[idx] = torch.randn(embedding_dim)  # Random vector for unknown words\n",
    "\n",
    "# Initialize model\n",
    "hidden_dim = 256\n",
    "vocab_size = len(vocab)\n",
    "num_ner_tags = len(ner_tag_to_ix)\n",
    "\n",
    "model = BiLSTMCRF_NER(vocab_size, embedding_dim, hidden_dim, num_ner_tags, fasttext_embeddings).to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # Add weight decay for L2 regularization\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Training loop with early stopping\n",
    "def train_model(model, train_loader, val_loader, test_loader, epochs):\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 3\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for sentences, ner_tags in train_loader:\n",
    "            sentences, ner_tags = sentences.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.compute_loss(sentences, ner_tags)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for sentences, ner_tags in val_loader:\n",
    "                sentences, ner_tags = sentences.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "                val_loss += model.compute_loss(sentences, ner_tags).item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    # Evaluate on test set after training\n",
    "    model.eval()\n",
    "    all_ner_preds, all_ner_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for sentences, ner_tags in test_loader:\n",
    "            sentences, ner_tags = sentences.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "            ner_preds = model.decode(sentences)\n",
    "\n",
    "            # Flatten the predictions and targets, excluding <PAD> tokens\n",
    "            for i in range(len(sentences)):\n",
    "                sentence_length = (sentences[i] != vocab[\"<PAD>\"]).sum().item()  # Length of the actual sentence\n",
    "                all_ner_preds.extend(ner_preds[i][:sentence_length])  # Truncate predictions to sentence length\n",
    "                all_ner_targets.extend(ner_tags[i][:sentence_length].cpu().numpy())  # Truncate targets to sentence length\n",
    "\n",
    "    # Convert predictions and targets to tag names\n",
    "    idx_to_ner = {v: k for k, v in ner_tag_to_ix.items()}\n",
    "\n",
    "    # Filter out padding tokens from predictions and targets\n",
    "    all_ner_preds_filtered = [idx_to_ner[idx] for idx in all_ner_preds]\n",
    "    all_ner_targets_filtered = [idx_to_ner[idx] for idx in all_ner_targets]\n",
    "\n",
    "    # Generate classification report\n",
    "    print(\"NER Classification Report:\")\n",
    "    print(classification_report(all_ner_targets_filtered, all_ner_preds_filtered, zero_division=0, digits=4))\n",
    "\n",
    "# Function to display random sentences with true and predicted NER tags\n",
    "def display_random_samples(model, test_loader, vocab, ner_tag_to_ix, num_samples=5):\n",
    "    model.eval()\n",
    "    idx_to_ner = {v: k for k, v in ner_tag_to_ix.items()}\n",
    "    vocab_inv = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    # Collect all sentences and their true/predicted tags\n",
    "    all_sentences = []\n",
    "    all_true_ner = []\n",
    "    all_pred_ner = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sentences, ner_tags in test_loader:\n",
    "            sentences, ner_tags = sentences.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "            ner_preds = model.decode(sentences)\n",
    "\n",
    "            # Convert indices to words and tags\n",
    "            for i in range(len(sentences)):\n",
    "                sentence_length = (sentences[i] != vocab[\"<PAD>\"]).sum().item()  # Length of the actual sentence\n",
    "                words = [vocab_inv[idx.item()] for idx in sentences[i][:sentence_length]]\n",
    "                true_ner = [idx_to_ner[idx.item()] for idx in ner_tags[i][:sentence_length]]\n",
    "                pred_ner = [idx_to_ner[idx] for idx in ner_preds[i][:sentence_length]]\n",
    "\n",
    "                all_sentences.append(words)\n",
    "                all_true_ner.append(true_ner)\n",
    "                all_pred_ner.append(pred_ner)\n",
    "\n",
    "    # Randomly select `num_samples` sentences\n",
    "    random_indices = random.sample(range(len(all_sentences)), num_samples)\n",
    "    for idx in random_indices:\n",
    "        print(f\"\\nSample {idx + 1}:\")\n",
    "        print(\"Sentence:    \", \" \".join(all_sentences[idx]))\n",
    "        print(\"True NER:    \", \" \".join(all_true_ner[idx]))\n",
    "        print(\"Predicted NER:\", \" \".join(all_pred_ner[idx]))\n",
    "\n",
    "# Train the model and evaluate on the test set\n",
    "train_model(model, train_loader, val_loader, test_loader, epochs=30)\n",
    "\n",
    "# Display 5 random samples from the test set\n",
    "display_random_samples(model, test_loader, vocab, ner_tag_to_ix, num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T13:45:01.891355Z",
     "iopub.status.busy": "2025-01-27T13:45:01.890777Z",
     "iopub.status.idle": "2025-01-27T13:53:49.934244Z",
     "shell.execute_reply": "2025-01-27T13:53:49.933223Z",
     "shell.execute_reply.started": "2025-01-27T13:45:01.891327Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 341179 words from FastText binary model.\n",
      "Epoch 1/30: Train Loss = 113844.7515, Val Loss = 5773.6969\n",
      "Epoch 2/30: Train Loss = 43958.6649, Val Loss = 4143.4077\n",
      "Epoch 3/30: Train Loss = 34416.4559, Val Loss = 3327.3332\n",
      "Epoch 4/30: Train Loss = 29143.5760, Val Loss = 2860.9609\n",
      "Epoch 5/30: Train Loss = 25652.2860, Val Loss = 2532.6725\n",
      "Epoch 6/30: Train Loss = 23208.4379, Val Loss = 2275.7706\n",
      "Epoch 7/30: Train Loss = 21526.7472, Val Loss = 2125.9115\n",
      "Epoch 8/30: Train Loss = 19911.2338, Val Loss = 2011.8973\n",
      "Epoch 9/30: Train Loss = 18970.7070, Val Loss = 1924.4657\n",
      "Epoch 10/30: Train Loss = 17862.6278, Val Loss = 1807.8838\n",
      "Epoch 11/30: Train Loss = 17252.2050, Val Loss = 1774.6386\n",
      "Epoch 12/30: Train Loss = 16479.5349, Val Loss = 1698.1722\n",
      "Epoch 13/30: Train Loss = 15917.6387, Val Loss = 1700.0676\n",
      "Epoch 14/30: Train Loss = 14929.2227, Val Loss = 1634.9834\n",
      "Epoch 15/30: Train Loss = 14547.7274, Val Loss = 1637.9751\n",
      "Epoch 16/30: Train Loss = 14084.8317, Val Loss = 1565.4626\n",
      "Epoch 17/30: Train Loss = 13484.6046, Val Loss = 1531.0999\n",
      "Epoch 18/30: Train Loss = 13250.0399, Val Loss = 1503.2489\n",
      "Epoch 19/30: Train Loss = 12919.6169, Val Loss = 1471.2712\n",
      "Epoch 20/30: Train Loss = 12489.6892, Val Loss = 1520.3240\n",
      "Epoch 21/30: Train Loss = 12200.5028, Val Loss = 1413.9039\n",
      "Epoch 22/30: Train Loss = 11972.5589, Val Loss = 1461.6730\n",
      "Epoch 23/30: Train Loss = 11977.1302, Val Loss = 1414.1288\n",
      "Epoch 24/30: Train Loss = 11489.9530, Val Loss = 1410.8982\n",
      "Epoch 25/30: Train Loss = 11095.2669, Val Loss = 1469.2665\n",
      "Epoch 26/30: Train Loss = 10936.7360, Val Loss = 1425.9678\n",
      "Epoch 27/30: Train Loss = 10554.9572, Val Loss = 1431.6359\n",
      "Early stopping triggered!\n",
      "NER Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-DATE     0.8281    0.8030    0.8154        66\n",
      "       B-LOC     0.9786    0.9653    0.9719      1182\n",
      "       B-NUM     1.0000    0.2667    0.4211        15\n",
      "       B-ORG     0.5769    0.3125    0.4054        48\n",
      "       B-PER     0.8182    0.7941    0.8060        34\n",
      "      B-TIME     0.8889    0.8889    0.8889         9\n",
      "      E-DATE     0.8281    0.8030    0.8154        66\n",
      "       E-LOC     0.9777    0.9653    0.9715      1182\n",
      "       E-NUM     1.0000    0.2667    0.4211        15\n",
      "       E-ORG     0.5000    0.2708    0.3514        48\n",
      "       E-PER     0.8182    0.7941    0.8060        34\n",
      "      E-TIME     0.8889    0.8889    0.8889         9\n",
      "      I-DATE     0.7436    0.7632    0.7532        38\n",
      "       I-LOC     0.9878    0.9662    0.9769       503\n",
      "       I-ORG     0.4000    0.1538    0.2222        39\n",
      "           O     0.9819    0.9941    0.9880     21324\n",
      "      S-DATE     0.9868    0.8523    0.9146        88\n",
      "       S-LOC     0.6154    0.3226    0.4233       124\n",
      "       S-NUM     0.9505    0.9705    0.9604       475\n",
      "       S-ORG     0.0000    0.0000    0.0000        21\n",
      "       S-PER     0.8294    0.6323    0.7176       223\n",
      "      S-TIME     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.9760     25543\n",
      "   macro avg     0.7545    0.6216    0.6600     25543\n",
      "weighted avg     0.9730    0.9760    0.9736     25543\n",
      "\n",
      "\n",
      "Sample 1170:\n",
      "Sentence:     ရှေ့နေ ဖြစ် ရန် ရည်ရွယ် ၍ လေ့လာ သည် ။\n",
      "True NER:     O O O O O O O O\n",
      "Predicted NER: O O O O O O O O\n",
      "\n",
      "Sample 550:\n",
      "Sentence:     ရွာ နေရာ ကုတ် မှာ ၂၁၆၉၅၅ ဖြစ် သည် ။\n",
      "True NER:     O O O O S-NUM O O O\n",
      "Predicted NER: O O O O S-NUM O O O\n",
      "\n",
      "Sample 646:\n",
      "Sentence:     ရွာ နေရာ ကုတ် မှာ ၂၁၉၇၂၇ ဖြစ် သည် ။\n",
      "True NER:     O O O O S-NUM O O O\n",
      "Predicted NER: O O O O S-NUM O O O\n",
      "\n",
      "Sample 885:\n",
      "Sentence:     ထို ဘုရင် သည် သူတစ်ပါး နှင့် မ တူ ဘဲ ပုံမှန် မျက်လုံး နှစ် လုံး အပြင် နဖူး အလယ် တည့်တည့် တွင် မျက်လုံး အပို တစ် လုံး ရှိ ပါ သည် ။\n",
      "True NER:     O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Predicted NER: O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "\n",
      "Sample 933:\n",
      "Sentence:     မင်း ပါးစပ် ဟ လိုက် တာ နဲ့ ၊ မင်း က ကျွမ်းကျင် တဲ့ လူ တစ် ယောက် ဆို တာ သိ လိုက် ပါ တယ် ။\n",
      "True NER:     O O O O O O O O O O O O O O O O O O O O O\n",
      "Predicted NER: O O O O O O O O O O O O O O O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "#embedding300/hiddendim128/batch32 (Single NER)\n",
    "# Load FastText binary model\n",
    "fasttext_bin_file = \"/kaggle/input/glove-100d/cc.my.300.bin\"  # Replace with your .bin file path\n",
    "fasttext_model = load_facebook_model(fasttext_bin_file)\n",
    "\n",
    "# Extract the word vectors\n",
    "fasttext_vectors = fasttext_model.wv\n",
    "print(f\"Loaded {len(fasttext_vectors)} words from FastText binary model.\")\n",
    "\n",
    "# Define Dataset Class (NER-only)\n",
    "class CoNLLDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.sentences, self.ner_tags = self.load_data(file_path)\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        sentences, ner_tags = [], []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            sentence, ner_tag = [], []\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    word, _, ner = line.strip().split(\"\\t\")  # Ignore POS tags\n",
    "                    sentence.append(word)\n",
    "                    ner_tag.append(ner)\n",
    "                else:\n",
    "                    if sentence:\n",
    "                        sentences.append(sentence)\n",
    "                        ner_tags.append(ner_tag)\n",
    "                    sentence, ner_tag = [], []\n",
    "            if sentence:\n",
    "                sentences.append(sentence)\n",
    "                ner_tags.append(ner_tag)\n",
    "        return sentences, ner_tags\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.ner_tags[idx]\n",
    "\n",
    "# Collate function for dynamic padding (NER-only)\n",
    "def collate_fn(batch):\n",
    "    sentences, ner_tags = zip(*batch)\n",
    "    max_len = max(len(s) for s in sentences)\n",
    "\n",
    "    sentence_tensors = []\n",
    "    ner_tensors = []\n",
    "\n",
    "    for s, n in zip(sentences, ner_tags):\n",
    "        padded_sentence = s + [\"<PAD>\"] * (max_len - len(s))\n",
    "        padded_ner = n + [\"<PAD>\"] * (max_len - len(n))\n",
    "\n",
    "        sentence_tensors.append(torch.tensor([vocab.get(word, vocab[\"<UNK>\"]) for word in padded_sentence], dtype=torch.long))\n",
    "        ner_tensors.append(torch.tensor([ner_tag_to_ix[tag] for tag in padded_ner], dtype=torch.long))\n",
    "\n",
    "    return torch.stack(sentence_tensors), torch.stack(ner_tensors)\n",
    "\n",
    "# Define BiLSTM-CRF Model for NER-only\n",
    "class BiLSTMCRF_NER(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_ner_tags, fasttext_embeddings):\n",
    "        super(BiLSTMCRF_NER, self).__init__()\n",
    "        # Initialize embedding layer with FastText embeddings (frozen)\n",
    "        self.embedding = nn.Embedding.from_pretrained(fasttext_embeddings, freeze=True)  # Freeze embeddings\n",
    "        self.dropout = nn.Dropout(0.5)  # Add dropout\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.ner_fc = nn.Linear(hidden_dim * 2, num_ner_tags)\n",
    "        self.ner_crf = CRF(num_ner_tags, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x)\n",
    "        embeddings = self.dropout(embeddings)  # Apply dropout\n",
    "        lstm_out, _ = self.bilstm(embeddings)\n",
    "        lstm_out = self.dropout(lstm_out)  # Apply dropout\n",
    "        ner_logits = self.ner_fc(lstm_out)\n",
    "        return ner_logits\n",
    "\n",
    "    def compute_loss(self, x, ner_tags):\n",
    "        ner_logits = self.forward(x)\n",
    "        ner_loss = -self.ner_crf(ner_logits, ner_tags, mask=(x != vocab[\"<PAD>\"]))\n",
    "        return ner_loss\n",
    "\n",
    "    def decode(self, x):\n",
    "        ner_logits = self.forward(x)\n",
    "        ner_tags = self.ner_crf.decode(ner_logits)\n",
    "        return ner_tags\n",
    "\n",
    "# Paths to pre-split datasets\n",
    "train_file_path = \"/kaggle/input/split-fix-data/train_v5.conll\"\n",
    "val_file_path = \"/kaggle/input/split-fix-data/val_v5.conll\"\n",
    "test_file_path = \"/kaggle/input/split-fix-data/test_v5.conll\"\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = CoNLLDataset(train_file_path)\n",
    "val_dataset = CoNLLDataset(val_file_path)\n",
    "test_dataset = CoNLLDataset(test_file_path)\n",
    "\n",
    "# Create vocabulary and tag-to-index mappings\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "ner_tag_to_ix = {\"<PAD>\": 0}  # Add <PAD> to NER tags\n",
    "\n",
    "# Build vocab and tag mappings\n",
    "for dataset in [train_dataset, val_dataset, test_dataset]:\n",
    "    for sentence, ner_tags in zip(dataset.sentences, dataset.ner_tags):\n",
    "        for word in sentence:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "        for ner_tag in ner_tags:\n",
    "            if ner_tag not in ner_tag_to_ix:\n",
    "                ner_tag_to_ix[ner_tag] = len(ner_tag_to_ix)\n",
    "\n",
    "# Create embedding matrix using FastText (dimension: 300)\n",
    "embedding_dim = 300  # FastText embedding dimension\n",
    "fasttext_embeddings = torch.zeros((len(vocab), embedding_dim))  # Initialize with zeros\n",
    "\n",
    "for word, idx in vocab.items():\n",
    "    if word in fasttext_vectors:\n",
    "        fasttext_embeddings[idx] = torch.tensor(fasttext_vectors[word])  # Use full 300 dimensions\n",
    "    elif word == \"<PAD>\":\n",
    "        fasttext_embeddings[idx] = torch.zeros(embedding_dim)  # Zero vector for padding\n",
    "    else:\n",
    "        fasttext_embeddings[idx] = torch.randn(embedding_dim)  # Random vector for unknown words\n",
    "\n",
    "# Initialize model\n",
    "hidden_dim = 128\n",
    "vocab_size = len(vocab)\n",
    "num_ner_tags = len(ner_tag_to_ix)\n",
    "\n",
    "model = BiLSTMCRF_NER(vocab_size, embedding_dim, hidden_dim, num_ner_tags, fasttext_embeddings).to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # Add weight decay for L2 regularization\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Training loop with early stopping\n",
    "def train_model(model, train_loader, val_loader, test_loader, epochs):\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 3\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for sentences, ner_tags in train_loader:\n",
    "            sentences, ner_tags = sentences.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.compute_loss(sentences, ner_tags)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for sentences, ner_tags in val_loader:\n",
    "                sentences, ner_tags = sentences.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "                val_loss += model.compute_loss(sentences, ner_tags).item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    # Evaluate on test set after training\n",
    "    model.eval()\n",
    "    all_ner_preds, all_ner_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for sentences, ner_tags in test_loader:\n",
    "            sentences, ner_tags = sentences.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "            ner_preds = model.decode(sentences)\n",
    "\n",
    "            # Flatten the predictions and targets, excluding <PAD> tokens\n",
    "            for i in range(len(sentences)):\n",
    "                sentence_length = (sentences[i] != vocab[\"<PAD>\"]).sum().item()  # Length of the actual sentence\n",
    "                all_ner_preds.extend(ner_preds[i][:sentence_length])  # Truncate predictions to sentence length\n",
    "                all_ner_targets.extend(ner_tags[i][:sentence_length].cpu().numpy())  # Truncate targets to sentence length\n",
    "\n",
    "    # Convert predictions and targets to tag names\n",
    "    idx_to_ner = {v: k for k, v in ner_tag_to_ix.items()}\n",
    "\n",
    "    # Filter out padding tokens from predictions and targets\n",
    "    all_ner_preds_filtered = [idx_to_ner[idx] for idx in all_ner_preds]\n",
    "    all_ner_targets_filtered = [idx_to_ner[idx] for idx in all_ner_targets]\n",
    "\n",
    "    # Generate classification report\n",
    "    print(\"NER Classification Report:\")\n",
    "    print(classification_report(all_ner_targets_filtered, all_ner_preds_filtered, zero_division=0, digits=4))\n",
    "\n",
    "# Function to display random sentences with true and predicted NER tags\n",
    "def display_random_samples(model, test_loader, vocab, ner_tag_to_ix, num_samples=5):\n",
    "    model.eval()\n",
    "    idx_to_ner = {v: k for k, v in ner_tag_to_ix.items()}\n",
    "    vocab_inv = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    # Collect all sentences and their true/predicted tags\n",
    "    all_sentences = []\n",
    "    all_true_ner = []\n",
    "    all_pred_ner = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sentences, ner_tags in test_loader:\n",
    "            sentences, ner_tags = sentences.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "            ner_preds = model.decode(sentences)\n",
    "\n",
    "            # Convert indices to words and tags\n",
    "            for i in range(len(sentences)):\n",
    "                sentence_length = (sentences[i] != vocab[\"<PAD>\"]).sum().item()  # Length of the actual sentence\n",
    "                words = [vocab_inv[idx.item()] for idx in sentences[i][:sentence_length]]\n",
    "                true_ner = [idx_to_ner[idx.item()] for idx in ner_tags[i][:sentence_length]]\n",
    "                pred_ner = [idx_to_ner[idx] for idx in ner_preds[i][:sentence_length]]\n",
    "\n",
    "                all_sentences.append(words)\n",
    "                all_true_ner.append(true_ner)\n",
    "                all_pred_ner.append(pred_ner)\n",
    "\n",
    "    # Randomly select `num_samples` sentences\n",
    "    random_indices = random.sample(range(len(all_sentences)), num_samples)\n",
    "    for idx in random_indices:\n",
    "        print(f\"\\nSample {idx + 1}:\")\n",
    "        print(\"Sentence:    \", \" \".join(all_sentences[idx]))\n",
    "        print(\"True NER:    \", \" \".join(all_true_ner[idx]))\n",
    "        print(\"Predicted NER:\", \" \".join(all_pred_ner[idx]))\n",
    "\n",
    "# Train the model and evaluate on the test set\n",
    "train_model(model, train_loader, val_loader, test_loader, epochs=30)\n",
    "\n",
    "# Display 5 random samples from the test set\n",
    "display_random_samples(model, test_loader, vocab, ner_tag_to_ix, num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T14:45:55.376399Z",
     "iopub.status.busy": "2025-01-27T14:45:55.375848Z",
     "iopub.status.idle": "2025-01-27T15:04:33.822935Z",
     "shell.execute_reply": "2025-01-27T15:04:33.821920Z",
     "shell.execute_reply.started": "2025-01-27T14:45:55.376369Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 341179 words from FastText binary model.\n",
      "Epoch 1/30: Train Loss = 169418.3035, Val Loss = 10607.5950\n",
      "Epoch 2/30: Train Loss = 82694.9764, Val Loss = 7511.4112\n",
      "Epoch 3/30: Train Loss = 67745.3278, Val Loss = 6173.5165\n",
      "Epoch 4/30: Train Loss = 60399.0341, Val Loss = 5515.0616\n",
      "Epoch 5/30: Train Loss = 54845.8512, Val Loss = 5047.3391\n",
      "Epoch 6/30: Train Loss = 50815.8288, Val Loss = 4643.5349\n",
      "Epoch 7/30: Train Loss = 48205.2372, Val Loss = 4397.6679\n",
      "Epoch 8/30: Train Loss = 45515.3411, Val Loss = 4165.3558\n",
      "Epoch 9/30: Train Loss = 43414.4916, Val Loss = 3932.5429\n",
      "Epoch 10/30: Train Loss = 41382.9853, Val Loss = 3770.7794\n",
      "Epoch 11/30: Train Loss = 39823.3249, Val Loss = 3625.9923\n",
      "Epoch 12/30: Train Loss = 38628.3483, Val Loss = 3537.8195\n",
      "Epoch 13/30: Train Loss = 37081.8537, Val Loss = 3424.8073\n",
      "Epoch 14/30: Train Loss = 36049.0337, Val Loss = 3318.6395\n",
      "Epoch 15/30: Train Loss = 34948.4065, Val Loss = 3257.7700\n",
      "Epoch 16/30: Train Loss = 34094.1119, Val Loss = 3175.0503\n",
      "Epoch 17/30: Train Loss = 33458.5028, Val Loss = 3085.5410\n",
      "Epoch 18/30: Train Loss = 32497.9845, Val Loss = 2999.5482\n",
      "Epoch 19/30: Train Loss = 31778.7695, Val Loss = 2974.3844\n",
      "Epoch 20/30: Train Loss = 31048.2837, Val Loss = 2989.4954\n",
      "Epoch 21/30: Train Loss = 30382.4979, Val Loss = 2886.5403\n",
      "Epoch 22/30: Train Loss = 29892.4697, Val Loss = 2825.5349\n",
      "Epoch 23/30: Train Loss = 29406.1601, Val Loss = 2776.7371\n",
      "Epoch 24/30: Train Loss = 28683.2281, Val Loss = 2757.5895\n",
      "Epoch 25/30: Train Loss = 28360.9965, Val Loss = 2748.1326\n",
      "Epoch 26/30: Train Loss = 27880.1407, Val Loss = 2648.9137\n",
      "Epoch 27/30: Train Loss = 27210.4129, Val Loss = 2635.2306\n",
      "Epoch 28/30: Train Loss = 26947.8914, Val Loss = 2648.2579\n",
      "Epoch 29/30: Train Loss = 26725.9009, Val Loss = 2615.8041\n",
      "Epoch 30/30: Train Loss = 25908.5067, Val Loss = 2576.5620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchcrf/__init__.py:308: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:530.)\n",
      "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         abb     0.0000    0.0000    0.0000        18\n",
      "         adj     0.8615    0.6450    0.7377       569\n",
      "         adv     0.8093    0.5365    0.6453       356\n",
      "        conj     0.9057    0.8972    0.9014       739\n",
      "          fw     0.8929    0.7692    0.8264        65\n",
      "         int     0.7857    0.6471    0.7097        17\n",
      "           n     0.9313    0.9747    0.9525      7694\n",
      "         num     0.9953    0.9906    0.9930       641\n",
      "        part     0.9585    0.9635    0.9610      4461\n",
      "         ppm     0.9861    0.9861    0.9861      4114\n",
      "        pron     0.9551    0.8651    0.9079       467\n",
      "        punc     1.0000    0.9966    0.9983      2919\n",
      "          sb     1.0000    0.9231    0.9600        13\n",
      "          tn     0.9387    0.9107    0.9245       168\n",
      "           v     0.9306    0.9255    0.9280      3302\n",
      "\n",
      "    accuracy                         0.9515     25543\n",
      "   macro avg     0.8634    0.8021    0.8288     25543\n",
      "weighted avg     0.9499    0.9515    0.9498     25543\n",
      "\n",
      "NER Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-DATE     0.8154    0.8030    0.8092        66\n",
      "       B-LOC     0.9711    0.9679    0.9695      1182\n",
      "       B-NUM     0.5714    0.2667    0.3636        15\n",
      "       B-ORG     0.6000    0.3125    0.4110        48\n",
      "       B-PER     0.7500    0.7941    0.7714        34\n",
      "      B-TIME     0.8750    0.7778    0.8235         9\n",
      "      E-DATE     0.8154    0.8030    0.8092        66\n",
      "       E-LOC     0.9703    0.9679    0.9691      1182\n",
      "       E-NUM     0.5714    0.2667    0.3636        15\n",
      "       E-ORG     0.5600    0.2917    0.3836        48\n",
      "       E-PER     0.7500    0.7941    0.7714        34\n",
      "      E-TIME     0.8750    0.7778    0.8235         9\n",
      "      I-DATE     0.7561    0.8158    0.7848        38\n",
      "       I-LOC     0.9683    0.9702    0.9692       503\n",
      "       I-ORG     0.4762    0.2564    0.3333        39\n",
      "           O     0.9841    0.9922    0.9882     21324\n",
      "      S-DATE     1.0000    0.8523    0.9202        88\n",
      "       S-LOC     0.6667    0.3387    0.4492       124\n",
      "       S-NUM     0.9526    0.9726    0.9625       475\n",
      "       S-ORG     0.2500    0.0476    0.0800        21\n",
      "       S-PER     0.7321    0.6861    0.7083       223\n",
      "      S-TIME     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.9756     25543\n",
      "   macro avg     0.7232    0.6252    0.6575     25543\n",
      "weighted avg     0.9730    0.9756    0.9736     25543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#embedding300/hiddendim128/batch32(Joint_Model)\n",
    "# Load FastText binary model\n",
    "fasttext_bin_file = \"/kaggle/input/glove-100d/cc.my.300.bin\"  \n",
    "fasttext_model = load_facebook_model(fasttext_bin_file)\n",
    "\n",
    "# Extract the word vectors\n",
    "fasttext_vectors = fasttext_model.wv\n",
    "print(f\"Loaded {len(fasttext_vectors)} words from FastText binary model.\")\n",
    "\n",
    "# Define Dataset Class\n",
    "class CoNLLDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.sentences, self.pos_tags, self.ner_tags = self.load_data(file_path)\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        sentences, pos_tags, ner_tags = [], [], []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            sentence, pos_tag, ner_tag = [], [], []\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    word, pos, ner = line.strip().split(\"\\t\")\n",
    "                    sentence.append(word)\n",
    "                    pos_tag.append(pos)\n",
    "                    ner_tag.append(ner)\n",
    "                else:\n",
    "                    if sentence:\n",
    "                        sentences.append(sentence)\n",
    "                        pos_tags.append(pos_tag)\n",
    "                        ner_tags.append(ner_tag)\n",
    "                    sentence, pos_tag, ner_tag = [], [], []\n",
    "            if sentence:\n",
    "                sentences.append(sentence)\n",
    "                pos_tags.append(pos_tag)\n",
    "                ner_tags.append(ner_tag)\n",
    "        return sentences, pos_tags, ner_tags\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.pos_tags[idx], self.ner_tags[idx]\n",
    "\n",
    "# Collate function for dynamic padding\n",
    "def collate_fn(batch):\n",
    "    sentences, pos_tags, ner_tags = zip(*batch)\n",
    "    max_len = max(len(s) for s in sentences)\n",
    "\n",
    "    sentence_tensors = []\n",
    "    pos_tensors = []\n",
    "    ner_tensors = []\n",
    "\n",
    "    for s, p, n in zip(sentences, pos_tags, ner_tags):\n",
    "        padded_sentence = s + [\"<PAD>\"] * (max_len - len(s))\n",
    "        padded_pos = p + [\"<PAD>\"] * (max_len - len(p))\n",
    "        padded_ner = n + [\"<PAD>\"] * (max_len - len(n))\n",
    "\n",
    "        sentence_tensors.append(torch.tensor([vocab.get(word, vocab[\"<UNK>\"]) for word in padded_sentence], dtype=torch.long))\n",
    "        pos_tensors.append(torch.tensor([pos_tag_to_ix[tag] for tag in padded_pos], dtype=torch.long))\n",
    "        ner_tensors.append(torch.tensor([ner_tag_to_ix[tag] for tag in padded_ner], dtype=torch.long))\n",
    "\n",
    "    return torch.stack(sentence_tensors), torch.stack(pos_tensors), torch.stack(ner_tensors)\n",
    "\n",
    "# Define BiLSTM-CRF Model with Frozen FastText Embeddings\n",
    "class BiLSTMCRF(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_pos_tags, num_ner_tags, fasttext_embeddings):\n",
    "        super(BiLSTMCRF, self).__init__()\n",
    "        # Initialize embedding layer with FastText embeddings (frozen)\n",
    "        self.embedding = nn.Embedding.from_pretrained(fasttext_embeddings, freeze=True)  # Freeze embeddings\n",
    "        self.dropout = nn.Dropout(0.5)  # Add dropout\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.pos_fc = nn.Linear(hidden_dim * 2, num_pos_tags)\n",
    "        self.ner_fc = nn.Linear(hidden_dim * 2, num_ner_tags)\n",
    "        self.pos_crf = CRF(num_pos_tags, batch_first=True)\n",
    "        self.ner_crf = CRF(num_ner_tags, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x)\n",
    "        embeddings = self.dropout(embeddings)  # Apply dropout\n",
    "        lstm_out, _ = self.bilstm(embeddings)\n",
    "        lstm_out = self.dropout(lstm_out)  # Apply dropout\n",
    "        pos_logits = self.pos_fc(lstm_out)\n",
    "        ner_logits = self.ner_fc(lstm_out)\n",
    "        return pos_logits, ner_logits\n",
    "\n",
    "    def compute_loss(self, x, pos_tags, ner_tags, alpha=0.5):\n",
    "        pos_logits, ner_logits = self.forward(x)\n",
    "        pos_loss = -self.pos_crf(pos_logits, pos_tags, mask=(x != vocab[\"<PAD>\"]))\n",
    "        ner_loss = -self.ner_crf(ner_logits, ner_tags, mask=(x != vocab[\"<PAD>\"]))\n",
    "        return alpha * pos_loss + (1 - alpha) * ner_loss\n",
    "\n",
    "    def decode(self, x):\n",
    "        pos_logits, ner_logits = self.forward(x)\n",
    "        pos_tags = self.pos_crf.decode(pos_logits)\n",
    "        ner_tags = self.ner_crf.decode(ner_logits)\n",
    "        return pos_tags, ner_tags\n",
    "\n",
    "# Paths to pre-split datasets\n",
    "train_file_path = \"/kaggle/input/split-fix-data/train_v5.conll\"\n",
    "val_file_path = \"/kaggle/input/split-fix-data/val_v5.conll\"\n",
    "test_file_path = \"/kaggle/input/split-fix-data/test_v5.conll\"\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = CoNLLDataset(train_file_path)\n",
    "val_dataset = CoNLLDataset(val_file_path)\n",
    "test_dataset = CoNLLDataset(test_file_path)\n",
    "\n",
    "# Create vocabulary and tag-to-index mappings\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "pos_tag_to_ix = {\"<PAD>\": 0}  # Add <PAD> to POS tags\n",
    "ner_tag_to_ix = {\"<PAD>\": 0}  # Add <PAD> to NER tags\n",
    "\n",
    "# Build vocab and tag mappings\n",
    "for dataset in [train_dataset, val_dataset, test_dataset]:\n",
    "    for sentence, pos_tags, ner_tags in zip(dataset.sentences, dataset.pos_tags, dataset.ner_tags):\n",
    "        for word in sentence:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "        for pos_tag in pos_tags:\n",
    "            if pos_tag not in pos_tag_to_ix:\n",
    "                pos_tag_to_ix[pos_tag] = len(pos_tag_to_ix)\n",
    "        for ner_tag in ner_tags:\n",
    "            if ner_tag not in ner_tag_to_ix:\n",
    "                ner_tag_to_ix[ner_tag] = len(ner_tag_to_ix)\n",
    "\n",
    "# Create embedding matrix using FastText (dimension: 300)\n",
    "embedding_dim = 300  # FastText embedding dimension\n",
    "fasttext_embeddings = torch.zeros((len(vocab), embedding_dim))  # Initialize with zeros\n",
    "\n",
    "for word, idx in vocab.items():\n",
    "    if word in fasttext_vectors:\n",
    "        fasttext_embeddings[idx] = torch.tensor(fasttext_vectors[word])  # Use full 300 dimensions\n",
    "    elif word == \"<PAD>\":\n",
    "        fasttext_embeddings[idx] = torch.zeros(embedding_dim)  # Zero vector for padding\n",
    "    else:\n",
    "        fasttext_embeddings[idx] = torch.randn(embedding_dim)  # Random vector for unknown words\n",
    "\n",
    "# Initialize model\n",
    "hidden_dim = 128  \n",
    "vocab_size = len(vocab)\n",
    "num_pos_tags = len(pos_tag_to_ix)\n",
    "num_ner_tags = len(ner_tag_to_ix)\n",
    "\n",
    "model = BiLSTMCRF(vocab_size, embedding_dim, hidden_dim, num_pos_tags, num_ner_tags, fasttext_embeddings).to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # Add weight decay for L2 regularization\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Training loop with early stopping\n",
    "def train_model(model, train_loader, val_loader, test_loader, epochs):\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for sentences, pos_tags, ner_tags in train_loader:\n",
    "            sentences, pos_tags, ner_tags = sentences.to(\"cuda\"), pos_tags.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.compute_loss(sentences, pos_tags, ner_tags, alpha=0.5)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for sentences, pos_tags, ner_tags in val_loader:\n",
    "                sentences, pos_tags, ner_tags = sentences.to(\"cuda\"), pos_tags.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "                val_loss += model.compute_loss(sentences, pos_tags, ner_tags).item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    # Evaluate on test set after training\n",
    "    model.eval()\n",
    "    all_pos_preds, all_pos_targets, all_ner_preds, all_ner_targets = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for sentences, pos_tags, ner_tags in test_loader:\n",
    "            sentences, pos_tags, ner_tags = sentences.to(\"cuda\"), pos_tags.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "            pos_preds, ner_preds = model.decode(sentences)\n",
    "\n",
    "            # Flatten the predictions and targets, excluding <PAD> tokens\n",
    "            for i in range(len(sentences)):\n",
    "                sentence_length = (sentences[i] != vocab[\"<PAD>\"]).sum().item()  # Length of the actual sentence\n",
    "                all_pos_preds.extend(pos_preds[i][:sentence_length])  # Truncate predictions to sentence length\n",
    "                all_pos_targets.extend(pos_tags[i][:sentence_length].cpu().numpy())  # Truncate targets to sentence length\n",
    "                all_ner_preds.extend(ner_preds[i][:sentence_length])  # Truncate predictions to sentence length\n",
    "                all_ner_targets.extend(ner_tags[i][:sentence_length].cpu().numpy())  # Truncate targets to sentence length\n",
    "\n",
    "    # Convert predictions and targets to tag names\n",
    "    idx_to_pos = {v: k for k, v in pos_tag_to_ix.items()}\n",
    "    idx_to_ner = {v: k for k, v in ner_tag_to_ix.items()}\n",
    "\n",
    "    # Filter out padding tokens from predictions and targets\n",
    "    all_pos_preds_filtered = [idx_to_pos[idx] for idx in all_pos_preds]\n",
    "    all_pos_targets_filtered = [idx_to_pos[idx] for idx in all_pos_targets]\n",
    "    all_ner_preds_filtered = [idx_to_ner[idx] for idx in all_ner_preds]\n",
    "    all_ner_targets_filtered = [idx_to_ner[idx] for idx in all_ner_targets]\n",
    "\n",
    "    # Generate classification reports\n",
    "    print(\"POS Classification Report:\")\n",
    "    print(classification_report(all_pos_targets_filtered, all_pos_preds_filtered, zero_division=0,digits=4))\n",
    "\n",
    "    print(\"NER Classification Report:\")\n",
    "    print(classification_report(all_ner_targets_filtered, all_ner_preds_filtered, zero_division=0,digits=4))\n",
    "\n",
    "# Train the model and evaluate on the test set\n",
    "train_model(model, train_loader, val_loader, test_loader, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T14:14:04.023986Z",
     "iopub.status.busy": "2025-01-27T14:14:04.023627Z",
     "iopub.status.idle": "2025-01-27T14:25:55.308637Z",
     "shell.execute_reply": "2025-01-27T14:25:55.307855Z",
     "shell.execute_reply.started": "2025-01-27T14:14:04.023959Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 341179 words from FastText binary model.\n",
      "Epoch 1/30: Train Loss = 189753.5316, Val Loss = 12197.6097\n",
      "Epoch 2/30: Train Loss = 90291.5476, Val Loss = 8242.5269\n",
      "Epoch 3/30: Train Loss = 72547.8926, Val Loss = 6888.6232\n",
      "Epoch 4/30: Train Loss = 64029.1100, Val Loss = 6070.9285\n",
      "Epoch 5/30: Train Loss = 58769.2189, Val Loss = 5614.3379\n",
      "Epoch 6/30: Train Loss = 54536.0291, Val Loss = 5305.4430\n",
      "Epoch 7/30: Train Loss = 51391.5148, Val Loss = 4844.2746\n",
      "Epoch 8/30: Train Loss = 48234.5868, Val Loss = 4592.1749\n",
      "Epoch 9/30: Train Loss = 45719.6047, Val Loss = 4316.2368\n",
      "Epoch 10/30: Train Loss = 43768.6254, Val Loss = 4184.5578\n",
      "Epoch 11/30: Train Loss = 41894.0872, Val Loss = 4009.8388\n",
      "Epoch 12/30: Train Loss = 40241.6173, Val Loss = 3840.2436\n",
      "Epoch 13/30: Train Loss = 38795.6116, Val Loss = 3687.9126\n",
      "Epoch 14/30: Train Loss = 37579.8927, Val Loss = 3629.5074\n",
      "Epoch 15/30: Train Loss = 36288.8907, Val Loss = 3450.6194\n",
      "Epoch 16/30: Train Loss = 34845.4041, Val Loss = 3399.1958\n",
      "Epoch 17/30: Train Loss = 34164.8549, Val Loss = 3342.4415\n",
      "Epoch 18/30: Train Loss = 33085.7942, Val Loss = 3248.7445\n",
      "Epoch 19/30: Train Loss = 32274.7512, Val Loss = 3193.6050\n",
      "Epoch 20/30: Train Loss = 31513.6086, Val Loss = 3142.6025\n",
      "Epoch 21/30: Train Loss = 30720.5567, Val Loss = 3084.1050\n",
      "Epoch 22/30: Train Loss = 29921.7189, Val Loss = 3016.5460\n",
      "Epoch 23/30: Train Loss = 29212.9388, Val Loss = 2971.2168\n",
      "Epoch 24/30: Train Loss = 28653.0873, Val Loss = 2937.9140\n",
      "Epoch 25/30: Train Loss = 28006.2523, Val Loss = 2858.7816\n",
      "Epoch 26/30: Train Loss = 27439.3664, Val Loss = 2830.7285\n",
      "Epoch 27/30: Train Loss = 27242.6572, Val Loss = 2832.8731\n",
      "Epoch 28/30: Train Loss = 26514.4939, Val Loss = 2776.7988\n",
      "Epoch 29/30: Train Loss = 25949.5758, Val Loss = 2757.7619\n",
      "Epoch 30/30: Train Loss = 25182.7988, Val Loss = 2740.5567\n",
      "Total training time: 693.43 seconds\n",
      "POS Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         abb     0.0000    0.0000    0.0000        18\n",
      "         adj     0.8711    0.5940    0.7064       569\n",
      "         adv     0.8182    0.4803    0.6053       356\n",
      "        conj     0.9224    0.8687    0.8948       739\n",
      "          fw     0.9592    0.7231    0.8246        65\n",
      "         int     0.8333    0.5882    0.6897        17\n",
      "           n     0.9262    0.9784    0.9516      7694\n",
      "         num     0.9969    0.9891    0.9930       641\n",
      "        part     0.9600    0.9570    0.9585      4461\n",
      "         ppm     0.9798    0.9881    0.9839      4114\n",
      "        pron     0.9614    0.8544    0.9048       467\n",
      "        punc     0.9997    0.9966    0.9981      2919\n",
      "          sb     0.9286    1.0000    0.9630        13\n",
      "          tn     0.9571    0.9286    0.9426       168\n",
      "           v     0.9194    0.9264    0.9229      3302\n",
      "\n",
      "    accuracy                         0.9490     25543\n",
      "   macro avg     0.8689    0.7915    0.8226     25543\n",
      "weighted avg     0.9474    0.9490    0.9466     25543\n",
      "\n",
      "NER Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-DATE     0.8308    0.8182    0.8244        66\n",
      "       B-LOC     0.9817    0.9518    0.9665      1182\n",
      "       B-NUM     0.5000    0.3333    0.4000        15\n",
      "       B-ORG     0.4407    0.5417    0.4860        48\n",
      "       B-PER     0.7838    0.8529    0.8169        34\n",
      "      B-TIME     0.8889    0.8889    0.8889         9\n",
      "      E-DATE     0.8462    0.8333    0.8397        66\n",
      "       E-LOC     0.9826    0.9535    0.9678      1182\n",
      "       E-NUM     0.5000    0.3333    0.4000        15\n",
      "       E-ORG     0.4576    0.5625    0.5047        48\n",
      "       E-PER     0.7838    0.8529    0.8169        34\n",
      "      E-TIME     0.8889    0.8889    0.8889         9\n",
      "      I-DATE     0.7500    0.7895    0.7692        38\n",
      "       I-LOC     0.9818    0.9662    0.9739       503\n",
      "       I-ORG     0.3529    0.4615    0.4000        39\n",
      "           O     0.9844    0.9909    0.9877     21324\n",
      "      S-DATE     0.9868    0.8523    0.9146        88\n",
      "       S-LOC     0.6852    0.2984    0.4157       124\n",
      "       S-NUM     0.9563    0.9663    0.9613       475\n",
      "       S-ORG     0.0000    0.0000    0.0000        21\n",
      "       S-PER     0.7162    0.7130    0.7146       223\n",
      "      S-TIME     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.9746     25543\n",
      "   macro avg     0.6954    0.6750    0.6790     25543\n",
      "weighted avg     0.9738    0.9746    0.9737     25543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#embedding300/hiddendim256/batch64 (Best REsults_joint_model)\n",
    "import time\n",
    "# Load FastText binary model\n",
    "fasttext_bin_file = \"/kaggle/input/glove-100d/cc.my.300.bin\"  \n",
    "fasttext_model = load_facebook_model(fasttext_bin_file)\n",
    "\n",
    "# Extract the word vectors\n",
    "fasttext_vectors = fasttext_model.wv\n",
    "print(f\"Loaded {len(fasttext_vectors)} words from FastText binary model.\")\n",
    "\n",
    "# Define Dataset Class\n",
    "class CoNLLDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.sentences, self.pos_tags, self.ner_tags = self.load_data(file_path)\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        sentences, pos_tags, ner_tags = [], [], []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            sentence, pos_tag, ner_tag = [], [], []\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    word, pos, ner = line.strip().split(\"\\t\")\n",
    "                    sentence.append(word)\n",
    "                    pos_tag.append(pos)\n",
    "                    ner_tag.append(ner)\n",
    "                else:\n",
    "                    if sentence:\n",
    "                        sentences.append(sentence)\n",
    "                        pos_tags.append(pos_tag)\n",
    "                        ner_tags.append(ner_tag)\n",
    "                    sentence, pos_tag, ner_tag = [], [], []\n",
    "            if sentence:\n",
    "                sentences.append(sentence)\n",
    "                pos_tags.append(pos_tag)\n",
    "                ner_tags.append(ner_tag)\n",
    "        return sentences, pos_tags, ner_tags\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.pos_tags[idx], self.ner_tags[idx]\n",
    "\n",
    "# Collate function for dynamic padding\n",
    "def collate_fn(batch):\n",
    "    sentences, pos_tags, ner_tags = zip(*batch)\n",
    "    max_len = max(len(s) for s in sentences)\n",
    "\n",
    "    sentence_tensors = []\n",
    "    pos_tensors = []\n",
    "    ner_tensors = []\n",
    "\n",
    "    for s, p, n in zip(sentences, pos_tags, ner_tags):\n",
    "        padded_sentence = s + [\"<PAD>\"] * (max_len - len(s))\n",
    "        padded_pos = p + [\"<PAD>\"] * (max_len - len(p))\n",
    "        padded_ner = n + [\"<PAD>\"] * (max_len - len(n))\n",
    "\n",
    "        sentence_tensors.append(torch.tensor([vocab.get(word, vocab[\"<UNK>\"]) for word in padded_sentence], dtype=torch.long))\n",
    "        pos_tensors.append(torch.tensor([pos_tag_to_ix[tag] for tag in padded_pos], dtype=torch.long))\n",
    "        ner_tensors.append(torch.tensor([ner_tag_to_ix[tag] for tag in padded_ner], dtype=torch.long))\n",
    "\n",
    "    return torch.stack(sentence_tensors), torch.stack(pos_tensors), torch.stack(ner_tensors)\n",
    "\n",
    "# Define BiLSTM-CRF Model with Frozen FastText Embeddings\n",
    "class BiLSTMCRF(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_pos_tags, num_ner_tags, fasttext_embeddings):\n",
    "        super(BiLSTMCRF, self).__init__()\n",
    "        # Initialize embedding layer with FastText embeddings (frozen)\n",
    "        self.embedding = nn.Embedding.from_pretrained(fasttext_embeddings, freeze=True)  # Freeze embeddings\n",
    "        self.dropout = nn.Dropout(0.5)  # Add dropout\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.pos_fc = nn.Linear(hidden_dim * 2, num_pos_tags)\n",
    "        self.ner_fc = nn.Linear(hidden_dim * 2, num_ner_tags)\n",
    "        self.pos_crf = CRF(num_pos_tags, batch_first=True)\n",
    "        self.ner_crf = CRF(num_ner_tags, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x)\n",
    "        embeddings = self.dropout(embeddings)  # Apply dropout\n",
    "        lstm_out, _ = self.bilstm(embeddings)\n",
    "        lstm_out = self.dropout(lstm_out)  # Apply dropout\n",
    "        pos_logits = self.pos_fc(lstm_out)\n",
    "        ner_logits = self.ner_fc(lstm_out)\n",
    "        return pos_logits, ner_logits\n",
    "\n",
    "    def compute_loss(self, x, pos_tags, ner_tags, alpha=0.5):\n",
    "        pos_logits, ner_logits = self.forward(x)\n",
    "        pos_loss = -self.pos_crf(pos_logits, pos_tags, mask=(x != vocab[\"<PAD>\"]))\n",
    "        ner_loss = -self.ner_crf(ner_logits, ner_tags, mask=(x != vocab[\"<PAD>\"]))\n",
    "        return alpha * pos_loss + (1 - alpha) * ner_loss\n",
    "\n",
    "    def decode(self, x):\n",
    "        pos_logits, ner_logits = self.forward(x)\n",
    "        pos_tags = self.pos_crf.decode(pos_logits)\n",
    "        ner_tags = self.ner_crf.decode(ner_logits)\n",
    "        return pos_tags, ner_tags\n",
    "\n",
    "# Paths to pre-split datasets\n",
    "train_file_path = \"/kaggle/input/split-fix-data/train_v5.conll\"\n",
    "val_file_path = \"/kaggle/input/split-fix-data/val_v5.conll\"\n",
    "test_file_path = \"/kaggle/input/split-fix-data/test_v5.conll\"\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = CoNLLDataset(train_file_path)\n",
    "val_dataset = CoNLLDataset(val_file_path)\n",
    "test_dataset = CoNLLDataset(test_file_path)\n",
    "\n",
    "# Create vocabulary and tag-to-index mappings\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "pos_tag_to_ix = {\"<PAD>\": 0}  # Add <PAD> to POS tags\n",
    "ner_tag_to_ix = {\"<PAD>\": 0}  # Add <PAD> to NER tags\n",
    "\n",
    "# Build vocab and tag mappings\n",
    "for dataset in [train_dataset, val_dataset, test_dataset]:\n",
    "    for sentence, pos_tags, ner_tags in zip(dataset.sentences, dataset.pos_tags, dataset.ner_tags):\n",
    "        for word in sentence:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "        for pos_tag in pos_tags:\n",
    "            if pos_tag not in pos_tag_to_ix:\n",
    "                pos_tag_to_ix[pos_tag] = len(pos_tag_to_ix)\n",
    "        for ner_tag in ner_tags:\n",
    "            if ner_tag not in ner_tag_to_ix:\n",
    "                ner_tag_to_ix[ner_tag] = len(ner_tag_to_ix)\n",
    "\n",
    "# Create embedding matrix using FastText (dimension: 300)\n",
    "embedding_dim = 300  # FastText embedding dimension\n",
    "fasttext_embeddings = torch.zeros((len(vocab), embedding_dim))  # Initialize with zeros\n",
    "\n",
    "for word, idx in vocab.items():\n",
    "    if word in fasttext_vectors:\n",
    "        fasttext_embeddings[idx] = torch.tensor(fasttext_vectors[word])  # Use full 300 dimensions\n",
    "    elif word == \"<PAD>\":\n",
    "        fasttext_embeddings[idx] = torch.zeros(embedding_dim)  # Zero vector for padding\n",
    "    else:\n",
    "        fasttext_embeddings[idx] = torch.randn(embedding_dim)  # Random vector for unknown words\n",
    "\n",
    "# Initialize model\n",
    "hidden_dim = 256\n",
    "vocab_size = len(vocab)\n",
    "num_pos_tags = len(pos_tag_to_ix)\n",
    "num_ner_tags = len(ner_tag_to_ix)\n",
    "\n",
    "model = BiLSTMCRF(vocab_size, embedding_dim, hidden_dim, num_pos_tags, num_ner_tags, fasttext_embeddings).to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # Add weight decay for L2 regularization\n",
    "\n",
    "# Create data loaders with batch size 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Training loop with early stopping and training time measurement\n",
    "def train_model(model, train_loader, val_loader, test_loader, epochs):\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 3\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    start_time = time.time() \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for sentences, pos_tags, ner_tags in train_loader:\n",
    "            sentences, pos_tags, ner_tags = sentences.to(\"cuda\"), pos_tags.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.compute_loss(sentences, pos_tags, ner_tags, alpha=0.5)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for sentences, pos_tags, ner_tags in val_loader:\n",
    "                sentences, pos_tags, ner_tags = sentences.to(\"cuda\"), pos_tags.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "                val_loss += model.compute_loss(sentences, pos_tags, ner_tags).item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    end_time = time.time()  # End measuring training time\n",
    "    training_time = end_time - start_time  # Calculate total training time\n",
    "    print(f\"Total training time: {training_time:.2f} seconds\")\n",
    "\n",
    "    # Evaluate on test set after training\n",
    "    model.eval()\n",
    "    all_pos_preds, all_pos_targets, all_ner_preds, all_ner_targets = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for sentences, pos_tags, ner_tags in test_loader:\n",
    "            sentences, pos_tags, ner_tags = sentences.to(\"cuda\"), pos_tags.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "            pos_preds, ner_preds = model.decode(sentences)\n",
    "\n",
    "            # Flatten the predictions and targets, excluding <PAD> tokens\n",
    "            for i in range(len(sentences)):\n",
    "                sentence_length = (sentences[i] != vocab[\"<PAD>\"]).sum().item()  # Length of the actual sentence\n",
    "                all_pos_preds.extend(pos_preds[i][:sentence_length])  # Truncate predictions to sentence length\n",
    "                all_pos_targets.extend(pos_tags[i][:sentence_length].cpu().numpy())  # Truncate targets to sentence length\n",
    "                all_ner_preds.extend(ner_preds[i][:sentence_length])  # Truncate predictions to sentence length\n",
    "                all_ner_targets.extend(ner_tags[i][:sentence_length].cpu().numpy())  # Truncate targets to sentence length\n",
    "\n",
    "    # Convert predictions and targets to tag names\n",
    "    idx_to_pos = {v: k for k, v in pos_tag_to_ix.items()}\n",
    "    idx_to_ner = {v: k for k, v in ner_tag_to_ix.items()}\n",
    "\n",
    "    # Filter out padding tokens from predictions and targets\n",
    "    all_pos_preds_filtered = [idx_to_pos[idx] for idx in all_pos_preds]\n",
    "    all_pos_targets_filtered = [idx_to_pos[idx] for idx in all_pos_targets]\n",
    "    all_ner_preds_filtered = [idx_to_ner[idx] for idx in all_ner_preds]\n",
    "    all_ner_targets_filtered = [idx_to_ner[idx] for idx in all_ner_targets]\n",
    "\n",
    "    # Generate classification reports with 4-digit precision\n",
    "    print(\"POS Classification Report:\")\n",
    "    print(classification_report(all_pos_targets_filtered, all_pos_preds_filtered, zero_division=0, digits=4))\n",
    "\n",
    "    print(\"NER Classification Report:\")\n",
    "    print(classification_report(all_ner_targets_filtered, all_ner_preds_filtered, zero_division=0, digits=4))\n",
    "\n",
    "# Train the model and evaluate on the test set\n",
    "train_model(model, train_loader, val_loader, test_loader, epochs=30)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6515326,
     "sourceId": 10566039,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6521472,
     "sourceId": 10583082,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
