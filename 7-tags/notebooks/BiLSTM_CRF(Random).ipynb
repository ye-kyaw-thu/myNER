{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T05:49:56.313184Z",
     "iopub.status.busy": "2025-01-27T05:49:56.312826Z",
     "iopub.status.idle": "2025-01-27T05:50:03.357428Z",
     "shell.execute_reply": "2025-01-27T05:50:03.356378Z",
     "shell.execute_reply.started": "2025-01-27T05:49:56.313152Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/kmkurn/pytorch-crf.git\n",
      "  Cloning https://github.com/kmkurn/pytorch-crf.git to /tmp/pip-req-build-zf5pge64\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/kmkurn/pytorch-crf.git /tmp/pip-req-build-zf5pge64\n",
      "  Resolved https://github.com/kmkurn/pytorch-crf.git to commit 623e3402d00a2728e99d6e8486010d67c754267b\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: pytorch-crf\n",
      "  Building wheel for pytorch-crf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pytorch-crf: filename=pytorch_crf-0.7.2-py3-none-any.whl size=6410 sha256=47a8daa9f603dd1fd4d1dbf766aef7ad80f7369900515d5a943041cef424aab5\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-7domka_u/wheels/39/5f/f6/4b48b35895d914f4f5fff5b600f87658c11693e37b6a4f118e\n",
      "Successfully built pytorch-crf\n",
      "Installing collected packages: pytorch-crf\n",
      "Successfully installed pytorch-crf-0.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/kmkurn/pytorch-crf.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T05:50:05.728013Z",
     "iopub.status.busy": "2025-01-27T05:50:05.727667Z",
     "iopub.status.idle": "2025-01-27T05:50:08.894928Z",
     "shell.execute_reply": "2025-01-27T05:50:08.893975Z",
     "shell.execute_reply.started": "2025-01-27T05:50:05.727983Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7.2\n"
     ]
    }
   ],
   "source": [
    "import torchcrf\n",
    "print(torchcrf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T05:50:13.920576Z",
     "iopub.status.busy": "2025-01-27T05:50:13.920189Z",
     "iopub.status.idle": "2025-01-27T05:50:19.218022Z",
     "shell.execute_reply": "2025-01-27T05:50:19.217057Z",
     "shell.execute_reply.started": "2025-01-27T05:50:13.920554Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gputil\n",
      "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: gputil\n",
      "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=d8bb3519f5dfdbcee301d61e8bdac78fda0808e11c7cd8da79632a7e7844c68e\n",
      "  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\n",
      "Successfully built gputil\n",
      "Installing collected packages: gputil\n",
      "Successfully installed gputil-1.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gputil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T05:53:36.893416Z",
     "iopub.status.busy": "2025-01-27T05:53:36.893021Z",
     "iopub.status.idle": "2025-01-27T05:53:37.505638Z",
     "shell.execute_reply": "2025-01-27T05:53:37.504943Z",
     "shell.execute_reply.started": "2025-01-27T05:53:36.893387Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchcrf import CRF\n",
    "from sklearn.metrics import classification_report\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T05:56:13.117272Z",
     "iopub.status.busy": "2025-01-27T05:56:13.116743Z",
     "iopub.status.idle": "2025-01-27T05:59:39.169344Z",
     "shell.execute_reply": "2025-01-27T05:59:39.168351Z",
     "shell.execute_reply.started": "2025-01-27T05:56:13.117244Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 74621.8118, Val Loss: 4297.6095, Epoch Time: 21.06s\n",
      "Epoch 2/10, Train Loss: 27532.0286, Val Loss: 3069.1376, Epoch Time: 19.86s\n",
      "Epoch 3/10, Train Loss: 17713.8225, Val Loss: 2596.5268, Epoch Time: 20.17s\n",
      "Epoch 4/10, Train Loss: 11614.7879, Val Loss: 2445.8801, Epoch Time: 20.15s\n",
      "Epoch 5/10, Train Loss: 7494.1585, Val Loss: 2400.1904, Epoch Time: 19.94s\n",
      "Epoch 6/10, Train Loss: 4740.3627, Val Loss: 2570.8025, Epoch Time: 20.03s\n",
      "Epoch 7/10, Train Loss: 3191.0941, Val Loss: 2714.5150, Epoch Time: 20.01s\n",
      "Epoch 8/10, Train Loss: 2171.1015, Val Loss: 2917.0572, Epoch Time: 20.04s\n",
      "Epoch 9/10, Train Loss: 1586.2564, Val Loss: 2945.3762, Epoch Time: 20.01s\n",
      "Epoch 10/10, Train Loss: 1277.6119, Val Loss: 2991.6491, Epoch Time: 19.90s\n",
      "Total Training Time: 201.17s\n",
      "Model saved as bilstm_crf_ner_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchcrf/__init__.py:308: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:530.)\n",
      "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Prediction Time: 1.68s\n",
      "NER Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-DATE     0.9057    0.7273    0.8067        66\n",
      "       B-LOC     0.9655    0.9695    0.9675      1182\n",
      "       B-NUM     0.3333    0.1333    0.1905        15\n",
      "       B-ORG     0.6364    0.5833    0.6087        48\n",
      "       B-PER     0.7879    0.7647    0.7761        34\n",
      "      B-TIME     0.7778    0.7778    0.7778         9\n",
      "      E-DATE     0.9074    0.7424    0.8167        66\n",
      "       E-LOC     0.9695    0.9679    0.9687      1182\n",
      "       E-NUM     0.4000    0.1333    0.2000        15\n",
      "       E-ORG     0.6042    0.6042    0.6042        48\n",
      "       E-PER     0.8065    0.7353    0.7692        34\n",
      "      E-TIME     0.7778    0.7778    0.7778         9\n",
      "      I-DATE     0.8108    0.7895    0.8000        38\n",
      "       I-LOC     0.9590    0.9761    0.9675       503\n",
      "       I-ORG     0.8000    0.4103    0.5424        39\n",
      "           O     0.9843    0.9902    0.9873     21324\n",
      "      S-DATE     0.9467    0.8068    0.8712        88\n",
      "       S-LOC     0.6129    0.6129    0.6129       124\n",
      "       S-NUM     0.9295    0.9158    0.9226       475\n",
      "       S-ORG     0.5556    0.2381    0.3333        21\n",
      "       S-PER     0.6902    0.5695    0.6241       223\n",
      "      S-TIME     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.9740     25543\n",
      "   macro avg     0.7346    0.6466    0.6784     25543\n",
      "weighted avg     0.9727    0.9740    0.9730     25543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Emb dim 128, hidden dim 128, batch load 32, epoch 10\n",
    "# Define Dataset Class\n",
    "class CoNLLDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.sentences, self.ner_tags = self.load_data(file_path)\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        sentences, ner_tags = [], []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            sentence, ner_tag = [], []\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    word, _, ner = line.strip().split(\"\\t\")\n",
    "                    sentence.append(word)\n",
    "                    ner_tag.append(ner)\n",
    "                else:\n",
    "                    if sentence:\n",
    "                        sentences.append(sentence)\n",
    "                        ner_tags.append(ner_tag)\n",
    "                    sentence, ner_tag = [], []\n",
    "            if sentence:\n",
    "                sentences.append(sentence)\n",
    "                ner_tags.append(ner_tag)\n",
    "        return sentences, ner_tags\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.ner_tags[idx]\n",
    "\n",
    "# Collate function for dynamic padding\n",
    "def collate_fn(batch):\n",
    "    sentences, ner_tags = zip(*batch)\n",
    "    max_len = max(len(s) for s in sentences)\n",
    "\n",
    "    sentence_tensors = []\n",
    "    ner_tensors = []\n",
    "\n",
    "    for s, n in zip(sentences, ner_tags):\n",
    "        padded_sentence = s + [\"<PAD>\"] * (max_len - len(s))\n",
    "        padded_ner = n + [\"<PAD>\"] * (max_len - len(n))\n",
    "\n",
    "        sentence_tensors.append(torch.tensor([vocab.get(word, vocab[\"<UNK>\"]) for word in padded_sentence], dtype=torch.long))\n",
    "        ner_tensors.append(torch.tensor([ner_tag_to_ix[tag] for tag in padded_ner], dtype=torch.long))\n",
    "\n",
    "    return torch.stack(sentence_tensors), torch.stack(ner_tensors)\n",
    "\n",
    "# Define BiLSTM-CRF Model\n",
    "class BiLSTMCRF_NER(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_ner_tags):\n",
    "        super(BiLSTMCRF_NER, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.ner_fc = nn.Linear(hidden_dim * 2, num_ner_tags)\n",
    "        self.ner_crf = CRF(num_ner_tags, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x)\n",
    "        lstm_out, _ = self.bilstm(embeddings)\n",
    "        ner_logits = self.ner_fc(lstm_out)\n",
    "        return ner_logits\n",
    "\n",
    "    def compute_loss(self, x, ner_tags):\n",
    "        ner_logits = self.forward(x)\n",
    "        ner_loss = -self.ner_crf(ner_logits, ner_tags, mask=(x != vocab[\"<PAD>\"]))\n",
    "        return ner_loss\n",
    "\n",
    "    def decode(self, x):\n",
    "        ner_logits = self.forward(x)\n",
    "        ner_tags = self.ner_crf.decode(ner_logits)\n",
    "        return ner_tags\n",
    "\n",
    "# Paths to pre-split datasets\n",
    "train_file_path = \"/kaggle/input/split-fix-data/train_v5.conll\"\n",
    "val_file_path = \"/kaggle/input/split-fix-data/val_v5.conll\"\n",
    "test_file_path = \"/kaggle/input/split-fix-data/test_v5.conll\"\n",
    "\n",
    "train_dataset = CoNLLDataset(train_file_path)\n",
    "val_dataset = CoNLLDataset(val_file_path)\n",
    "test_dataset = CoNLLDataset(test_file_path)\n",
    "\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "ner_tag_to_ix = {\"<PAD>\": 0}\n",
    "\n",
    "for dataset in [train_dataset, val_dataset, test_dataset]:\n",
    "    for sentence, ner_tags in zip(dataset.sentences, dataset.ner_tags):\n",
    "        for word in sentence:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "        for ner_tag in ner_tags:\n",
    "            if ner_tag not in ner_tag_to_ix:\n",
    "                ner_tag_to_ix[ner_tag] = len(ner_tag_to_ix)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Initialize model\n",
    "embedding_dim = 128\n",
    "hidden_dim = 128\n",
    "vocab_size = len(vocab)\n",
    "num_ner_tags = len(ner_tag_to_ix)\n",
    "\n",
    "model = BiLSTMCRF_NER(vocab_size, embedding_dim, hidden_dim, num_ner_tags).to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, val_loader, epochs):\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for sentences, ner_tags in train_loader:\n",
    "            sentences, ner_tags = sentences.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.compute_loss(sentences, ner_tags)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for sentences, ner_tags in val_loader:\n",
    "                sentences, ner_tags = sentences.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "                val_loss += model.compute_loss(sentences, ner_tags).item()\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Epoch Time: {epoch_end_time - epoch_start_time:.2f}s\")\n",
    "\n",
    "    total_training_time = time.time() - start_time\n",
    "    print(f\"Total Training Time: {total_training_time:.2f}s\")\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), \"bilstm_crf_ner_model(no_emb).pth\")\n",
    "    print(\"Model saved as bilstm_crf_ner_model.pth\")\n",
    "\n",
    "train_model(model, train_loader, val_loader, epochs=10)\n",
    "\n",
    "# Evaluate on test set\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_ner_preds, all_ner_targets = [], []\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for sentences, ner_tags in test_loader:\n",
    "            sentences, ner_tags = sentences.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "            ner_preds = model.decode(sentences)\n",
    "\n",
    "            for i, ner_seq in enumerate(ner_preds):\n",
    "                length = (sentences[i] != vocab[\"<PAD>\"]).sum().item()\n",
    "                all_ner_preds.extend(ner_seq[:length])\n",
    "                all_ner_targets.extend(ner_tags[i, :length].tolist())\n",
    "    end_time = time.time()\n",
    "\n",
    "    total_prediction_time = end_time - start_time\n",
    "    print(f\"Total Prediction Time: {total_prediction_time:.2f}s\")\n",
    "\n",
    "    # Convert indices to tags\n",
    "    idx_to_ner = {v: k for k, v in ner_tag_to_ix.items()}\n",
    "\n",
    "    def convert_indices_to_tags(indices, idx_to_tag):\n",
    "        return [idx_to_tag[idx] for idx in indices if idx != 0]\n",
    "\n",
    "    ner_preds_tags = convert_indices_to_tags(all_ner_preds, idx_to_ner)\n",
    "    ner_targets_tags = convert_indices_to_tags(all_ner_targets, idx_to_ner)\n",
    "\n",
    "    print(\"NER Classification Report:\")\n",
    "    print(classification_report(ner_targets_tags, ner_preds_tags,zero_division=0, digits=4))\n",
    "\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T06:29:43.575326Z",
     "iopub.status.busy": "2025-01-27T06:29:43.574971Z",
     "iopub.status.idle": "2025-01-27T06:36:11.259995Z",
     "shell.execute_reply": "2025-01-27T06:36:11.258996Z",
     "shell.execute_reply.started": "2025-01-27T06:29:43.575301Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 104722.4262, Val Loss: 5820.4296\n",
      "Epoch 2/10, Train Loss: 35709.2739, Val Loss: 3888.3945\n",
      "Epoch 3/10, Train Loss: 23553.7400, Val Loss: 3206.9156\n",
      "Epoch 4/10, Train Loss: 16536.2546, Val Loss: 2876.3075\n",
      "Epoch 5/10, Train Loss: 11613.5927, Val Loss: 2751.7534\n",
      "Epoch 6/10, Train Loss: 7988.1992, Val Loss: 2763.1847\n",
      "Epoch 7/10, Train Loss: 5345.6169, Val Loss: 2736.0782\n",
      "Epoch 8/10, Train Loss: 3577.6333, Val Loss: 2882.9834\n",
      "Epoch 9/10, Train Loss: 2379.0356, Val Loss: 2963.6501\n",
      "Epoch 10/10, Train Loss: 1622.9802, Val Loss: 3167.1150\n",
      "Training completed in 383.54 seconds.\n",
      "Model saved to bilstm_crf_joint_model.pth\n",
      "Prediction completed in 3.15 seconds.\n",
      "POS Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         abb     0.9412    0.8889    0.9143        18\n",
      "         adj     0.8559    0.8559    0.8559       569\n",
      "         adv     0.8519    0.7753    0.8118       356\n",
      "        conj     0.9362    0.9526    0.9443       739\n",
      "          fw     0.3860    0.3385    0.3607        65\n",
      "         int     0.9375    0.8824    0.9091        17\n",
      "           n     0.9682    0.9706    0.9694      7694\n",
      "         num     0.9834    0.9267    0.9542       641\n",
      "        part     0.9738    0.9834    0.9786      4461\n",
      "         ppm     0.9922    0.9949    0.9936      4114\n",
      "        pron     0.9571    0.9550    0.9561       467\n",
      "        punc     0.9997    1.0000    0.9998      2919\n",
      "          sb     1.0000    0.9231    0.9600        13\n",
      "          tn     0.9524    0.9524    0.9524       168\n",
      "           v     0.9466    0.9446    0.9456      3302\n",
      "\n",
      "    accuracy                         0.9677     25543\n",
      "   macro avg     0.9121    0.8896    0.9004     25543\n",
      "weighted avg     0.9674    0.9677    0.9675     25543\n",
      "\n",
      "NER Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-DATE     0.8475    0.7576    0.8000        66\n",
      "       B-LOC     0.9818    0.9594    0.9705      1182\n",
      "       B-NUM     0.5000    0.2667    0.3478        15\n",
      "       B-ORG     0.6579    0.5208    0.5814        48\n",
      "       B-PER     0.9062    0.8529    0.8788        34\n",
      "      B-TIME     0.8571    0.6667    0.7500         9\n",
      "      E-DATE     0.8621    0.7576    0.8065        66\n",
      "       E-LOC     0.9852    0.9585    0.9717      1182\n",
      "       E-NUM     0.4444    0.2667    0.3333        15\n",
      "       E-ORG     0.6316    0.5000    0.5581        48\n",
      "       E-PER     0.9062    0.8529    0.8788        34\n",
      "      E-TIME     0.8333    0.5556    0.6667         9\n",
      "      I-DATE     0.8158    0.8158    0.8158        38\n",
      "       I-LOC     0.9918    0.9583    0.9747       503\n",
      "       I-ORG     0.5455    0.3077    0.3934        39\n",
      "           O     0.9810    0.9927    0.9868     21324\n",
      "      S-DATE     0.9722    0.7955    0.8750        88\n",
      "       S-LOC     0.6134    0.5887    0.6008       124\n",
      "       S-NUM     0.9555    0.9032    0.9286       475\n",
      "       S-ORG     0.6667    0.3810    0.4848        21\n",
      "       S-PER     0.6864    0.5202    0.5918       223\n",
      "      S-TIME     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.9742     25543\n",
      "   macro avg     0.7564    0.6445    0.6907     25543\n",
      "weighted avg     0.9726    0.9742    0.9730     25543\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Emb dim 128, hidden dim 128, batch load 32, epoch 10\n",
    "# Define Dataset Class\n",
    "class CoNLLDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.sentences, self.pos_tags, self.ner_tags = self.load_data(file_path)\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        sentences, pos_tags, ner_tags = [], [], []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            sentence, pos_tag, ner_tag = [], [], []\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    word, pos, ner = line.strip().split(\"\\t\")\n",
    "                    sentence.append(word)\n",
    "                    pos_tag.append(pos)\n",
    "                    ner_tag.append(ner)\n",
    "                else:\n",
    "                    if sentence:\n",
    "                        sentences.append(sentence)\n",
    "                        pos_tags.append(pos_tag)\n",
    "                        ner_tags.append(ner_tag)\n",
    "                    sentence, pos_tag, ner_tag = [], [], []\n",
    "            if sentence:\n",
    "                sentences.append(sentence)\n",
    "                pos_tags.append(pos_tag)\n",
    "                ner_tags.append(ner_tag)\n",
    "        return sentences, pos_tags, ner_tags\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.pos_tags[idx], self.ner_tags[idx]\n",
    "\n",
    "# Collate function for dynamic padding\n",
    "def collate_fn(batch):\n",
    "    sentences, pos_tags, ner_tags = zip(*batch)\n",
    "    max_len = max(len(s) for s in sentences)\n",
    "\n",
    "    sentence_tensors = []\n",
    "    pos_tensors = []\n",
    "    ner_tensors = []\n",
    "\n",
    "    for s, p, n in zip(sentences, pos_tags, ner_tags):\n",
    "        padded_sentence = s + [\"<PAD>\"] * (max_len - len(s))\n",
    "        padded_pos = p + [\"<PAD>\"] * (max_len - len(p))\n",
    "        padded_ner = n + [\"<PAD>\"] * (max_len - len(n))\n",
    "\n",
    "        sentence_tensors.append(torch.tensor([vocab.get(word, vocab[\"<UNK>\"]) for word in padded_sentence], dtype=torch.long))\n",
    "        pos_tensors.append(torch.tensor([pos_tag_to_ix[tag] for tag in padded_pos], dtype=torch.long))\n",
    "        ner_tensors.append(torch.tensor([ner_tag_to_ix[tag] for tag in padded_ner], dtype=torch.long))\n",
    "\n",
    "    return torch.stack(sentence_tensors), torch.stack(pos_tensors), torch.stack(ner_tensors)\n",
    "\n",
    "# Define BiLSTM-CRF Model\n",
    "class BiLSTMCRF_Joint(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_pos_tags, num_ner_tags):\n",
    "        super(BiLSTMCRF_Joint, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.pos_fc = nn.Linear(hidden_dim * 2, num_pos_tags)\n",
    "        self.ner_fc = nn.Linear(hidden_dim * 2, num_ner_tags)\n",
    "        self.pos_crf = CRF(num_pos_tags, batch_first=True)\n",
    "        self.ner_crf = CRF(num_ner_tags, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x)\n",
    "        lstm_out, _ = self.bilstm(embeddings)\n",
    "        pos_logits = self.pos_fc(lstm_out)\n",
    "        ner_logits = self.ner_fc(lstm_out)\n",
    "        return pos_logits, ner_logits\n",
    "\n",
    "    def compute_loss(self, x, pos_tags, ner_tags, alpha=0.5):\n",
    "        pos_logits, ner_logits = self.forward(x)\n",
    "        pos_loss = -self.pos_crf(pos_logits, pos_tags, mask=(x != vocab[\"<PAD>\"]))\n",
    "        ner_loss = -self.ner_crf(ner_logits, ner_tags, mask=(x != vocab[\"<PAD>\"]))\n",
    "        return alpha * pos_loss + (1 - alpha) * ner_loss\n",
    "\n",
    "    def decode(self, x):\n",
    "        pos_logits, ner_logits = self.forward(x)\n",
    "        pos_tags = self.pos_crf.decode(pos_logits)\n",
    "        ner_tags = self.ner_crf.decode(ner_logits)\n",
    "        return pos_tags, ner_tags\n",
    "\n",
    "train_file_path = \"/kaggle/input/split-fix-data/train_v5.conll\"\n",
    "val_file_path = \"/kaggle/input/split-fix-data/val_v5.conll\"\n",
    "test_file_path = \"/kaggle/input/split-fix-data/test_v5.conll\"\n",
    "\n",
    "train_dataset = CoNLLDataset(train_file_path)\n",
    "val_dataset = CoNLLDataset(val_file_path)\n",
    "test_dataset = CoNLLDataset(test_file_path)\n",
    "\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "pos_tag_to_ix = {\"<PAD>\": 0}\n",
    "ner_tag_to_ix = {\"<PAD>\": 0}\n",
    "\n",
    "for dataset in [train_dataset, val_dataset, test_dataset]:\n",
    "    for sentence, pos_tags, ner_tags in zip(dataset.sentences, dataset.pos_tags, dataset.ner_tags):\n",
    "        for word in sentence:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "        for pos_tag in pos_tags:\n",
    "            if pos_tag not in pos_tag_to_ix:\n",
    "                pos_tag_to_ix[pos_tag] = len(pos_tag_to_ix)\n",
    "        for ner_tag in ner_tags:\n",
    "            if ner_tag not in ner_tag_to_ix:\n",
    "                ner_tag_to_ix[ner_tag] = len(ner_tag_to_ix)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Initialize model\n",
    "embedding_dim = 128\n",
    "hidden_dim = 128\n",
    "vocab_size = len(vocab)\n",
    "num_pos_tags = len(pos_tag_to_ix)\n",
    "num_ner_tags = len(ner_tag_to_ix)\n",
    "\n",
    "model = BiLSTMCRF_Joint(vocab_size, embedding_dim, hidden_dim, num_pos_tags, num_ner_tags).to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#training\n",
    "def train_model(model, train_loader, val_loader, epochs, save_path):\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for sentences, pos_tags, ner_tags in train_loader:\n",
    "            sentences, pos_tags, ner_tags = sentences.to(\"cuda\"), pos_tags.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.compute_loss(sentences, pos_tags, ner_tags, alpha=0.5)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for sentences, pos_tags, ner_tags in val_loader:\n",
    "                sentences, pos_tags, ner_tags = sentences.to(\"cuda\"), pos_tags.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "                val_loss += model.compute_loss(sentences, pos_tags, ner_tags).item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds.\")\n",
    "    \n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Model saved to {save_path}\")\n",
    "\n",
    "# Define the path to save the model\n",
    "save_model_path = \"bilstm_crf_joint_model.pth\"\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, epochs=10, save_path=save_model_path)\n",
    "\n",
    "# Evaluation with timing\n",
    "model.eval()\n",
    "all_pos_preds, all_pos_targets, all_ner_preds, all_ner_targets = [], [], [], []\n",
    "prediction_start = time.time()\n",
    "with torch.no_grad():\n",
    "    for sentences, pos_tags, ner_tags in test_loader:\n",
    "        sentences, pos_tags, ner_tags = sentences.to(\"cuda\"), pos_tags.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "        pos_preds, ner_preds = model.decode(sentences)\n",
    "\n",
    "        for i, (pos_seq, ner_seq) in enumerate(zip(pos_preds, ner_preds)):\n",
    "            length = (sentences[i] != vocab[\"<PAD>\"]).sum().item()\n",
    "            all_pos_preds.extend(pos_seq[:length])\n",
    "            all_ner_preds.extend(ner_seq[:length])\n",
    "            all_pos_targets.extend(pos_tags[i, :length].tolist())\n",
    "            all_ner_targets.extend(ner_tags[i, :length].tolist())\n",
    "\n",
    "prediction_end = time.time()\n",
    "prediction_time = prediction_end - prediction_start\n",
    "print(f\"Prediction completed in {prediction_time:.2f} seconds.\")\n",
    "\n",
    "# Map indices to tags\n",
    "idx_to_pos = {v: k for k, v in pos_tag_to_ix.items()}\n",
    "idx_to_ner = {v: k for k, v in ner_tag_to_ix.items()}\n",
    "\n",
    "def convert_indices_to_tags(indices, idx_to_tag):\n",
    "    return [idx_to_tag[idx] for idx in indices if idx != 0]\n",
    "\n",
    "pos_preds_tags = convert_indices_to_tags(all_pos_preds, idx_to_pos)\n",
    "pos_targets_tags = convert_indices_to_tags(all_pos_targets, idx_to_pos)\n",
    "ner_preds_tags = convert_indices_to_tags(all_ner_preds, idx_to_ner)\n",
    "ner_targets_tags = convert_indices_to_tags(all_ner_targets, idx_to_ner)\n",
    "\n",
    "# Print classification reports with precision formatting\n",
    "print(\"POS Classification Report:\")\n",
    "print(classification_report(pos_targets_tags, pos_preds_tags, digits=4))\n",
    "\n",
    "print(\"NER Classification Report:\")\n",
    "print(classification_report(ner_targets_tags, ner_preds_tags, digits=4))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6515326,
     "sourceId": 10566039,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6521472,
     "sourceId": 10583082,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
