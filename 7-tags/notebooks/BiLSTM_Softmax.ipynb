{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-27T16:55:03.022011Z",
     "iopub.status.busy": "2025-01-27T16:55:03.021552Z",
     "iopub.status.idle": "2025-01-27T16:55:07.823941Z",
     "shell.execute_reply": "2025-01-27T16:55:07.823114Z",
     "shell.execute_reply.started": "2025-01-27T16:55:03.021979Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/kmkurn/pytorch-crf.git\n",
      "  Cloning https://github.com/kmkurn/pytorch-crf.git to /tmp/pip-req-build-4zcb6d_t\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/kmkurn/pytorch-crf.git /tmp/pip-req-build-4zcb6d_t\n",
      "  Resolved https://github.com/kmkurn/pytorch-crf.git to commit 623e3402d00a2728e99d6e8486010d67c754267b\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/kmkurn/pytorch-crf.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T16:55:07.825153Z",
     "iopub.status.busy": "2025-01-27T16:55:07.824930Z",
     "iopub.status.idle": "2025-01-27T16:55:09.409209Z",
     "shell.execute_reply": "2025-01-27T16:55:09.408305Z",
     "shell.execute_reply.started": "2025-01-27T16:55:07.825133Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7.2\n"
     ]
    }
   ],
   "source": [
    "import torchcrf\n",
    "print(torchcrf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T16:55:09.411820Z",
     "iopub.status.busy": "2025-01-27T16:55:09.411497Z",
     "iopub.status.idle": "2025-01-27T16:55:12.672183Z",
     "shell.execute_reply": "2025-01-27T16:55:12.671070Z",
     "shell.execute_reply.started": "2025-01-27T16:55:09.411798Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gputil in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gputil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T16:55:12.674470Z",
     "iopub.status.busy": "2025-01-27T16:55:12.674178Z",
     "iopub.status.idle": "2025-01-27T16:55:39.461226Z",
     "shell.execute_reply": "2025-01-27T16:55:39.460476Z",
     "shell.execute_reply.started": "2025-01-27T16:55:12.674444Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 90.7267, Val Loss: 4.5394\n",
      "Epoch 2/10, Train Loss: 33.6738, Val Loss: 3.2901\n",
      "Epoch 3/10, Train Loss: 23.2366, Val Loss: 2.8356\n",
      "Epoch 4/10, Train Loss: 16.6166, Val Loss: 2.6711\n",
      "Epoch 5/10, Train Loss: 11.8033, Val Loss: 2.6069\n",
      "Epoch 6/10, Train Loss: 8.0359, Val Loss: 2.7788\n",
      "Epoch 7/10, Train Loss: 5.4195, Val Loss: 3.0423\n",
      "Epoch 8/10, Train Loss: 3.7292, Val Loss: 2.9134\n",
      "Epoch 9/10, Train Loss: 2.8159, Val Loss: 3.0479\n",
      "Epoch 10/10, Train Loss: 2.2106, Val Loss: 3.1790\n",
      "Training completed in 24.35 seconds.\n",
      "Prediction completed in 0.30 seconds.\n",
      "NER Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-DATE     0.8095    0.7727    0.7907        66\n",
      "       B-LOC     0.9802    0.9619    0.9710      1182\n",
      "       B-NUM     0.3333    0.1333    0.1905        15\n",
      "       B-ORG     0.5946    0.4583    0.5176        48\n",
      "       B-PER     0.8824    0.8824    0.8824        34\n",
      "      B-TIME     0.5556    0.5556    0.5556         9\n",
      "      E-DATE     0.8571    0.7273    0.7869        66\n",
      "       E-LOC     0.9801    0.9602    0.9701      1182\n",
      "       E-NUM     0.4444    0.2667    0.3333        15\n",
      "       E-ORG     0.7143    0.5208    0.6024        48\n",
      "       E-PER     0.9062    0.8529    0.8788        34\n",
      "      E-TIME     0.8750    0.7778    0.8235         9\n",
      "      I-DATE     0.7879    0.6842    0.7324        38\n",
      "       I-LOC     0.9857    0.9602    0.9728       503\n",
      "       I-ORG     0.7500    0.3846    0.5085        39\n",
      "      I-TIME     0.0000    0.0000    0.0000         0\n",
      "           O     0.9812    0.9930    0.9870     21324\n",
      "      S-DATE     0.9863    0.8182    0.8944        88\n",
      "       S-LOC     0.6286    0.5323    0.5764       124\n",
      "       S-NUM     0.9258    0.8926    0.9089       475\n",
      "       S-ORG     0.7500    0.2857    0.4138        21\n",
      "       S-PER     0.7048    0.5247    0.6015       223\n",
      "      S-TIME     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.9740     25543\n",
      "   macro avg     0.7145    0.6063    0.6478     25543\n",
      "weighted avg     0.9721    0.9740    0.9725     25543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Single NER Training\n",
    "# Define Dataset Class\n",
    "class CoNLLDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.sentences, self.ner_tags = self.load_data(file_path)\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        sentences, ner_tags = [], []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            sentence, ner_tag = [], []\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    word, _, ner = line.strip().split(\"\\t\")\n",
    "                    sentence.append(word)\n",
    "                    ner_tag.append(ner)\n",
    "                else:\n",
    "                    if sentence:\n",
    "                        sentences.append(sentence)\n",
    "                        ner_tags.append(ner_tag)\n",
    "                    sentence, ner_tag = [], []\n",
    "            if sentence:\n",
    "                sentences.append(sentence)\n",
    "                ner_tags.append(ner_tag)\n",
    "        return sentences, ner_tags\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.ner_tags[idx]\n",
    "\n",
    "# Collate function for dynamic padding\n",
    "def collate_fn(batch):\n",
    "    sentences, ner_tags = zip(*batch)\n",
    "    max_len = max(len(s) for s in sentences)\n",
    "\n",
    "    sentence_tensors = []\n",
    "    ner_tensors = []\n",
    "\n",
    "    for s, n in zip(sentences, ner_tags):\n",
    "        padded_sentence = s + [\"<PAD>\"] * (max_len - len(s))\n",
    "        padded_ner = n + [\"<PAD>\"] * (max_len - len(n))\n",
    "\n",
    "        sentence_tensors.append(torch.tensor([vocab.get(word, vocab[\"<UNK>\"]) for word in padded_sentence], dtype=torch.long))\n",
    "        ner_tensors.append(torch.tensor([ner_tag_to_ix[tag] for tag in padded_ner], dtype=torch.long))\n",
    "\n",
    "    return torch.stack(sentence_tensors), torch.stack(ner_tensors)\n",
    "\n",
    "# Define BiLSTM Model with Softmax for NER\n",
    "class BiLSTMSoftmax_NER(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_ner_tags):\n",
    "        super(BiLSTMSoftmax_NER, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.ner_fc = nn.Linear(hidden_dim * 2, num_ner_tags)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x)\n",
    "        lstm_out, _ = self.bilstm(embeddings)\n",
    "        ner_logits = self.ner_fc(lstm_out)\n",
    "        return ner_logits\n",
    "\n",
    "    def compute_loss(self, x, ner_tags):\n",
    "        ner_logits = self.forward(x)\n",
    "        loss_fn = nn.CrossEntropyLoss(ignore_index=ner_tag_to_ix[\"<PAD>\"])\n",
    "        ner_loss = loss_fn(ner_logits.view(-1, ner_logits.size(-1)), ner_tags.view(-1))\n",
    "        return ner_loss\n",
    "\n",
    "    def decode(self, x):\n",
    "        ner_logits = self.forward(x)\n",
    "        ner_tags = torch.argmax(ner_logits, dim=-1)\n",
    "        return ner_tags\n",
    "\n",
    "# Load data\n",
    "train_file_path = \"/kaggle/input/split-fix-data/train_v5.conll\"\n",
    "val_file_path = \"/kaggle/input/split-fix-data/val_v5.conll\"\n",
    "test_file_path = \"/kaggle/input/split-fix-data/test_v5.conll\"\n",
    "\n",
    "train_dataset = CoNLLDataset(train_file_path)\n",
    "val_dataset = CoNLLDataset(val_file_path)\n",
    "test_dataset = CoNLLDataset(test_file_path)\n",
    "\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "ner_tag_to_ix = {\"<PAD>\": 0}\n",
    "\n",
    "for dataset in [train_dataset, val_dataset, test_dataset]:\n",
    "    for sentence, ner_tags in zip(dataset.sentences, dataset.ner_tags):\n",
    "        for word in sentence:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "        for ner_tag in ner_tags:\n",
    "            if ner_tag not in ner_tag_to_ix:\n",
    "                ner_tag_to_ix[ner_tag] = len(ner_tag_to_ix)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Initialize model\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "vocab_size = len(vocab)\n",
    "num_ner_tags = len(ner_tag_to_ix)\n",
    "\n",
    "model = BiLSTMSoftmax_NER(vocab_size, embedding_dim, hidden_dim, num_ner_tags).to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training\n",
    "def train_model(model, train_loader, val_loader, epochs):\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for sentences, ner_tags in train_loader:\n",
    "            sentences, ner_tags = sentences.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.compute_loss(sentences, ner_tags)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for sentences, ner_tags in val_loader:\n",
    "                sentences, ner_tags = sentences.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "                val_loss += model.compute_loss(sentences, ner_tags).item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds.\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, epochs=10)\n",
    "\n",
    "# Evaluation with timing\n",
    "model.eval()\n",
    "all_ner_preds, all_ner_targets = [], []\n",
    "prediction_start = time.time()\n",
    "with torch.no_grad():\n",
    "    for sentences, ner_tags in test_loader:\n",
    "        sentences, ner_tags = sentences.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "        ner_preds = model.decode(sentences)\n",
    "\n",
    "        for i in range(len(sentences)):\n",
    "            length = (sentences[i] != vocab[\"<PAD>\"]).sum().item()\n",
    "            all_ner_preds.extend(ner_preds[i, :length].tolist())\n",
    "            all_ner_targets.extend(ner_tags[i, :length].tolist())\n",
    "\n",
    "prediction_end = time.time()\n",
    "prediction_time = prediction_end - prediction_start\n",
    "print(f\"Prediction completed in {prediction_time:.2f} seconds.\")\n",
    "\n",
    "# Map indices to tags\n",
    "idx_to_ner = {v: k for k, v in ner_tag_to_ix.items()}\n",
    "\n",
    "def convert_indices_to_tags(indices, idx_to_tag):\n",
    "    return [idx_to_tag[idx] for idx in indices if idx != ner_tag_to_ix[\"<PAD>\"]]\n",
    "\n",
    "ner_preds_tags = convert_indices_to_tags(all_ner_preds, idx_to_ner)\n",
    "ner_targets_tags = convert_indices_to_tags(all_ner_targets, idx_to_ner)\n",
    "\n",
    "# Print classification report\n",
    "print(\"NER Classification Report:\")\n",
    "print(classification_report(ner_targets_tags, ner_preds_tags,zero_division=0, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T16:55:39.462654Z",
     "iopub.status.busy": "2025-01-27T16:55:39.462193Z",
     "iopub.status.idle": "2025-01-27T16:56:07.685216Z",
     "shell.execute_reply": "2025-01-27T16:56:07.684543Z",
     "shell.execute_reply.started": "2025-01-27T16:55:39.462624Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 125.2499, Val Loss: 6.9331\n",
      "Epoch 2/10, Train Loss: 44.0055, Val Loss: 4.7021\n",
      "Epoch 3/10, Train Loss: 30.0742, Val Loss: 3.8780\n",
      "Epoch 4/10, Train Loss: 21.9689, Val Loss: 3.4411\n",
      "Epoch 5/10, Train Loss: 16.2488, Val Loss: 3.2199\n",
      "Epoch 6/10, Train Loss: 11.5921, Val Loss: 3.1307\n",
      "Epoch 7/10, Train Loss: 8.0973, Val Loss: 3.2098\n",
      "Epoch 8/10, Train Loss: 5.4384, Val Loss: 3.2843\n",
      "Epoch 9/10, Train Loss: 3.6980, Val Loss: 3.3282\n",
      "Epoch 10/10, Train Loss: 2.6161, Val Loss: 3.5715\n",
      "Training completed in 26.84 seconds.\n",
      "Prediction completed in 0.37 seconds.\n",
      "POS Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         abb     1.0000    0.8333    0.9091        18\n",
      "         adj     0.8441    0.8278    0.8358       569\n",
      "         adv     0.8390    0.7612    0.7982       356\n",
      "        conj     0.9302    0.9553    0.9426       739\n",
      "          fw     0.4762    0.3077    0.3738        65\n",
      "         int     0.9375    0.8824    0.9091        17\n",
      "           n     0.9607    0.9685    0.9646      7694\n",
      "         num     0.9706    0.9267    0.9481       641\n",
      "        part     0.9759    0.9807    0.9783      4461\n",
      "         ppm     0.9915    0.9930    0.9922      4114\n",
      "        pron     0.9605    0.9379    0.9491       467\n",
      "        punc     0.9993    1.0000    0.9997      2919\n",
      "          sb     1.0000    0.8462    0.9167        13\n",
      "          tn     0.9518    0.9405    0.9461       168\n",
      "           v     0.9422    0.9428    0.9425      3302\n",
      "\n",
      "    accuracy                         0.9648     25543\n",
      "   macro avg     0.9186    0.8736    0.8937     25543\n",
      "weighted avg     0.9641    0.9648    0.9643     25543\n",
      "\n",
      "NER Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-DATE     0.8421    0.7273    0.7805        66\n",
      "       B-LOC     0.9704    0.9704    0.9704      1182\n",
      "       B-NUM     0.2857    0.1333    0.1818        15\n",
      "       B-ORG     0.6207    0.3750    0.4675        48\n",
      "       B-PER     0.7941    0.7941    0.7941        34\n",
      "      B-TIME     0.7778    0.7778    0.7778         9\n",
      "      E-DATE     0.8000    0.7273    0.7619        66\n",
      "       E-LOC     0.9751    0.9628    0.9689      1182\n",
      "       E-NUM     0.8750    0.4667    0.6087        15\n",
      "       E-ORG     0.5862    0.3542    0.4416        48\n",
      "       E-PER     0.9231    0.7059    0.8000        34\n",
      "      E-TIME     0.8571    0.6667    0.7500         9\n",
      "      I-DATE     0.7692    0.7895    0.7792        38\n",
      "       I-LOC     0.9817    0.9602    0.9709       503\n",
      "       I-ORG     0.6316    0.3077    0.4138        39\n",
      "      I-TIME     0.0000    0.0000    0.0000         0\n",
      "           O     0.9806    0.9921    0.9863     21324\n",
      "      S-DATE     0.9000    0.8182    0.8571        88\n",
      "       S-LOC     0.6321    0.5403    0.5826       124\n",
      "       S-NUM     0.9443    0.8926    0.9177       475\n",
      "       S-ORG     0.7000    0.3333    0.4516        21\n",
      "       S-PER     0.7434    0.5067    0.6027       223\n",
      "      S-TIME     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.9730     25543\n",
      "   macro avg     0.7213    0.6001    0.6463     25543\n",
      "weighted avg     0.9709    0.9730    0.9714     25543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# JOint Traing\n",
    "\n",
    "# Define Dataset Class\n",
    "class CoNLLDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.sentences, self.pos_tags, self.ner_tags = self.load_data(file_path)\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        sentences, pos_tags, ner_tags = [], [], []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            sentence, pos_tag, ner_tag = [], [], []\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    word, pos, ner = line.strip().split(\"\\t\")\n",
    "                    sentence.append(word)\n",
    "                    pos_tag.append(pos)\n",
    "                    ner_tag.append(ner)\n",
    "                else:\n",
    "                    if sentence:\n",
    "                        sentences.append(sentence)\n",
    "                        pos_tags.append(pos_tag)\n",
    "                        ner_tags.append(ner_tag)\n",
    "                    sentence, pos_tag, ner_tag = [], [], []\n",
    "            if sentence:\n",
    "                sentences.append(sentence)\n",
    "                pos_tags.append(pos_tag)\n",
    "                ner_tags.append(ner_tag)\n",
    "        return sentences, pos_tags, ner_tags\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.pos_tags[idx], self.ner_tags[idx]\n",
    "\n",
    "# Collate function for dynamic padding\n",
    "def collate_fn(batch):\n",
    "    sentences, pos_tags, ner_tags = zip(*batch)\n",
    "    max_len = max(len(s) for s in sentences)\n",
    "\n",
    "    sentence_tensors = []\n",
    "    pos_tensors = []\n",
    "    ner_tensors = []\n",
    "\n",
    "    for s, p, n in zip(sentences, pos_tags, ner_tags):\n",
    "        padded_sentence = s + [\"<PAD>\"] * (max_len - len(s))\n",
    "        padded_pos = p + [\"<PAD>\"] * (max_len - len(p))\n",
    "        padded_ner = n + [\"<PAD>\"] * (max_len - len(n))\n",
    "\n",
    "        sentence_tensors.append(torch.tensor([vocab.get(word, vocab[\"<UNK>\"]) for word in padded_sentence], dtype=torch.long))\n",
    "        pos_tensors.append(torch.tensor([pos_tag_to_ix[tag] for tag in padded_pos], dtype=torch.long))\n",
    "        ner_tensors.append(torch.tensor([ner_tag_to_ix[tag] for tag in padded_ner], dtype=torch.long))\n",
    "\n",
    "    return torch.stack(sentence_tensors), torch.stack(pos_tensors), torch.stack(ner_tensors)\n",
    "\n",
    "# Define BiLSTM Model with Softmax\n",
    "class BiLSTMSoftmax_Joint(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_pos_tags, num_ner_tags):\n",
    "        super(BiLSTMSoftmax_Joint, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.pos_fc = nn.Linear(hidden_dim * 2, num_pos_tags)\n",
    "        self.ner_fc = nn.Linear(hidden_dim * 2, num_ner_tags)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x)\n",
    "        lstm_out, _ = self.bilstm(embeddings)\n",
    "        pos_logits = self.pos_fc(lstm_out)\n",
    "        ner_logits = self.ner_fc(lstm_out)\n",
    "        return pos_logits, ner_logits\n",
    "\n",
    "    def compute_loss(self, x, pos_tags, ner_tags, alpha=0.5):\n",
    "        pos_logits, ner_logits = self.forward(x)\n",
    "        pos_loss = nn.CrossEntropyLoss(ignore_index=pos_tag_to_ix[\"<PAD>\"])(pos_logits.view(-1, pos_logits.size(-1)), pos_tags.view(-1))\n",
    "        ner_loss = nn.CrossEntropyLoss(ignore_index=ner_tag_to_ix[\"<PAD>\"])(ner_logits.view(-1, ner_logits.size(-1)), ner_tags.view(-1))\n",
    "        return alpha * pos_loss + (1 - alpha) * ner_loss\n",
    "\n",
    "    def decode(self, x):\n",
    "        pos_logits, ner_logits = self.forward(x)\n",
    "        pos_tags = torch.argmax(pos_logits, dim=-1)\n",
    "        ner_tags = torch.argmax(ner_logits, dim=-1)\n",
    "        return pos_tags, ner_tags\n",
    "\n",
    "# Load data\n",
    "train_file_path = \"/kaggle/input/split-fix-data/train_v5.conll\"\n",
    "val_file_path = \"/kaggle/input/split-fix-data/val_v5.conll\"\n",
    "test_file_path = \"/kaggle/input/split-fix-data/test_v5.conll\"\n",
    "\n",
    "train_dataset = CoNLLDataset(train_file_path)\n",
    "val_dataset = CoNLLDataset(val_file_path)\n",
    "test_dataset = CoNLLDataset(test_file_path)\n",
    "\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "pos_tag_to_ix = {\"<PAD>\": 0}\n",
    "ner_tag_to_ix = {\"<PAD>\": 0}\n",
    "\n",
    "for dataset in [train_dataset, val_dataset, test_dataset]:\n",
    "    for sentence, pos_tags, ner_tags in zip(dataset.sentences, dataset.pos_tags, dataset.ner_tags):\n",
    "        for word in sentence:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "        for pos_tag in pos_tags:\n",
    "            if pos_tag not in pos_tag_to_ix:\n",
    "                pos_tag_to_ix[pos_tag] = len(pos_tag_to_ix)\n",
    "        for ner_tag in ner_tags:\n",
    "            if ner_tag not in ner_tag_to_ix:\n",
    "                ner_tag_to_ix[ner_tag] = len(ner_tag_to_ix)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Initialize model\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "vocab_size = len(vocab)\n",
    "num_pos_tags = len(pos_tag_to_ix)\n",
    "num_ner_tags = len(ner_tag_to_ix)\n",
    "\n",
    "model = BiLSTMSoftmax_Joint(vocab_size, embedding_dim, hidden_dim, num_pos_tags, num_ner_tags).to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training\n",
    "def train_model(model, train_loader, val_loader, epochs):\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for sentences, pos_tags, ner_tags in train_loader:\n",
    "            sentences, pos_tags, ner_tags = sentences.to(\"cuda\"), pos_tags.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.compute_loss(sentences, pos_tags, ner_tags, alpha=0.5)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for sentences, pos_tags, ner_tags in val_loader:\n",
    "                sentences, pos_tags, ner_tags = sentences.to(\"cuda\"), pos_tags.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "                val_loss += model.compute_loss(sentences, pos_tags, ner_tags).item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds.\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, epochs=10)\n",
    "\n",
    "# Evaluation with timing\n",
    "model.eval()\n",
    "all_pos_preds, all_pos_targets, all_ner_preds, all_ner_targets = [], [], [], []\n",
    "prediction_start = time.time()\n",
    "with torch.no_grad():\n",
    "    for sentences, pos_tags, ner_tags in test_loader:\n",
    "        sentences, pos_tags, ner_tags = sentences.to(\"cuda\"), pos_tags.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "        pos_preds, ner_preds = model.decode(sentences)\n",
    "\n",
    "        for i in range(len(sentences)):\n",
    "            length = (sentences[i] != vocab[\"<PAD>\"]).sum().item()\n",
    "            all_pos_preds.extend(pos_preds[i, :length].tolist())\n",
    "            all_ner_preds.extend(ner_preds[i, :length].tolist())\n",
    "            all_pos_targets.extend(pos_tags[i, :length].tolist())\n",
    "            all_ner_targets.extend(ner_tags[i, :length].tolist())\n",
    "\n",
    "prediction_end = time.time()\n",
    "prediction_time = prediction_end - prediction_start\n",
    "print(f\"Prediction completed in {prediction_time:.2f} seconds.\")\n",
    "\n",
    "# Map indices to tags\n",
    "idx_to_pos = {v: k for k, v in pos_tag_to_ix.items()}\n",
    "idx_to_ner = {v: k for k, v in ner_tag_to_ix.items()}\n",
    "\n",
    "def convert_indices_to_tags(indices, idx_to_tag):\n",
    "    return [idx_to_tag[idx] for idx in indices if idx != 0]\n",
    "\n",
    "pos_preds_tags = convert_indices_to_tags(all_pos_preds, idx_to_pos)\n",
    "pos_targets_tags = convert_indices_to_tags(all_pos_targets, idx_to_pos)\n",
    "ner_preds_tags = convert_indices_to_tags(all_ner_preds, idx_to_ner)\n",
    "ner_targets_tags = convert_indices_to_tags(all_ner_targets, idx_to_ner)\n",
    "\n",
    "# Print classification reports\n",
    "print(\"POS Classification Report:\")\n",
    "print(classification_report(pos_targets_tags, pos_preds_tags,zero_division=0, digits=4))\n",
    "\n",
    "print(\"NER Classification Report:\")\n",
    "print(classification_report(ner_targets_tags, ner_preds_tags,zero_division=0, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T16:56:17.891888Z",
     "iopub.status.busy": "2025-01-27T16:56:17.891356Z",
     "iopub.status.idle": "2025-01-27T16:57:06.209850Z",
     "shell.execute_reply": "2025-01-27T16:57:06.208970Z",
     "shell.execute_reply.started": "2025-01-27T16:56:17.891862Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5005, Val Loss: 0.1390\n",
      "Epoch 2/10, Train Loss: 0.1165, Val Loss: 0.1024\n",
      "Epoch 3/10, Train Loss: 0.0699, Val Loss: 0.0864\n",
      "Epoch 4/10, Train Loss: 0.0467, Val Loss: 0.0811\n",
      "Epoch 5/10, Train Loss: 0.0349, Val Loss: 0.0804\n",
      "Epoch 6/10, Train Loss: 0.0267, Val Loss: 0.0837\n",
      "Epoch 7/10, Train Loss: 0.0222, Val Loss: 0.0903\n",
      "Epoch 8/10, Train Loss: 0.0188, Val Loss: 0.0944\n",
      "Epoch 9/10, Train Loss: 0.0160, Val Loss: 0.1002\n",
      "Epoch 10/10, Train Loss: 0.0137, Val Loss: 0.0979\n",
      "Training completed in 33.85 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-80d5e301b717>:173: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed in 0.34 seconds.\n",
      "NER Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-DATE     0.8871    0.8333    0.8594        66\n",
      "       B-LOC     0.9803    0.9695    0.9749      1182\n",
      "       B-NUM     0.2727    0.2000    0.2308        15\n",
      "       B-ORG     0.5088    0.6042    0.5524        48\n",
      "       B-PER     0.7949    0.9118    0.8493        34\n",
      "      B-TIME     0.6667    0.6667    0.6667         9\n",
      "      E-DATE     0.9483    0.8333    0.8871        66\n",
      "       E-LOC     0.9754    0.9721    0.9737      1182\n",
      "       E-NUM     0.4000    0.4000    0.4000        15\n",
      "       E-ORG     0.6667    0.6250    0.6452        48\n",
      "       E-PER     0.8824    0.8824    0.8824        34\n",
      "      E-TIME     0.7778    0.7778    0.7778         9\n",
      "      I-DATE     0.7500    0.8684    0.8049        38\n",
      "       I-LOC     0.9721    0.9682    0.9701       503\n",
      "       I-NUM     0.0000    0.0000    0.0000         0\n",
      "       I-ORG     0.6087    0.3590    0.4516        39\n",
      "      I-TIME     0.0000    0.0000    0.0000         0\n",
      "           O     0.9878    0.9920    0.9899     21324\n",
      "      S-DATE     0.9868    0.8523    0.9146        88\n",
      "       S-LOC     0.7048    0.5968    0.6463       124\n",
      "       S-NUM     0.9494    0.9474    0.9484       475\n",
      "       S-ORG     0.6667    0.3810    0.4848        21\n",
      "       S-PER     0.7374    0.6547    0.6936       223\n",
      "      S-TIME     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.9783     25543\n",
      "   macro avg     0.6719    0.6373    0.6502     25543\n",
      "weighted avg     0.9779    0.9783    0.9779     25543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Single NER with fastText\n",
    "# Load FastText binary model\n",
    "fasttext_bin_file = \"/kaggle/input/glove-100d/cc.my.300.bin\"\n",
    "fasttext_model = load_facebook_model(fasttext_bin_file)\n",
    "fasttext_vectors = fasttext_model.wv\n",
    "\n",
    "# Define Dataset Class\n",
    "class CoNLLDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.sentences, self.pos_tags, self.ner_tags = self.load_data(file_path)\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        sentences, pos_tags, ner_tags = [], [], []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            sentence, pos_tag, ner_tag = [], [], []\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    word, pos, ner = line.strip().split(\"\\t\")\n",
    "                    sentence.append(word)\n",
    "                    pos_tag.append(pos)\n",
    "                    ner_tag.append(ner)\n",
    "                else:\n",
    "                    if sentence:\n",
    "                        sentences.append(sentence)\n",
    "                        pos_tags.append(pos_tag)\n",
    "                        ner_tags.append(ner_tag)\n",
    "                    sentence, pos_tag, ner_tag = [], [], []\n",
    "            if sentence:\n",
    "                sentences.append(sentence)\n",
    "                pos_tags.append(pos_tag)\n",
    "                ner_tags.append(ner_tag)\n",
    "        return sentences, pos_tags, ner_tags\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.pos_tags[idx], self.ner_tags[idx]\n",
    "\n",
    "# Collate function for dynamic padding\n",
    "def collate_fn(batch):\n",
    "    sentences, pos_tags, ner_tags = zip(*batch)\n",
    "    max_len = max(len(s) for s in sentences)\n",
    "\n",
    "    sentence_tensors = []\n",
    "    pos_tensors = []\n",
    "    ner_tensors = []\n",
    "\n",
    "    for s, p, n in zip(sentences, pos_tags, ner_tags):\n",
    "        padded_sentence = s + [\"<PAD>\"] * (max_len - len(s))\n",
    "        padded_pos = p + [\"<PAD>\"] * (max_len - len(p))\n",
    "        padded_ner = n + [\"<PAD>\"] * (max_len - len(n))\n",
    "\n",
    "        sentence_tensors.append(torch.tensor([vocab.get(word, vocab[\"<UNK>\"]) for word in padded_sentence], dtype=torch.long))\n",
    "        pos_tensors.append(torch.tensor([pos_tag_to_ix[tag] for tag in padded_pos], dtype=torch.long))\n",
    "        ner_tensors.append(torch.tensor([ner_tag_to_ix[tag] for tag in padded_ner], dtype=torch.long))\n",
    "\n",
    "    return torch.stack(sentence_tensors), torch.stack(pos_tensors), torch.stack(ner_tensors)\n",
    "\n",
    "# Create vocabulary and tag-to-index mappings\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "pos_tag_to_ix = {\"<PAD>\": 0}\n",
    "ner_tag_to_ix = {\"<PAD>\": 0}\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = CoNLLDataset(train_file_path)\n",
    "val_dataset = CoNLLDataset(val_file_path)\n",
    "test_dataset = CoNLLDataset(test_file_path)\n",
    "\n",
    "# Build vocab and tag mappings\n",
    "for dataset in [train_dataset, val_dataset, test_dataset]:\n",
    "    for sentence, pos_tags, ner_tags in zip(dataset.sentences, dataset.pos_tags, dataset.ner_tags):\n",
    "        for word in sentence:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "        for pos_tag in pos_tags:\n",
    "            if pos_tag not in pos_tag_to_ix:\n",
    "                pos_tag_to_ix[pos_tag] = len(pos_tag_to_ix)\n",
    "        for ner_tag in ner_tags:\n",
    "            if ner_tag not in ner_tag_to_ix:\n",
    "                ner_tag_to_ix[ner_tag] = len(ner_tag_to_ix)\n",
    "\n",
    "# Create embedding matrix using FastText (dimension: 300)\n",
    "embedding_dim = 300\n",
    "fasttext_embeddings = torch.zeros((len(vocab), embedding_dim))\n",
    "\n",
    "for word, idx in vocab.items():\n",
    "    if word in fasttext_vectors:\n",
    "        fasttext_embeddings[idx] = torch.tensor(fasttext_vectors[word])\n",
    "    elif word == \"<PAD>\":\n",
    "        fasttext_embeddings[idx] = torch.zeros(embedding_dim)\n",
    "    else:\n",
    "        fasttext_embeddings[idx] = torch.randn(embedding_dim)\n",
    "\n",
    "# BiLSTM Model with Pre-trained Embeddings\n",
    "class BiLSTMSoftmax_NER(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_ner_tags, fasttext_embeddings):\n",
    "        super(BiLSTMSoftmax_NER, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(fasttext_embeddings, freeze=False)  # Set freeze=False for fine-tuning\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.ner_fc = nn.Linear(hidden_dim * 2, num_ner_tags)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x)\n",
    "        lstm_out, _ = self.bilstm(embeddings)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        ner_logits = self.ner_fc(lstm_out)\n",
    "        return ner_logits\n",
    "\n",
    "    def compute_loss(self, x, ner_tags):\n",
    "        ner_logits = self.forward(x)\n",
    "        loss_fn = nn.CrossEntropyLoss(ignore_index=ner_tag_to_ix[\"<PAD>\"])\n",
    "        ner_loss = loss_fn(ner_logits.view(-1, ner_logits.size(-1)), ner_tags.view(-1))\n",
    "        return ner_loss\n",
    "\n",
    "    def decode(self, x):\n",
    "        ner_logits = self.forward(x)\n",
    "        ner_tags = torch.argmax(ner_logits, dim=-1)\n",
    "        return ner_tags\n",
    "\n",
    "# Initialize Model\n",
    "hidden_dim = 256\n",
    "vocab_size = len(vocab)\n",
    "num_ner_tags = len(ner_tag_to_ix)\n",
    "\n",
    "model = BiLSTMSoftmax_NER(vocab_size, embedding_dim, hidden_dim, num_ner_tags, fasttext_embeddings).to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training\n",
    "def train_model(model, train_loader, val_loader, epochs):\n",
    "    start_time = time.time()\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for sentences, _, ner_tags in train_loader:\n",
    "            sentences, ner_tags = sentences.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.compute_loss(sentences, ner_tags)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for sentences, _, ner_tags in val_loader:\n",
    "                sentences, ner_tags = sentences.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "                val_loss += model.compute_loss(sentences, ner_tags).item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Save the best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), \"single_2.pth\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds.\")\n",
    "\n",
    "# Train the model\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "train_model(model, train_loader, val_loader, epochs=10)\n",
    "\n",
    "# Evaluation with timing\n",
    "model.load_state_dict(torch.load(\"single_2.pth\"))\n",
    "model.eval()\n",
    "all_ner_preds, all_ner_targets = [], []\n",
    "prediction_start = time.time()\n",
    "with torch.no_grad():\n",
    "    for sentences, _, ner_tags in test_loader:\n",
    "        sentences, ner_tags = sentences.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "        ner_preds = model.decode(sentences)\n",
    "\n",
    "        for i in range(len(sentences)):\n",
    "            length = (sentences[i] != vocab[\"<PAD>\"]).sum().item()\n",
    "            all_ner_preds.extend(ner_preds[i, :length].tolist())\n",
    "            all_ner_targets.extend(ner_tags[i, :length].tolist())\n",
    "\n",
    "prediction_end = time.time()\n",
    "prediction_time = prediction_end - prediction_start\n",
    "print(f\"Prediction completed in {prediction_time:.2f} seconds.\")\n",
    "\n",
    "# Map indices to tags\n",
    "idx_to_ner = {v: k for k, v in ner_tag_to_ix.items()}\n",
    "\n",
    "def convert_indices_to_tags(indices, idx_to_tag):\n",
    "    return [idx_to_tag[idx] for idx in indices if idx != ner_tag_to_ix[\"<PAD>\"]]\n",
    "\n",
    "ner_preds_tags = convert_indices_to_tags(all_ner_preds, idx_to_ner)\n",
    "ner_targets_tags = convert_indices_to_tags(all_ner_targets, idx_to_ner)\n",
    "\n",
    "# Print classification report\n",
    "print(\"NER Classification Report:\")\n",
    "print(classification_report(ner_targets_tags, ner_preds_tags, zero_division=0, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T16:57:06.211383Z",
     "iopub.status.busy": "2025-01-27T16:57:06.210995Z",
     "iopub.status.idle": "2025-01-27T16:57:55.651941Z",
     "shell.execute_reply": "2025-01-27T16:57:55.651224Z",
     "shell.execute_reply.started": "2025-01-27T16:57:06.211346Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 1.3696, Val Loss: 0.3542\n",
      "Epoch 2/10, Train Loss: 0.2602, Val Loss: 0.2144\n",
      "Epoch 3/10, Train Loss: 0.1491, Val Loss: 0.1865\n",
      "Epoch 4/10, Train Loss: 0.1066, Val Loss: 0.1726\n",
      "Epoch 5/10, Train Loss: 0.0836, Val Loss: 0.1763\n",
      "Epoch 6/10, Train Loss: 0.0689, Val Loss: 0.1764\n",
      "Epoch 7/10, Train Loss: 0.0578, Val Loss: 0.1748\n",
      "Epoch 8/10, Train Loss: 0.0490, Val Loss: 0.1812\n",
      "Epoch 9/10, Train Loss: 0.0417, Val Loss: 0.1790\n",
      "Epoch 10/10, Train Loss: 0.0369, Val Loss: 0.1918\n",
      "Training completed in 34.58 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-cc3f20be287e>:179: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"/kaggle/working/bilstm_softmax_joint.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed in 0.40 seconds.\n",
      "POS Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         abb     1.0000    0.4444    0.6154        18\n",
      "         adj     0.8946    0.8805    0.8875       569\n",
      "         adv     0.9764    0.8118    0.8865       356\n",
      "        conj     0.9486    0.9486    0.9486       739\n",
      "          fw     0.7826    0.5538    0.6486        65\n",
      "         int     0.9286    0.7647    0.8387        17\n",
      "           n     0.9737    0.9879    0.9808      7694\n",
      "         num     0.9968    0.9626    0.9794       641\n",
      "        part     0.9835    0.9753    0.9794      4461\n",
      "         ppm     0.9880    0.9971    0.9925      4114\n",
      "        pron     0.9655    0.9593    0.9624       467\n",
      "        punc     0.9997    1.0000    0.9998      2919\n",
      "          sb     1.0000    0.9231    0.9600        13\n",
      "          tn     0.9573    0.9345    0.9458       168\n",
      "           v     0.9608    0.9658    0.9633      3302\n",
      "\n",
      "    accuracy                         0.9765     25543\n",
      "   macro avg     0.9571    0.8740    0.9059     25543\n",
      "weighted avg     0.9764    0.9765    0.9762     25543\n",
      "\n",
      "NER Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-DATE     0.8689    0.8030    0.8346        66\n",
      "       B-LOC     0.9761    0.9687    0.9724      1182\n",
      "       B-NUM     0.0000    0.0000    0.0000        15\n",
      "       B-ORG     0.5682    0.5208    0.5435        48\n",
      "       B-PER     0.9032    0.8235    0.8615        34\n",
      "      B-TIME     0.8333    0.5556    0.6667         9\n",
      "      E-DATE     0.9298    0.8030    0.8618        66\n",
      "       E-LOC     0.9786    0.9679    0.9732      1182\n",
      "       E-NUM     0.5714    0.2667    0.3636        15\n",
      "       E-ORG     0.5714    0.5000    0.5333        48\n",
      "       E-PER     0.8529    0.8529    0.8529        34\n",
      "      E-TIME     0.8571    0.6667    0.7500         9\n",
      "      I-DATE     0.7619    0.8421    0.8000        38\n",
      "       I-LOC     0.9645    0.9722    0.9683       503\n",
      "       I-ORG     0.3667    0.2821    0.3188        39\n",
      "           O     0.9846    0.9947    0.9896     21324\n",
      "      S-DATE     1.0000    0.8636    0.9268        88\n",
      "       S-LOC     0.8140    0.5645    0.6667       124\n",
      "       S-NUM     0.9481    0.9621    0.9551       475\n",
      "       S-ORG     0.7143    0.2381    0.3571        21\n",
      "       S-PER     0.8507    0.5112    0.6387       223\n",
      "      S-TIME     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.9780     25543\n",
      "   macro avg     0.7416    0.6345    0.6743     25543\n",
      "weighted avg     0.9762    0.9780    0.9764     25543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Joint Training fastText\n",
    "# Load FastText binary model\n",
    "fasttext_bin_file = \"/kaggle/input/glove-100d/cc.my.300.bin\"\n",
    "fasttext_model = load_facebook_model(fasttext_bin_file)\n",
    "fasttext_vectors = fasttext_model.wv\n",
    "\n",
    "# Define Dataset Class\n",
    "class CoNLLDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.sentences, self.pos_tags, self.ner_tags = self.load_data(file_path)\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        sentences, pos_tags, ner_tags = [], [], []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            sentence, pos_tag, ner_tag = [], [], []\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    word, pos, ner = line.strip().split(\"\\t\")\n",
    "                    sentence.append(word)\n",
    "                    pos_tag.append(pos)\n",
    "                    ner_tag.append(ner)\n",
    "                else:\n",
    "                    if sentence:\n",
    "                        sentences.append(sentence)\n",
    "                        pos_tags.append(pos_tag)\n",
    "                        ner_tags.append(ner_tag)\n",
    "                    sentence, pos_tag, ner_tag = [], [], []\n",
    "            if sentence:\n",
    "                sentences.append(sentence)\n",
    "                pos_tags.append(pos_tag)\n",
    "                ner_tags.append(ner_tag)\n",
    "        return sentences, pos_tags, ner_tags\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.pos_tags[idx], self.ner_tags[idx]\n",
    "\n",
    "# Collate function for dynamic padding\n",
    "def collate_fn(batch):\n",
    "    sentences, pos_tags, ner_tags = zip(*batch)\n",
    "    max_len = max(len(s) for s in sentences)\n",
    "\n",
    "    sentence_tensors = []\n",
    "    pos_tensors = []\n",
    "    ner_tensors = []\n",
    "\n",
    "    for s, p, n in zip(sentences, pos_tags, ner_tags):\n",
    "        padded_sentence = s + [\"<PAD>\"] * (max_len - len(s))\n",
    "        padded_pos = p + [\"<PAD>\"] * (max_len - len(p))\n",
    "        padded_ner = n + [\"<PAD>\"] * (max_len - len(n))\n",
    "\n",
    "        sentence_tensors.append(torch.tensor([vocab.get(word, vocab[\"<UNK>\"]) for word in padded_sentence], dtype=torch.long))\n",
    "        pos_tensors.append(torch.tensor([pos_tag_to_ix[tag] for tag in padded_pos], dtype=torch.long))\n",
    "        ner_tensors.append(torch.tensor([ner_tag_to_ix[tag] for tag in padded_ner], dtype=torch.long))\n",
    "\n",
    "    return torch.stack(sentence_tensors), torch.stack(pos_tensors), torch.stack(ner_tensors)\n",
    "\n",
    "# Create vocabulary and tag-to-index mappings\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "pos_tag_to_ix = {\"<PAD>\": 0}\n",
    "ner_tag_to_ix = {\"<PAD>\": 0}\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = CoNLLDataset(train_file_path)\n",
    "val_dataset = CoNLLDataset(val_file_path)\n",
    "test_dataset = CoNLLDataset(test_file_path)\n",
    "\n",
    "# Build vocab and tag mappings\n",
    "for dataset in [train_dataset, val_dataset, test_dataset]:\n",
    "    for sentence, pos_tags, ner_tags in zip(dataset.sentences, dataset.pos_tags, dataset.ner_tags):\n",
    "        for word in sentence:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "        for pos_tag in pos_tags:\n",
    "            if pos_tag not in pos_tag_to_ix:\n",
    "                pos_tag_to_ix[pos_tag] = len(pos_tag_to_ix)\n",
    "        for ner_tag in ner_tags:\n",
    "            if ner_tag not in ner_tag_to_ix:\n",
    "                ner_tag_to_ix[ner_tag] = len(ner_tag_to_ix)\n",
    "\n",
    "# Create embedding matrix using FastText (dimension: 300)\n",
    "embedding_dim = 300\n",
    "fasttext_embeddings = torch.zeros((len(vocab), embedding_dim))\n",
    "\n",
    "for word, idx in vocab.items():\n",
    "    if word in fasttext_vectors:\n",
    "        fasttext_embeddings[idx] = torch.tensor(fasttext_vectors[word])\n",
    "    elif word == \"<PAD>\":\n",
    "        fasttext_embeddings[idx] = torch.zeros(embedding_dim)\n",
    "    else:\n",
    "        fasttext_embeddings[idx] = torch.randn(embedding_dim)\n",
    "\n",
    "# BiLSTM Model with Pre-trained Embeddings\n",
    "class BiLSTMJointPOSNER(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_pos_tags, num_ner_tags, fasttext_embeddings):\n",
    "        super(BiLSTMJointPOSNER, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(fasttext_embeddings, freeze=False)  # Set freeze=False for fine-tuning\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.pos_fc = nn.Linear(hidden_dim * 2, num_pos_tags)\n",
    "        self.ner_fc = nn.Linear(hidden_dim * 2, num_ner_tags)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x)\n",
    "        lstm_out, _ = self.bilstm(embeddings)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        pos_logits = self.pos_fc(lstm_out)\n",
    "        ner_logits = self.ner_fc(lstm_out)\n",
    "        return pos_logits, ner_logits\n",
    "\n",
    "    def compute_loss(self, x, pos_tags, ner_tags):\n",
    "        pos_logits, ner_logits = self.forward(x)\n",
    "        pos_loss_fn = nn.CrossEntropyLoss(ignore_index=pos_tag_to_ix[\"<PAD>\"])\n",
    "        ner_loss_fn = nn.CrossEntropyLoss(ignore_index=ner_tag_to_ix[\"<PAD>\"])\n",
    "        pos_loss = pos_loss_fn(pos_logits.view(-1, pos_logits.size(-1)), pos_tags.view(-1))\n",
    "        ner_loss = ner_loss_fn(ner_logits.view(-1, ner_logits.size(-1)), ner_tags.view(-1))\n",
    "        return pos_loss + ner_loss\n",
    "\n",
    "    def decode(self, x):\n",
    "        pos_logits, ner_logits = self.forward(x)\n",
    "        pos_tags = torch.argmax(pos_logits, dim=-1)\n",
    "        ner_tags = torch.argmax(ner_logits, dim=-1)\n",
    "        return pos_tags, ner_tags\n",
    "\n",
    "# Initialize Model\n",
    "hidden_dim = 256\n",
    "vocab_size = len(vocab)\n",
    "num_pos_tags = len(pos_tag_to_ix)\n",
    "num_ner_tags = len(ner_tag_to_ix)\n",
    "\n",
    "model = BiLSTMJointPOSNER(vocab_size, embedding_dim, hidden_dim, num_pos_tags, num_ner_tags, fasttext_embeddings).to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training\n",
    "def train_model(model, train_loader, val_loader, epochs):\n",
    "    start_time = time.time()\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for sentences, pos_tags, ner_tags in train_loader:\n",
    "            sentences, pos_tags, ner_tags = sentences.to(\"cuda\"), pos_tags.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.compute_loss(sentences, pos_tags, ner_tags)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for sentences, pos_tags, ner_tags in val_loader:\n",
    "                sentences, pos_tags, ner_tags = sentences.to(\"cuda\"), pos_tags.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "                val_loss += model.compute_loss(sentences, pos_tags, ner_tags).item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Save the best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), \"/kaggle/working/bilstm_softmax_joint.pth\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds.\")\n",
    "\n",
    "# Train the model\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "train_model(model, train_loader, val_loader, epochs=10)\n",
    "\n",
    "# Evaluation with timing\n",
    "model.load_state_dict(torch.load(\"/kaggle/working/bilstm_softmax_joint.pth\"))\n",
    "model.eval()\n",
    "all_pos_preds, all_pos_targets = [], []\n",
    "all_ner_preds, all_ner_targets = [], []\n",
    "\n",
    "prediction_start = time.time()\n",
    "with torch.no_grad():\n",
    "    for sentences, pos_tags, ner_tags in test_loader:\n",
    "        sentences, pos_tags, ner_tags = sentences.to(\"cuda\"), pos_tags.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "        pos_preds, ner_preds = model.decode(sentences)\n",
    "\n",
    "        for i in range(len(sentences)):\n",
    "            length = (sentences[i] != vocab[\"<PAD>\"]).sum().item()\n",
    "            all_pos_preds.extend(pos_preds[i, :length].tolist())\n",
    "            all_pos_targets.extend(pos_tags[i, :length].tolist())\n",
    "            all_ner_preds.extend(ner_preds[i, :length].tolist())\n",
    "            all_ner_targets.extend(ner_tags[i, :length].tolist())\n",
    "\n",
    "prediction_end = time.time()\n",
    "prediction_time = prediction_end - prediction_start\n",
    "print(f\"Prediction completed in {prediction_time:.2f} seconds.\")\n",
    "\n",
    "# Map indices to tags\n",
    "idx_to_pos = {v: k for k, v in pos_tag_to_ix.items()}\n",
    "idx_to_ner = {v: k for k, v in ner_tag_to_ix.items()}\n",
    "\n",
    "def convert_indices_to_tags(indices, idx_to_tag):\n",
    "    return [idx_to_tag[idx] for idx in indices if idx != pos_tag_to_ix[\"<PAD>\"]]\n",
    "\n",
    "pos_preds_tags = convert_indices_to_tags(all_pos_preds, idx_to_pos)\n",
    "pos_targets_tags = convert_indices_to_tags(all_pos_targets, idx_to_pos)\n",
    "ner_preds_tags = convert_indices_to_tags(all_ner_preds, idx_to_ner)\n",
    "ner_targets_tags = convert_indices_to_tags(all_ner_targets, idx_to_ner)\n",
    "\n",
    "# Print classification reports\n",
    "print(\"POS Classification Report:\")\n",
    "print(classification_report(pos_targets_tags, pos_preds_tags,zero_division=0, digits=4))\n",
    "\n",
    "print(\"NER Classification Report:\")\n",
    "print(classification_report(ner_targets_tags, ner_preds_tags,zero_division=0, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6515326,
     "sourceId": 10566039,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6521472,
     "sourceId": 10583082,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
