{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-20T15:49:10.912637Z",
     "iopub.status.busy": "2025-03-20T15:49:10.912251Z",
     "iopub.status.idle": "2025-03-20T15:49:15.493623Z",
     "shell.execute_reply": "2025-03-20T15:49:15.492819Z",
     "shell.execute_reply.started": "2025-03-20T15:49:10.912601Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
      "Collecting sklearn-crfsuite\n",
      "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
      "Collecting python-crfsuite>=0.9.7 (from sklearn-crfsuite)\n",
      "  Downloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite) (0.9.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\n",
      "Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: python-crfsuite, sklearn-crfsuite\n",
      "Successfully installed python-crfsuite-0.9.11 sklearn-crfsuite-0.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn sklearn-crfsuite gensim numpy pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T11:01:03.350665Z",
     "iopub.status.busy": "2025-01-27T11:01:03.350265Z",
     "iopub.status.idle": "2025-01-27T11:01:03.355276Z",
     "shell.execute_reply": "2025-01-27T11:01:03.353954Z",
     "shell.execute_reply.started": "2025-01-27T11:01:03.350618Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import platform\n",
    "import time\n",
    "import sys\n",
    "import sklearn\n",
    "import gensim\n",
    "from importlib.metadata import version, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T11:01:06.344957Z",
     "iopub.status.busy": "2025-01-27T11:01:06.344635Z",
     "iopub.status.idle": "2025-01-27T11:03:03.782991Z",
     "shell.execute_reply": "2025-01-27T11:03:03.781805Z",
     "shell.execute_reply.started": "2025-01-27T11:01:06.344929Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Environment Details ===\n",
      "Python Version: 3.10.16 (main, Dec 25 2024, 01:31:21) [GCC 12.2.0]\n",
      "Platform: Linux 6.1.42+\n",
      "scikit-learn Version: 1.6.1\n",
      "sklearn-crfsuite version: 0.5.0\n",
      "Gensim Version: 4.3.3\n",
      "===========================\n",
      "Loaded 335230 words from FastText.\n",
      "Training CRF model for NER...\n",
      "Training completed in 52.29 seconds.\n",
      "NER Validation F1-score: 0.9776269694795953\n",
      "\n",
      "NER Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-DATE     0.8833    0.8030    0.8413        66\n",
      "       B-LOC     0.9837    0.9695    0.9766      1182\n",
      "       B-NUM     0.5000    0.3333    0.4000        15\n",
      "       B-ORG     0.7179    0.5833    0.6437        48\n",
      "       B-PER     0.9375    0.8824    0.9091        34\n",
      "      B-TIME     0.8750    0.7778    0.8235         9\n",
      "      E-DATE     0.8710    0.8182    0.8438        66\n",
      "       E-LOC     0.9687    0.9687    0.9687      1182\n",
      "       E-NUM     0.5000    0.3333    0.4000        15\n",
      "       E-ORG     0.7436    0.6042    0.6667        48\n",
      "       E-PER     0.9375    0.8824    0.9091        34\n",
      "      E-TIME     0.8750    0.7778    0.8235         9\n",
      "      I-DATE     0.7561    0.8158    0.7848        38\n",
      "       I-LOC     0.9833    0.9364    0.9593       503\n",
      "       I-ORG     0.7037    0.4872    0.5758        39\n",
      "           O     0.9882    0.9956    0.9919     21324\n",
      "      S-DATE     0.9744    0.8636    0.9157        88\n",
      "       S-LOC     0.7600    0.6129    0.6786       124\n",
      "       S-NUM     0.9490    0.9789    0.9637       475\n",
      "       S-ORG     0.5714    0.3810    0.4571        21\n",
      "       S-PER     0.9106    0.7309    0.8109       223\n",
      "      S-TIME     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.9818     25543\n",
      "   macro avg     0.7905    0.7062    0.7429     25543\n",
      "weighted avg     0.9810    0.9818    0.9811     25543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fasttext CRF Model for NER Training and Prediction\n",
    "def check_environment():\n",
    "    print(\"=== Environment Details ===\")\n",
    "    print(f\"Python Version: {sys.version}\")\n",
    "    print(f\"Platform: {platform.system()} {platform.release()}\")\n",
    "    print(f\"scikit-learn Version: {sklearn.__version__}\")\n",
    "    print(f\"sklearn-crfsuite version: {pkg_resources.get_distribution('sklearn-crfsuite').version}\")\n",
    "    print(f\"Gensim Version: {gensim.__version__}\")\n",
    "    print(\"===========================\")\n",
    "\n",
    "# Check environment\n",
    "check_environment()\n",
    "\n",
    "# Load FastText embeddings\n",
    "fasttext_file = \"/kaggle/input/glove-100d/cc.my.300.vec\"  \n",
    "fasttext_model = KeyedVectors.load_word2vec_format(fasttext_file)\n",
    "print(f\"Loaded {len(fasttext_model)} words from FastText.\")\n",
    "\n",
    "# Read CoNLL data\n",
    "def read_conll(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in lines:\n",
    "        if line.strip() == \"\":\n",
    "            if sentence:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "        else:\n",
    "            token, pos, ner = line.strip().split('\\t')\n",
    "            sentence.append((token, pos, ner))\n",
    "\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "def is_numeric(token):\n",
    "    numeric_chars = set(\"၁၂၃၄၅၆၇၈၉၀\")\n",
    "    return token.isdigit() or all(char in numeric_chars for char in token)\n",
    "\n",
    "# Feature extraction with FastText embeddings\n",
    "def extract_features(sentence, index):\n",
    "    token = sentence[index][0]\n",
    "    features = {\n",
    "        'word': token,\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'prefix-1': token[0],\n",
    "        'prefix-2': token[:2],\n",
    "        'prefix-3': token[:3],\n",
    "        'suffix-1': token[-1],\n",
    "        'suffix-2': token[-2:],\n",
    "        'suffix-3': token[-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1][0],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1][0],\n",
    "        'has_hyphen': '-' in token,\n",
    "        'is_numeric': is_numeric(token),  # Use the combined numeric check\n",
    "    }\n",
    "    if token in fasttext_model:\n",
    "        features['fasttext_avg'] = np.mean(fasttext_model[token])\n",
    "    else:\n",
    "        features['fasttext_avg'] = 0.0  \n",
    "    return features\n",
    "\n",
    "def prepare_data(conll_data):\n",
    "    X = []\n",
    "    y_ner = []\n",
    "    for sentence in conll_data:\n",
    "        X_sentence = []\n",
    "        y_ner_sentence = []\n",
    "        for i in range(len(sentence)):\n",
    "            X_sentence.append(extract_features(sentence, i))\n",
    "            y_ner_sentence.append(sentence[i][2])  # NER label\n",
    "        X.append(X_sentence)\n",
    "        y_ner.append(y_ner_sentence)\n",
    "    return X, y_ner\n",
    "\n",
    "train_file_path = \"/kaggle/input/split-fix-data/train_v5.conll\"\n",
    "val_file_path = \"/kaggle/input/split-fix-data/val_v5.conll\"\n",
    "test_file_path = \"/kaggle/input/split-fix-data/test_v5.conll\"\n",
    "\n",
    "train_data = read_conll(train_file_path)\n",
    "val_data = read_conll(val_file_path)\n",
    "test_data = read_conll(test_file_path)\n",
    "\n",
    "X_train, y_train_ner = prepare_data(train_data)\n",
    "X_val, y_val_ner = prepare_data(val_data)\n",
    "X_test, y_test_ner = prepare_data(test_data)\n",
    "\n",
    "crf_ner = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,  # L1 regularization\n",
    "    c2=0.1,  # L2 regularization\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "# Measure training time\n",
    "start_time = time.time()\n",
    "print(\"Training CRF model for NER...\")\n",
    "crf_ner.fit(X_train, y_train_ner)\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds.\")\n",
    "\n",
    "# Validate NER model\n",
    "print(\"NER Validation F1-score:\", metrics.flat_f1_score(y_val_ner, crf_ner.predict(X_val), average='weighted'))\n",
    "\n",
    "# Test NER model\n",
    "y_test_pred_ner = crf_ner.predict(X_test)\n",
    "\n",
    "# Flatten the true and predicted labels for classification report\n",
    "y_test_ner_flat = [label for sent in y_test_ner for label in sent]\n",
    "y_test_pred_ner_flat = [label for sent in y_test_pred_ner for label in sent]\n",
    "\n",
    "# Generate classification report for NER\n",
    "print(\"\\nNER Classification Report:\")\n",
    "print(classification_report(y_test_ner_flat, y_test_pred_ner_flat, zero_division=0,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T11:03:27.769085Z",
     "iopub.status.busy": "2025-01-27T11:03:27.768782Z",
     "iopub.status.idle": "2025-01-27T11:04:20.682969Z",
     "shell.execute_reply": "2025-01-27T11:04:20.681138Z",
     "shell.execute_reply.started": "2025-01-27T11:03:27.769058Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Environment Details ===\n",
      "Python Version: 3.10.16 (main, Dec 25 2024, 01:31:21) [GCC 12.2.0]\n",
      "Platform: Linux 6.1.42+\n",
      "scikit-learn Version: 1.6.1\n",
      "sklearn-crfsuite version: 0.5.0\n",
      "===========================\n",
      "Training CRF model for NER...\n",
      "Training completed in 51.00 seconds.\n",
      "NER Validation F1-score: 0.9781392852758948\n",
      "\n",
      "NER Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-DATE     0.8667    0.7879    0.8254        66\n",
      "       B-LOC     0.9837    0.9712    0.9774      1182\n",
      "       B-NUM     0.5000    0.3333    0.4000        15\n",
      "       B-ORG     0.6667    0.5833    0.6222        48\n",
      "       B-PER     0.9394    0.9118    0.9254        34\n",
      "      B-TIME     0.8750    0.7778    0.8235         9\n",
      "      E-DATE     0.8710    0.8182    0.8438        66\n",
      "       E-LOC     0.9688    0.9704    0.9696      1182\n",
      "       E-NUM     0.5000    0.3333    0.4000        15\n",
      "       E-ORG     0.6905    0.6042    0.6444        48\n",
      "       E-PER     0.9394    0.9118    0.9254        34\n",
      "      E-TIME     0.8750    0.7778    0.8235         9\n",
      "      I-DATE     0.7500    0.7895    0.7692        38\n",
      "       I-LOC     0.9833    0.9364    0.9593       503\n",
      "       I-ORG     0.6552    0.4872    0.5588        39\n",
      "           O     0.9886    0.9954    0.9920     21324\n",
      "      S-DATE     0.9744    0.8636    0.9157        88\n",
      "       S-LOC     0.7624    0.6210    0.6844       124\n",
      "       S-NUM     0.9490    0.9789    0.9637       475\n",
      "       S-ORG     0.5714    0.3810    0.4571        21\n",
      "       S-PER     0.9106    0.7309    0.8109       223\n",
      "      S-TIME     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.9818     25543\n",
      "   macro avg     0.7828    0.7075    0.7405     25543\n",
      "weighted avg     0.9810    0.9818    0.9812     25543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Base CRF Model for NER Training and Prediction\n",
    "def check_environment():\n",
    "    print(\"=== Environment Details ===\")\n",
    "    print(f\"Python Version: {sys.version}\")\n",
    "    print(f\"Platform: {platform.system()} {platform.release()}\")\n",
    "    print(f\"scikit-learn Version: {sklearn.__version__}\")\n",
    "    print(f\"sklearn-crfsuite version: {pkg_resources.get_distribution('sklearn-crfsuite').version}\")\n",
    "    print(\"===========================\")\n",
    "\n",
    "# Check environment\n",
    "check_environment()\n",
    "\n",
    "# Read CoNLL data\n",
    "def read_conll(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in lines:\n",
    "        if line.strip() == \"\":\n",
    "            if sentence:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "        else:\n",
    "            token, pos, ner = line.strip().split('\\t')\n",
    "            sentence.append((token, pos, ner))\n",
    "\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "def is_numeric(token):\n",
    "    numeric_chars = set(\"၁၂၃၄၅၆၇၈၉၀\")\n",
    "    return token.isdigit() or all(char in numeric_chars for char in token)\n",
    "\n",
    "# Feature extraction without FastText embeddings\n",
    "def extract_features(sentence, index):\n",
    "    token = sentence[index][0]\n",
    "    features = {\n",
    "        'word': token,\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'prefix-1': token[0],\n",
    "        'prefix-2': token[:2],\n",
    "        'prefix-3': token[:3],\n",
    "        'suffix-1': token[-1],\n",
    "        'suffix-2': token[-2:],\n",
    "        'suffix-3': token[-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1][0],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1][0],\n",
    "        'has_hyphen': '-' in token,\n",
    "        'is_numeric': is_numeric(token),  # Use the combined numeric check\n",
    "    }\n",
    "    return features\n",
    "\n",
    "def prepare_data(conll_data):\n",
    "    X = []\n",
    "    y_ner = []\n",
    "    for sentence in conll_data:\n",
    "        X_sentence = []\n",
    "        y_ner_sentence = []\n",
    "        for i in range(len(sentence)):\n",
    "            X_sentence.append(extract_features(sentence, i))\n",
    "            y_ner_sentence.append(sentence[i][2])  # NER label\n",
    "        X.append(X_sentence)\n",
    "        y_ner.append(y_ner_sentence)\n",
    "    return X, y_ner\n",
    "\n",
    "train_file_path = \"/kaggle/input/split-fix-data/train_v5.conll\"\n",
    "val_file_path = \"/kaggle/input/split-fix-data/val_v5.conll\"\n",
    "test_file_path = \"/kaggle/input/split-fix-data/test_v5.conll\"\n",
    "\n",
    "train_data = read_conll(train_file_path)\n",
    "val_data = read_conll(val_file_path)\n",
    "test_data = read_conll(test_file_path)\n",
    "\n",
    "X_train, y_train_ner = prepare_data(train_data)\n",
    "X_val, y_val_ner = prepare_data(val_data)\n",
    "X_test, y_test_ner = prepare_data(test_data)\n",
    "\n",
    "crf_ner = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,  # L1 regularization\n",
    "    c2=0.1,  # L2 regularization\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "# Measure training time\n",
    "start_time = time.time()\n",
    "print(\"Training CRF model for NER...\")\n",
    "crf_ner.fit(X_train, y_train_ner)\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds.\")\n",
    "\n",
    "# Validate NER model\n",
    "print(\"NER Validation F1-score:\", metrics.flat_f1_score(y_val_ner, crf_ner.predict(X_val), average='weighted'))\n",
    "\n",
    "# Test NER model\n",
    "y_test_pred_ner = crf_ner.predict(X_test)\n",
    "\n",
    "# Flatten the true and predicted labels for classification report\n",
    "y_test_ner_flat = [label for sent in y_test_ner for label in sent]\n",
    "y_test_pred_ner_flat = [label for sent in y_test_pred_ner for label in sent]\n",
    "\n",
    "# Generate classification report for NER\n",
    "print(\"\\nNER Classification Report:\")\n",
    "print(classification_report(y_test_ner_flat, y_test_pred_ner_flat, zero_division=0, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T11:17:57.516485Z",
     "iopub.status.busy": "2025-01-27T11:17:57.516180Z",
     "iopub.status.idle": "2025-01-27T11:38:31.528002Z",
     "shell.execute_reply": "2025-01-27T11:38:31.526445Z",
     "shell.execute_reply.started": "2025-01-27T11:17:57.516461Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Environment Details ===\n",
      "Python Version: 3.10.16 (main, Dec 25 2024, 01:31:21) [GCC 12.2.0]\n",
      "Platform: Linux 6.1.42+\n",
      "scikit-learn Version: 1.6.1\n",
      "sklearn-crfsuite version: 0.5.0\n",
      "===========================\n",
      "Loaded 335230 words from FastText.\n",
      "Training joint CRF model...\n",
      "Joint Validation F1-score: 0.9576719339780975\n",
      "\n",
      "POS Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         abb     1.0000    0.9444    0.9714        18\n",
      "         adj     0.8875    0.8735    0.8804       569\n",
      "         adv     0.9470    0.8034    0.8693       356\n",
      "        conj     0.9462    0.9513    0.9487       739\n",
      "          fw     0.9655    0.8615    0.9106        65\n",
      "         int     0.9412    0.9412    0.9412        17\n",
      "           n     0.9783    0.9840    0.9811      7694\n",
      "         num     0.9984    0.9984    0.9984       641\n",
      "        part     0.9781    0.9827    0.9804      4461\n",
      "         ppm     0.9934    0.9947    0.9940      4114\n",
      "        pron     0.9699    0.9657    0.9678       467\n",
      "        punc     1.0000    1.0000    1.0000      2919\n",
      "          sb     1.0000    0.9231    0.9600        13\n",
      "          tn     0.9521    0.9464    0.9493       168\n",
      "           v     0.9546    0.9546    0.9546      3302\n",
      "\n",
      "    accuracy                         0.9770     25543\n",
      "   macro avg     0.9675    0.9417    0.9538     25543\n",
      "weighted avg     0.9769    0.9770    0.9768     25543\n",
      "\n",
      "\n",
      "NER Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-DATE     0.8814    0.7879    0.8320        66\n",
      "       B-LOC     0.9770    0.9712    0.9741      1182\n",
      "       B-NUM     0.4615    0.4000    0.4286        15\n",
      "       B-ORG     0.6667    0.5417    0.5977        48\n",
      "       B-PER     0.9118    0.9118    0.9118        34\n",
      "      B-TIME     0.8571    0.6667    0.7500         9\n",
      "      E-DATE     0.8852    0.8182    0.8504        66\n",
      "       E-LOC     0.9664    0.9729    0.9696      1182\n",
      "       E-NUM     0.4615    0.4000    0.4286        15\n",
      "       E-ORG     0.6923    0.5625    0.6207        48\n",
      "       E-PER     0.9118    0.9118    0.9118        34\n",
      "      E-TIME     0.8571    0.6667    0.7500         9\n",
      "      I-DATE     0.7857    0.8684    0.8250        38\n",
      "       I-LOC     0.9833    0.9364    0.9593       503\n",
      "       I-ORG     0.6250    0.3846    0.4762        39\n",
      "           O     0.9888    0.9939    0.9913     21324\n",
      "      S-DATE     0.9747    0.8750    0.9222        88\n",
      "       S-LOC     0.7290    0.6290    0.6753       124\n",
      "       S-NUM     0.9509    0.9789    0.9647       475\n",
      "       S-ORG     0.7273    0.3810    0.5000        21\n",
      "       S-PER     0.8657    0.7803    0.8208       223\n",
      "      S-TIME     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.9810     25543\n",
      "   macro avg     0.7800    0.7018    0.7345     25543\n",
      "weighted avg     0.9803    0.9810    0.9804     25543\n",
      "\n",
      "\n",
      "=== 5 Predicted Sentences from Test Set ===\n",
      "\n",
      "Sentence 1:\n",
      "Token\t\tTrue POS\tPredicted POS\tTrue NER\tPredicted NER\n",
      "------------------------------------------------------------\n",
      "ရွာ\t\tn\t\tn\t\tO\t\tO\n",
      "နေရာ\t\tn\t\tn\t\tO\t\tO\n",
      "ကုတ်\t\tn\t\tn\t\tO\t\tO\n",
      "မှာ\t\tppm\t\tppm\t\tO\t\tO\n",
      "၂၁၃၉၈၃\t\tnum\t\tnum\t\tS-NUM\t\tS-NUM\n",
      "ဖြစ်\t\tv\t\tv\t\tO\t\tO\n",
      "သည်\t\tppm\t\tppm\t\tO\t\tO\n",
      "။\t\tpunc\t\tpunc\t\tO\t\tO\n",
      "\n",
      "Sentence 2:\n",
      "Token\t\tTrue POS\tPredicted POS\tTrue NER\tPredicted NER\n",
      "------------------------------------------------------------\n",
      "ကျောက်ဖြူတိုင်\t\tn\t\tn\t\tB-LOC\t\tB-LOC\n",
      "ရွာ\t\tn\t\tn\t\tE-LOC\t\tE-LOC\n",
      "သည်\t\tppm\t\tppm\t\tO\t\tO\n",
      "စစ်ကိုင်း\t\tn\t\tn\t\tB-LOC\t\tB-LOC\n",
      "တိုင်း\t\tpart\t\tpart\t\tI-LOC\t\tI-LOC\n",
      "ဒေသကြီး\t\tn\t\tn\t\tE-LOC\t\tE-LOC\n",
      "၊\t\tpunc\t\tpunc\t\tO\t\tO\n",
      "ယင်းမာပင်\t\tn\t\tn\t\tB-LOC\t\tB-LOC\n",
      "ခရိုင်\t\tn\t\tn\t\tE-LOC\t\tE-LOC\n",
      "၊\t\tpunc\t\tpunc\t\tO\t\tO\n",
      "ဆားလင်းကြီး\t\tn\t\tn\t\tB-LOC\t\tB-LOC\n",
      "မြို့နယ်\t\tn\t\tn\t\tE-LOC\t\tE-LOC\n",
      "၊\t\tpunc\t\tpunc\t\tO\t\tO\n",
      "ဖောင်းကတာ\t\tn\t\tn\t\tB-LOC\t\tB-LOC\n",
      "ကျေးရွာ\t\tn\t\tn\t\tI-LOC\t\tI-LOC\n",
      "အုပ်စု\t\tn\t\tn\t\tE-LOC\t\tE-LOC\n",
      "၌\t\tppm\t\tppm\t\tO\t\tO\n",
      "တည်ရှိ\t\tv\t\tv\t\tO\t\tO\n",
      "သည်\t\tppm\t\tppm\t\tO\t\tO\n",
      "။\t\tpunc\t\tpunc\t\tO\t\tO\n",
      "\n",
      "Sentence 3:\n",
      "Token\t\tTrue POS\tPredicted POS\tTrue NER\tPredicted NER\n",
      "------------------------------------------------------------\n",
      "ကြက်ခြေနီ\t\tn\t\tn\t\tS-ORG\t\tO\n",
      "မှ\t\tppm\t\tppm\t\tO\t\tO\n",
      "ပြောရေးဆိုခွင့်ရှိသူ\t\tn\t\tn\t\tO\t\tO\n",
      "MattCochrane\t\tfw\t\tfw\t\tS-PER\t\tO\n",
      "မော့တ်ကော့ချရင်း\t\tn\t\tn\t\tS-PER\t\tO\n",
      "က\t\tppm\t\tppm\t\tO\t\tO\n",
      "ထို\t\tadj\t\tadj\t\tO\t\tO\n",
      "ဆိုင်ကလုန်း\t\tn\t\tn\t\tO\t\tO\n",
      "ကြောင့်\t\tppm\t\tppm\t\tO\t\tO\n",
      "သေကံမရောက်သက်မပျောက်\t\tv\t\tv\t\tO\t\tO\n",
      "ဘဲ\t\tpart\t\tpart\t\tO\t\tO\n",
      "ကျန်ရစ်\t\tn\t\tn\t\tO\t\tO\n",
      "များ\t\tpart\t\tpart\t\tO\t\tO\n",
      "သည်\t\tppm\t\tppm\t\tO\t\tO\n",
      "အလုံးစုံ\t\tn\t\tadv\t\tO\t\tO\n",
      "လိုအပ်\t\tv\t\tv\t\tO\t\tO\n",
      "သည်\t\tppm\t\tppm\t\tO\t\tO\n",
      "ဟု\t\tpart\t\tpart\t\tO\t\tO\n",
      "ပြော\t\tv\t\tv\t\tO\t\tO\n",
      "ခဲ့\t\tpart\t\tpart\t\tO\t\tO\n",
      "သည်\t\tppm\t\tppm\t\tO\t\tO\n",
      "။\t\tpunc\t\tpunc\t\tO\t\tO\n",
      "\n",
      "Sentence 4:\n",
      "Token\t\tTrue POS\tPredicted POS\tTrue NER\tPredicted NER\n",
      "------------------------------------------------------------\n",
      "အေးသဇင်ဟန်\t\tn\t\tn\t\tS-PER\t\tS-PER\n",
      "သည်\t\tppm\t\tppm\t\tO\t\tO\n",
      "သူ\t\tpron\t\tpron\t\tO\t\tO\n",
      "၏\t\tppm\t\tppm\t\tO\t\tO\n",
      "အားလပ်\t\tv\t\tv\t\tO\t\tO\n",
      "ရက်\t\tn\t\tn\t\tO\t\tO\n",
      "တွင်\t\tppm\t\tppm\t\tO\t\tO\n",
      "ဒေသ\t\tn\t\tn\t\tO\t\tO\n",
      "တွင်း\t\tn\t\tppm\t\tO\t\tO\n",
      "ပရဟိတ\t\tn\t\tv\t\tO\t\tO\n",
      "ပွဲ\t\tn\t\tn\t\tO\t\tO\n",
      "တစ်\t\ttn\t\ttn\t\tO\t\tO\n",
      "ခု\t\tpart\t\tpart\t\tO\t\tO\n",
      "တွင်\t\tppm\t\tppm\t\tO\t\tO\n",
      "စေတနာ့\t\tn\t\tn\t\tO\t\tO\n",
      "ဝန်ထမ်း\t\tn\t\tn\t\tO\t\tO\n",
      "လုပ်\t\tv\t\tv\t\tO\t\tO\n",
      "ခဲ့\t\tpart\t\tpart\t\tO\t\tO\n",
      "သည်\t\tppm\t\tppm\t\tO\t\tO\n",
      "။\t\tpunc\t\tpunc\t\tO\t\tO\n",
      "\n",
      "Sentence 5:\n",
      "Token\t\tTrue POS\tPredicted POS\tTrue NER\tPredicted NER\n",
      "------------------------------------------------------------\n",
      "ဆရာ\t\tn\t\tn\t\tB-PER\t\tO\n",
      "ဂင်မ်\t\tn\t\tn\t\tE-PER\t\tS-PER\n",
      "က\t\tppm\t\tppm\t\tO\t\tO\n",
      "တာဝန်ယူ\t\tv\t\tv\t\tO\t\tO\n",
      "တတ်\t\tpart\t\tpart\t\tO\t\tO\n",
      "လွန်း\t\tpart\t\tpart\t\tO\t\tO\n",
      "တဲ့\t\tpart\t\tpart\t\tO\t\tO\n",
      "သူ\t\tn\t\tn\t\tO\t\tO\n",
      "ဖြစ်\t\tv\t\tv\t\tO\t\tO\n",
      "တာ\t\tpart\t\tpart\t\tO\t\tO\n",
      "မို့\t\tpart\t\tpart\t\tO\t\tO\n",
      "ထင်\t\tv\t\tv\t\tO\t\tO\n",
      "ထား\t\tpart\t\tpart\t\tO\t\tO\n",
      "တဲ့\t\tpart\t\tpart\t\tO\t\tO\n",
      "အတိုင်း\t\tppm\t\tppm\t\tO\t\tO\n",
      "အဲ့ဒီ\t\tpron\t\tpron\t\tO\t\tO\n",
      "အလုပ်\t\tn\t\tn\t\tO\t\tO\n",
      "ကို\t\tppm\t\tppm\t\tO\t\tO\n",
      "အသေချာ\t\tadv\t\tadv\t\tO\t\tO\n",
      "အဆုံးသတ်\t\tv\t\tv\t\tO\t\tO\n",
      "လုပ်\t\tv\t\tv\t\tO\t\tO\n",
      "သွား\t\tpart\t\tpart\t\tO\t\tO\n",
      "မှာ\t\tppm\t\tppm\t\tO\t\tO\n",
      "ပါ\t\tpart\t\tpart\t\tO\t\tO\n",
      "။\t\tpunc\t\tpunc\t\tO\t\tO\n"
     ]
    }
   ],
   "source": [
    "# Fasttext CRF Model for Joint POS and NER Training and Prediction\n",
    "\n",
    "def check_environment():\n",
    "    print(\"=== Environment Details ===\")\n",
    "    print(f\"Python Version: {sys.version}\")\n",
    "    print(f\"Platform: {platform.system()} {platform.release()}\")\n",
    "    print(f\"scikit-learn Version: {sklearn.__version__}\")\n",
    "    print(f\"sklearn-crfsuite version: {pkg_resources.get_distribution('sklearn-crfsuite').version}\")\n",
    "    print(\"===========================\")\n",
    "\n",
    "# Check environment\n",
    "check_environment()\n",
    "\n",
    "# Load FastText embeddings\n",
    "fasttext_file = \"/kaggle/input/glove-100d/cc.my.300.vec\"  \n",
    "fasttext_model = KeyedVectors.load_word2vec_format(fasttext_file)\n",
    "print(f\"Loaded {len(fasttext_model)} words from FastText.\")\n",
    "\n",
    "def read_conll(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in lines:\n",
    "        if line.strip() == \"\":\n",
    "            if sentence:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "        else:\n",
    "            token, pos, ner = line.strip().split('\\t')\n",
    "            sentence.append((token, pos, ner))\n",
    "\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "def is_numeric(token):\n",
    "    numeric_chars = set(\"၁၂၃၄၅၆၇၈၉၀\")\n",
    "    return token.isdigit() or all(char in numeric_chars for char in token)\n",
    "\n",
    "# Feature extraction with FastText embeddings\n",
    "def extract_features(sentence, index):\n",
    "    token = sentence[index][0]\n",
    "    features = {\n",
    "        'word': token,\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'prefix-1': token[0],\n",
    "        'prefix-2': token[:2],\n",
    "        'prefix-3': token[:3],\n",
    "        'suffix-1': token[-1],\n",
    "        'suffix-2': token[-2:],\n",
    "        'suffix-3': token[-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1][0],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1][0],\n",
    "        'has_hyphen': '-' in token,\n",
    "        'is_numeric': is_numeric(token),  \n",
    "    }\n",
    "    if token in fasttext_model:\n",
    "        features['fasttext_avg'] = np.mean(fasttext_model[token])\n",
    "    else:\n",
    "        features['fasttext_avg'] = 0.0  # Default value if token is not in FastText\n",
    "    return features\n",
    "\n",
    "# Combine POS and NER labels into a single label\n",
    "def combine_labels(pos_labels, ner_labels):\n",
    "    combined_labels = []\n",
    "    for pos, ner in zip(pos_labels, ner_labels):\n",
    "        combined_labels.append(f\"{pos}_{ner}\")\n",
    "    return combined_labels\n",
    "\n",
    "def prepare_data(conll_data):\n",
    "    X = []\n",
    "    y_joint = []\n",
    "    for sentence in conll_data:\n",
    "        X_sentence = []\n",
    "        y_joint_sentence = []\n",
    "        for i in range(len(sentence)):\n",
    "            X_sentence.append(extract_features(sentence, i))\n",
    "        y_pos_sentence = [token[1] for token in sentence]  # POS labels\n",
    "        y_ner_sentence = [token[2] for token in sentence]  # NER labels\n",
    "        y_joint_sentence = combine_labels(y_pos_sentence, y_ner_sentence)  # Combined labels\n",
    "        X.append(X_sentence)\n",
    "        y_joint.append(y_joint_sentence)\n",
    "    return X, y_joint\n",
    "\n",
    "train_file_path = \"/kaggle/input/split-fix-data/train_v5.conll\"\n",
    "val_file_path = \"/kaggle/input/split-fix-data/val_v5.conll\"\n",
    "test_file_path = \"/kaggle/input/split-fix-data/test_v5.conll\"\n",
    "\n",
    "train_data = read_conll(train_file_path)\n",
    "val_data = read_conll(val_file_path)\n",
    "test_data = read_conll(test_file_path)\n",
    "\n",
    "X_train, y_train_joint = prepare_data(train_data)\n",
    "X_val, y_val_joint = prepare_data(val_data)\n",
    "X_test, y_test_joint = prepare_data(test_data)\n",
    "\n",
    "crf_joint = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,  # L1 regularization\n",
    "    c2=0.1,  # L2 regularization\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "print(\"Training joint CRF model...\")\n",
    "crf_joint.fit(X_train, y_train_joint)\n",
    "\n",
    "y_val_pred_joint = crf_joint.predict(X_val)\n",
    "print(\"Joint Validation F1-score:\", metrics.flat_f1_score(y_val_joint, y_val_pred_joint, average='weighted'))\n",
    "\n",
    "y_test_pred_joint = crf_joint.predict(X_test)\n",
    "\n",
    "def split_labels(combined_labels):\n",
    "    pos_labels = []\n",
    "    ner_labels = []\n",
    "    for label in combined_labels:\n",
    "        pos, ner = label.split(\"_\")\n",
    "        pos_labels.append(pos)\n",
    "        ner_labels.append(ner)\n",
    "    return pos_labels, ner_labels\n",
    "\n",
    "y_test_joint_flat = [label for sent in y_test_joint for label in sent]\n",
    "y_test_pred_joint_flat = [label for sent in y_test_pred_joint for label in sent]\n",
    "\n",
    "y_test_pos_flat, y_test_ner_flat = split_labels(y_test_joint_flat)\n",
    "y_test_pred_pos_flat, y_test_pred_ner_flat = split_labels(y_test_pred_joint_flat)\n",
    "\n",
    "print(\"\\nPOS Classification Report:\")\n",
    "print(classification_report(y_test_pos_flat, y_test_pred_pos_flat, zero_division=0,digits=4))\n",
    "\n",
    "print(\"\\nNER Classification Report:\")\n",
    "print(classification_report(y_test_ner_flat, y_test_pred_ner_flat, zero_division=0,digits=4))\n",
    "\n",
    "# Print 5 predicted sentences from the test set\n",
    "print(\"\\n=== 5 Predicted Sentences from Test Set ===\")\n",
    "for i in range(5): \n",
    "    print(f\"\\nSentence {i + 1}:\")\n",
    "    print(\"Token\\t\\tTrue POS\\tPredicted POS\\tTrue NER\\tPredicted NER\")\n",
    "    print(\"-\" * 60)\n",
    "    for token, true_joint, pred_joint in zip(test_data[i], y_test_joint[i], y_test_pred_joint[i]):\n",
    "        true_pos, true_ner = true_joint.split(\"_\")\n",
    "        pred_pos, pred_ner = pred_joint.split(\"_\")\n",
    "        print(f\"{token[0]}\\t\\t{true_pos}\\t\\t{pred_pos}\\t\\t{true_ner}\\t\\t{pred_ner}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T11:40:05.788159Z",
     "iopub.status.busy": "2025-01-27T11:40:05.787759Z",
     "iopub.status.idle": "2025-01-27T12:00:17.487542Z",
     "shell.execute_reply": "2025-01-27T12:00:17.486291Z",
     "shell.execute_reply.started": "2025-01-27T11:40:05.788131Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Environment Details ===\n",
      "Python Version: 3.10.16 (main, Dec 25 2024, 01:31:21) [GCC 12.2.0]\n",
      "Platform: Linux 6.1.42+\n",
      "scikit-learn Version: 1.6.1\n",
      "sklearn-crfsuite version: 0.5.0\n",
      "===========================\n",
      "Training joint CRF model for POS and NER...\n",
      "Training completed in 1207.79 seconds.\n",
      "Joint Validation F1-score: 0.9579652288656015\n",
      "\n",
      "POS Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         abb     1.0000    0.9444    0.9714        18\n",
      "         adj     0.8887    0.8699    0.8792       569\n",
      "         adv     0.9349    0.8062    0.8658       356\n",
      "        conj     0.9474    0.9499    0.9486       739\n",
      "          fw     0.9655    0.8615    0.9106        65\n",
      "         int     0.9412    0.9412    0.9412        17\n",
      "           n     0.9784    0.9839    0.9811      7694\n",
      "         num     0.9984    0.9984    0.9984       641\n",
      "        part     0.9773    0.9827    0.9800      4461\n",
      "         ppm     0.9934    0.9947    0.9940      4114\n",
      "        pron     0.9678    0.9657    0.9668       467\n",
      "        punc     1.0000    1.0000    1.0000      2919\n",
      "          sb     1.0000    0.9231    0.9600        13\n",
      "          tn     0.9521    0.9464    0.9493       168\n",
      "           v     0.9545    0.9537    0.9541      3302\n",
      "\n",
      "    accuracy                         0.9767     25543\n",
      "   macro avg     0.9666    0.9415    0.9534     25543\n",
      "weighted avg     0.9766    0.9767    0.9766     25543\n",
      "\n",
      "\n",
      "NER Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-DATE     0.8852    0.8182    0.8504        66\n",
      "       B-LOC     0.9770    0.9712    0.9741      1182\n",
      "       B-NUM     0.4615    0.4000    0.4286        15\n",
      "       B-ORG     0.6667    0.5417    0.5977        48\n",
      "       B-PER     0.9118    0.9118    0.9118        34\n",
      "      B-TIME     0.8571    0.6667    0.7500         9\n",
      "      E-DATE     0.8889    0.8485    0.8682        66\n",
      "       E-LOC     0.9680    0.9738    0.9709      1182\n",
      "       E-NUM     0.4615    0.4000    0.4286        15\n",
      "       E-ORG     0.6923    0.5625    0.6207        48\n",
      "       E-PER     0.9118    0.9118    0.9118        34\n",
      "      E-TIME     0.8571    0.6667    0.7500         9\n",
      "      I-DATE     0.7907    0.8947    0.8395        38\n",
      "       I-LOC     0.9833    0.9384    0.9603       503\n",
      "       I-ORG     0.6250    0.3846    0.4762        39\n",
      "           O     0.9890    0.9938    0.9914     21324\n",
      "      S-DATE     0.9747    0.8750    0.9222        88\n",
      "       S-LOC     0.7156    0.6290    0.6695       124\n",
      "       S-NUM     0.9529    0.9789    0.9657       475\n",
      "       S-ORG     0.7273    0.3810    0.5000        21\n",
      "       S-PER     0.8614    0.7803    0.8188       223\n",
      "      S-TIME     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.9812     25543\n",
      "   macro avg     0.7799    0.7058    0.7367     25543\n",
      "weighted avg     0.9805    0.9812    0.9807     25543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Base CRF Model for Joint POS and NER Training and Prediction\n",
    "def check_environment():\n",
    "    print(\"=== Environment Details ===\")\n",
    "    print(f\"Python Version: {sys.version}\")\n",
    "    print(f\"Platform: {platform.system()} {platform.release()}\")\n",
    "    print(f\"scikit-learn Version: {sklearn.__version__}\")\n",
    "    print(f\"sklearn-crfsuite version: {pkg_resources.get_distribution('sklearn-crfsuite').version}\")\n",
    "    print(\"===========================\")\n",
    "\n",
    "# Check environment\n",
    "check_environment()\n",
    "\n",
    "# Read CoNLL data\n",
    "def read_conll(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in lines:\n",
    "        if line.strip() == \"\":\n",
    "            if sentence:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "        else:\n",
    "            token, pos, ner = line.strip().split('\\t')\n",
    "            sentence.append((token, pos, ner))\n",
    "\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "def is_numeric(token):\n",
    "    numeric_chars = set(\"၁၂၃၄၅၆၇၈၉၀\")\n",
    "    return token.isdigit() or all(char in numeric_chars for char in token)\n",
    "\n",
    "# Feature extraction for joint POS and NER\n",
    "def extract_features(sentence, index):\n",
    "    token = sentence[index][0]\n",
    "    features = {\n",
    "        'word': token,\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'prefix-1': token[0],\n",
    "        'prefix-2': token[:2],\n",
    "        'prefix-3': token[:3],\n",
    "        'suffix-1': token[-1],\n",
    "        'suffix-2': token[-2:],\n",
    "        'suffix-3': token[-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1][0],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1][0],\n",
    "        'has_hyphen': '-' in token,\n",
    "        'is_numeric': is_numeric(token),  # Use the combined numeric check\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# Combine POS and NER labels into a single label\n",
    "def combine_labels(pos_labels, ner_labels):\n",
    "    combined_labels = []\n",
    "    for pos, ner in zip(pos_labels, ner_labels):\n",
    "        combined_labels.append(f\"{pos}_{ner}\")  # Combine POS and NER with an underscore\n",
    "    return combined_labels\n",
    "\n",
    "def prepare_data(conll_data):\n",
    "    X = []\n",
    "    y_joint = []\n",
    "    for sentence in conll_data:\n",
    "        X_sentence = []\n",
    "        y_joint_sentence = []\n",
    "        for i in range(len(sentence)):\n",
    "            X_sentence.append(extract_features(sentence, i))\n",
    "        y_pos_sentence = [token[1] for token in sentence]  # POS labels\n",
    "        y_ner_sentence = [token[2] for token in sentence]  # NER labels\n",
    "        y_joint_sentence = combine_labels(y_pos_sentence, y_ner_sentence)  # Combined labels\n",
    "        X.append(X_sentence)\n",
    "        y_joint.append(y_joint_sentence)\n",
    "    return X, y_joint\n",
    "\n",
    "train_file_path = \"/kaggle/input/split-fix-data/train_v5.conll\"\n",
    "val_file_path = \"/kaggle/input/split-fix-data/val_v5.conll\"\n",
    "test_file_path = \"/kaggle/input/split-fix-data/test_v5.conll\"\n",
    "\n",
    "train_data = read_conll(train_file_path)\n",
    "val_data = read_conll(val_file_path)\n",
    "test_data = read_conll(test_file_path)\n",
    "\n",
    "X_train, y_train_joint = prepare_data(train_data)\n",
    "X_val, y_val_joint = prepare_data(val_data)\n",
    "X_test, y_test_joint = prepare_data(test_data)\n",
    "\n",
    "# Joint CRF model for POS and NER\n",
    "crf_joint = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,  # L1 regularization\n",
    "    c2=0.1,  # L2 regularization\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "# Measure training time\n",
    "start_time = time.time()\n",
    "print(\"Training joint CRF model for POS and NER...\")\n",
    "crf_joint.fit(X_train, y_train_joint)\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds.\")\n",
    "\n",
    "# Validate joint model\n",
    "y_val_pred_joint = crf_joint.predict(X_val)\n",
    "print(\"Joint Validation F1-score:\", metrics.flat_f1_score(y_val_joint, y_val_pred_joint, average='weighted'))\n",
    "\n",
    "# Test joint model\n",
    "y_test_pred_joint = crf_joint.predict(X_test)\n",
    "\n",
    "# Split combined labels back into POS and NER for evaluation\n",
    "def split_labels(combined_labels):\n",
    "    pos_labels = []\n",
    "    ner_labels = []\n",
    "    for label in combined_labels:\n",
    "        pos, ner = label.split(\"_\")  # Split combined label into POS and NER\n",
    "        pos_labels.append(pos)\n",
    "        ner_labels.append(ner)\n",
    "    return pos_labels, ner_labels\n",
    "\n",
    "# Flatten the true and predicted labels for classification report\n",
    "y_test_joint_flat = [label for sent in y_test_joint for label in sent]\n",
    "y_test_pred_joint_flat = [label for sent in y_test_pred_joint for label in sent]\n",
    "\n",
    "# Split combined labels into POS and NER\n",
    "y_test_pos_flat, y_test_ner_flat = split_labels(y_test_joint_flat)\n",
    "y_test_pred_pos_flat, y_test_pred_ner_flat = split_labels(y_test_pred_joint_flat)\n",
    "\n",
    "# Generate classification reports for POS and NER\n",
    "print(\"\\nPOS Classification Report:\")\n",
    "print(classification_report(y_test_pos_flat, y_test_pred_pos_flat, zero_division=0, digits=4))\n",
    "\n",
    "print(\"\\nNER Classification Report:\")\n",
    "print(classification_report(y_test_ner_flat, y_test_pred_ner_flat, zero_division=0, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6515326,
     "sourceId": 10566039,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6521472,
     "sourceId": 10800502,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30841,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
