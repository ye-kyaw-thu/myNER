{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-27T13:17:55.488537Z",
     "iopub.status.busy": "2025-01-27T13:17:55.488245Z",
     "iopub.status.idle": "2025-01-27T13:18:02.010544Z",
     "shell.execute_reply": "2025-01-27T13:18:02.009762Z",
     "shell.execute_reply.started": "2025-01-27T13:17:55.488515Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/kmkurn/pytorch-crf.git\n",
      "  Cloning https://github.com/kmkurn/pytorch-crf.git to /tmp/pip-req-build-llv3qnsv\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/kmkurn/pytorch-crf.git /tmp/pip-req-build-llv3qnsv\n",
      "  Resolved https://github.com/kmkurn/pytorch-crf.git to commit 623e3402d00a2728e99d6e8486010d67c754267b\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: pytorch-crf\n",
      "  Building wheel for pytorch-crf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pytorch-crf: filename=pytorch_crf-0.7.2-py3-none-any.whl size=6410 sha256=46e4a4f1fea294db3737617d29eb14fa7f4955e61bf6ae0ec8522c393996b226\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2m_7rwps/wheels/39/5f/f6/4b48b35895d914f4f5fff5b600f87658c11693e37b6a4f118e\n",
      "Successfully built pytorch-crf\n",
      "Installing collected packages: pytorch-crf\n",
      "Successfully installed pytorch-crf-0.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/kmkurn/pytorch-crf.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Experiment: Named Entity Recognition (NER) using BiLSTM-CRF with FastText Embeddings(Unfrozen)\n",
    "# Configuration Settings:\n",
    "# - Embedding Dimension: 300 (FastText)\n",
    "# - Hidden Dimension: 256/128 (BiLSTM)\n",
    "# - Batch Size: 64/32\n",
    "# - Model: BiLSTM-CRF (Single NER MOdel/Joint Model for POS and NER)\n",
    "# - Optimizer: Adam with L2 regularization (weight_decay=1e-5)\n",
    "# - Learning Rate: 0.001\n",
    "# - Dropout: 0.5\n",
    "# - Early Stopping: Patience of 3 epochs\n",
    "# - Training Epochs: 20 up to 30\n",
    "# - Dataset: Pre-split CoNLL format (train_v5.conll, val_v5.conll, test_v5.conll)\n",
    "# - FastText Model: cc.my.300.bin (pre-trained FastText embeddings)\n",
    "# - Evaluation Metrics: Precision, Recall, F1-Score for both POS and NER tasks\n",
    "# - Hardware: NVIDIA Tesla T4 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T13:18:04.933899Z",
     "iopub.status.busy": "2025-01-27T13:18:04.933610Z",
     "iopub.status.idle": "2025-01-27T13:18:23.510688Z",
     "shell.execute_reply": "2025-01-27T13:18:23.510075Z",
     "shell.execute_reply.started": "2025-01-27T13:18:04.933874Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchcrf import CRF\n",
    "from sklearn.metrics import classification_report\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T13:18:50.662480Z",
     "iopub.status.busy": "2025-01-27T13:18:50.661922Z",
     "iopub.status.idle": "2025-01-27T13:21:08.427433Z",
     "shell.execute_reply": "2025-01-27T13:21:08.426665Z",
     "shell.execute_reply.started": "2025-01-27T13:18:50.662451Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 341179 words from FastText binary model.\n",
      "Epoch 1/20: Train Loss = 107106.1570, Val Loss = 4207.9764\n",
      "Epoch 2/20: Train Loss = 26054.9429, Val Loss = 2864.9137\n",
      "Epoch 3/20: Train Loss = 16470.2239, Val Loss = 2481.7054\n",
      "Epoch 4/20: Train Loss = 11656.9056, Val Loss = 2276.5809\n",
      "Epoch 5/20: Train Loss = 8383.0836, Val Loss = 2156.6266\n",
      "Epoch 6/20: Train Loss = 6405.9056, Val Loss = 2255.2282\n",
      "Epoch 7/20: Train Loss = 5265.9274, Val Loss = 2252.1882\n",
      "Epoch 8/20: Train Loss = 4318.5959, Val Loss = 2280.8368\n",
      "Early stopping triggered!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchcrf/__init__.py:308: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:530.)\n",
      "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-DATE     0.8462    0.8333    0.8397        66\n",
      "       B-LOC     0.9583    0.9729    0.9656      1182\n",
      "       B-NUM     0.3571    0.3333    0.3448        15\n",
      "       B-ORG     0.5357    0.6250    0.5769        48\n",
      "       B-PER     0.7750    0.9118    0.8378        34\n",
      "      B-TIME     0.7778    0.7778    0.7778         9\n",
      "      E-DATE     0.9032    0.8485    0.8750        66\n",
      "       E-LOC     0.9729    0.9712    0.9721      1182\n",
      "       E-NUM     0.3750    0.4000    0.3871        15\n",
      "       E-ORG     0.5357    0.6250    0.5769        48\n",
      "       E-PER     0.7317    0.8824    0.8000        34\n",
      "      E-TIME     0.7778    0.7778    0.7778         9\n",
      "      I-DATE     0.7826    0.9474    0.8571        38\n",
      "       I-LOC     0.9624    0.9662    0.9643       503\n",
      "       I-ORG     0.6000    0.4615    0.5217        39\n",
      "           O     0.9884    0.9890    0.9887     21324\n",
      "      S-DATE     0.9868    0.8523    0.9146        88\n",
      "       S-LOC     0.6357    0.6613    0.6482       124\n",
      "       S-NUM     0.9299    0.9495    0.9396       475\n",
      "       S-ORG     0.6471    0.5238    0.5789        21\n",
      "       S-PER     0.7012    0.5157    0.5943       223\n",
      "      S-TIME     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.9755     25543\n",
      "   macro avg     0.7173    0.7193    0.7154     25543\n",
      "weighted avg     0.9755    0.9755    0.9753     25543\n",
      "\n",
      "\n",
      "Sample 556:\n",
      "Sentence:     သူ နှစ်တိုင်း မြန်မာ ပြည် ကို လာ ပါ တယ် ၊ တောက်လျှောက် ကျွန်တော့် ကို လာ တွေ့ တယ် ၊ ကျွန်တော် တို့ မိတ်ဆွေ ဟောင်း တွေ ဖြစ် နေ ကြ တယ် ။\n",
      "True NER:     O O B-LOC E-LOC O O O O O O O O O O O O O O O O O O O O O O\n",
      "Predicted NER: O O B-LOC E-LOC O O O O O O O O O O O O O O O O O O O O O O\n",
      "\n",
      "Sample 1640:\n",
      "Sentence:     ကျွန်တော် ချောင်းဆိုး ခြင်း အတွက် တစ် ခု ခု လို ချင် တယ် ။\n",
      "True NER:     O O O O O O O O O O O\n",
      "Predicted NER: O O O O O O O O O O O\n",
      "\n",
      "Sample 367:\n",
      "Sentence:     ၂၀၁၄ သန်းခေါင် စာရင်း အရ တောင်ကြား ကျေးရွာ အုပ်စု တွင် ကျား ၁၂၈၅ ဦး ၊ မ ၁၃၃၆ ဦး ၊ လူဦးရေ စုစုပေါင်း ၂၆၂၁ ဦး နေထိုင် သည် ။\n",
      "True NER:     S-DATE O O O B-LOC I-LOC E-LOC O O S-NUM O O O S-NUM O O O O S-NUM O O O O\n",
      "Predicted NER: S-DATE O O O B-LOC I-LOC E-LOC O O S-NUM O O O S-NUM O O O O S-NUM O O O O\n",
      "\n",
      "Sample 1161:\n",
      "Sentence:     ခင်ဗျား သူ့ ကို မေး လည်း အလကား ပဲ ။\n",
      "True NER:     O O O O O O O O\n",
      "Predicted NER: O O O O O O O O\n",
      "\n",
      "Sample 636:\n",
      "Sentence:     ထို ခေတ် အခါ လောက် တွင် ပင် အိန္ဒိယ ပြည် ၌ ဗေဒ ကျမ်း ကြီး များ ပေါ်ထွန်း ခဲ့ သည် ။\n",
      "True NER:     O O O O O O B-LOC E-LOC O O O O O O O O O\n",
      "Predicted NER: O O O O O O B-LOC E-LOC O O O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "#embedding300/hiddendim256/batch64(Single NER Model)\n",
    "# Load FastText binary model\n",
    "fasttext_bin_file = \"/kaggle/input/glove-100d/cc.my.300.bin\"  \n",
    "fasttext_model = load_facebook_model(fasttext_bin_file)\n",
    "\n",
    "# Extract the word vectors\n",
    "fasttext_vectors = fasttext_model.wv\n",
    "print(f\"Loaded {len(fasttext_vectors)} words from FastText binary model.\")\n",
    "\n",
    "# Define Dataset Class (NER-only)\n",
    "class CoNLLDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.sentences, self.ner_tags = self.load_data(file_path)\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        sentences, ner_tags = [], []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            sentence, ner_tag = [], []\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    word, _, ner = line.strip().split(\"\\t\")  # Ignore POS tags\n",
    "                    sentence.append(word)\n",
    "                    ner_tag.append(ner)\n",
    "                else:\n",
    "                    if sentence:\n",
    "                        sentences.append(sentence)\n",
    "                        ner_tags.append(ner_tag)\n",
    "                    sentence, ner_tag = [], []\n",
    "            if sentence:\n",
    "                sentences.append(sentence)\n",
    "                ner_tags.append(ner_tag)\n",
    "        return sentences, ner_tags\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.ner_tags[idx]\n",
    "\n",
    "# Collate function for dynamic padding (NER-only)\n",
    "def collate_fn(batch):\n",
    "    sentences, ner_tags = zip(*batch)\n",
    "    max_len = max(len(s) for s in sentences)\n",
    "\n",
    "    sentence_tensors = []\n",
    "    ner_tensors = []\n",
    "\n",
    "    for s, n in zip(sentences, ner_tags):\n",
    "        padded_sentence = s + [\"<PAD>\"] * (max_len - len(s))\n",
    "        padded_ner = n + [\"<PAD>\"] * (max_len - len(n))\n",
    "\n",
    "        sentence_tensors.append(torch.tensor([vocab.get(word, vocab[\"<UNK>\"]) for word in padded_sentence], dtype=torch.long))\n",
    "        ner_tensors.append(torch.tensor([ner_tag_to_ix[tag] for tag in padded_ner], dtype=torch.long))\n",
    "\n",
    "    return torch.stack(sentence_tensors), torch.stack(ner_tensors)\n",
    "\n",
    "# Define BiLSTM-CRF Model for NER-only\n",
    "class BiLSTMCRF_NER(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_ner_tags, fasttext_embeddings):\n",
    "        super(BiLSTMCRF_NER, self).__init__()\n",
    "        # Initialize embedding layer with FastText embeddings (frozen)\n",
    "        self.embedding = nn.Embedding.from_pretrained(fasttext_embeddings, freeze=False)  # Freeze embeddings\n",
    "        self.dropout = nn.Dropout(0.5)  # Add dropout\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.ner_fc = nn.Linear(hidden_dim * 2, num_ner_tags)\n",
    "        self.ner_crf = CRF(num_ner_tags, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x)\n",
    "        embeddings = self.dropout(embeddings)  # Apply dropout\n",
    "        lstm_out, _ = self.bilstm(embeddings)\n",
    "        lstm_out = self.dropout(lstm_out)  # Apply dropout\n",
    "        ner_logits = self.ner_fc(lstm_out)\n",
    "        return ner_logits\n",
    "\n",
    "    def compute_loss(self, x, ner_tags):\n",
    "        ner_logits = self.forward(x)\n",
    "        ner_loss = -self.ner_crf(ner_logits, ner_tags, mask=(x != vocab[\"<PAD>\"]))\n",
    "        return ner_loss\n",
    "\n",
    "    def decode(self, x):\n",
    "        ner_logits = self.forward(x)\n",
    "        ner_tags = self.ner_crf.decode(ner_logits)\n",
    "        return ner_tags\n",
    "\n",
    "# Paths to pre-split datasets\n",
    "train_file_path = \"/kaggle/input/split-fix-data/train_v5.conll\"\n",
    "val_file_path = \"/kaggle/input/split-fix-data/val_v5.conll\"\n",
    "test_file_path = \"/kaggle/input/split-fix-data/test_v5.conll\"\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = CoNLLDataset(train_file_path)\n",
    "val_dataset = CoNLLDataset(val_file_path)\n",
    "test_dataset = CoNLLDataset(test_file_path)\n",
    "\n",
    "# Create vocabulary and tag-to-index mappings\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "ner_tag_to_ix = {\"<PAD>\": 0}  # Add <PAD> to NER tags\n",
    "\n",
    "# Build vocab and tag mappings\n",
    "for dataset in [train_dataset, val_dataset, test_dataset]:\n",
    "    for sentence, ner_tags in zip(dataset.sentences, dataset.ner_tags):\n",
    "        for word in sentence:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "        for ner_tag in ner_tags:\n",
    "            if ner_tag not in ner_tag_to_ix:\n",
    "                ner_tag_to_ix[ner_tag] = len(ner_tag_to_ix)\n",
    "\n",
    "# Create embedding matrix using FastText (dimension: 300)\n",
    "embedding_dim = 300  # FastText embedding dimension\n",
    "fasttext_embeddings = torch.zeros((len(vocab), embedding_dim))  # Initialize with zeros\n",
    "\n",
    "for word, idx in vocab.items():\n",
    "    if word in fasttext_vectors:\n",
    "        fasttext_embeddings[idx] = torch.tensor(fasttext_vectors[word])  # Use full 300 dimensions\n",
    "    elif word == \"<PAD>\":\n",
    "        fasttext_embeddings[idx] = torch.zeros(embedding_dim)  # Zero vector for padding\n",
    "    else:\n",
    "        fasttext_embeddings[idx] = torch.randn(embedding_dim)  # Random vector for unknown words\n",
    "\n",
    "# Initialize model\n",
    "hidden_dim = 256\n",
    "vocab_size = len(vocab)\n",
    "num_ner_tags = len(ner_tag_to_ix)\n",
    "\n",
    "model = BiLSTMCRF_NER(vocab_size, embedding_dim, hidden_dim, num_ner_tags, fasttext_embeddings).to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # Add weight decay for L2 regularization\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Training loop with early stopping\n",
    "def train_model(model, train_loader, val_loader, test_loader, epochs):\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 3\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for sentences, ner_tags in train_loader:\n",
    "            sentences, ner_tags = sentences.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.compute_loss(sentences, ner_tags)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for sentences, ner_tags in val_loader:\n",
    "                sentences, ner_tags = sentences.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "                val_loss += model.compute_loss(sentences, ner_tags).item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    # Evaluate on test set after training\n",
    "    model.eval()\n",
    "    all_ner_preds, all_ner_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for sentences, ner_tags in test_loader:\n",
    "            sentences, ner_tags = sentences.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "            ner_preds = model.decode(sentences)\n",
    "\n",
    "            # Flatten the predictions and targets, excluding <PAD> tokens\n",
    "            for i in range(len(sentences)):\n",
    "                sentence_length = (sentences[i] != vocab[\"<PAD>\"]).sum().item()  # Length of the actual sentence\n",
    "                all_ner_preds.extend(ner_preds[i][:sentence_length])  # Truncate predictions to sentence length\n",
    "                all_ner_targets.extend(ner_tags[i][:sentence_length].cpu().numpy())  # Truncate targets to sentence length\n",
    "\n",
    "    # Convert predictions and targets to tag names\n",
    "    idx_to_ner = {v: k for k, v in ner_tag_to_ix.items()}\n",
    "\n",
    "    # Filter out padding tokens from predictions and targets\n",
    "    all_ner_preds_filtered = [idx_to_ner[idx] for idx in all_ner_preds]\n",
    "    all_ner_targets_filtered = [idx_to_ner[idx] for idx in all_ner_targets]\n",
    "\n",
    "    # Generate classification report\n",
    "    print(\"NER Classification Report:\")\n",
    "    print(classification_report(all_ner_targets_filtered, all_ner_preds_filtered, zero_division=0, digits=4))\n",
    "\n",
    "# Function to display random sentences with true and predicted NER tags\n",
    "def display_random_samples(model, test_loader, vocab, ner_tag_to_ix, num_samples=5):\n",
    "    model.eval()\n",
    "    idx_to_ner = {v: k for k, v in ner_tag_to_ix.items()}\n",
    "    vocab_inv = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    # Collect all sentences and their true/predicted tags\n",
    "    all_sentences = []\n",
    "    all_true_ner = []\n",
    "    all_pred_ner = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sentences, ner_tags in test_loader:\n",
    "            sentences, ner_tags = sentences.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "            ner_preds = model.decode(sentences)\n",
    "\n",
    "            # Convert indices to words and tags\n",
    "            for i in range(len(sentences)):\n",
    "                sentence_length = (sentences[i] != vocab[\"<PAD>\"]).sum().item()  # Length of the actual sentence\n",
    "                words = [vocab_inv[idx.item()] for idx in sentences[i][:sentence_length]]\n",
    "                true_ner = [idx_to_ner[idx.item()] for idx in ner_tags[i][:sentence_length]]\n",
    "                pred_ner = [idx_to_ner[idx] for idx in ner_preds[i][:sentence_length]]\n",
    "\n",
    "                all_sentences.append(words)\n",
    "                all_true_ner.append(true_ner)\n",
    "                all_pred_ner.append(pred_ner)\n",
    "\n",
    "    # Randomly select `num_samples` sentences\n",
    "    random_indices = random.sample(range(len(all_sentences)), num_samples)\n",
    "    for idx in random_indices:\n",
    "        print(f\"\\nSample {idx + 1}:\")\n",
    "        print(\"Sentence:    \", \" \".join(all_sentences[idx]))\n",
    "        print(\"True NER:    \", \" \".join(all_true_ner[idx]))\n",
    "        print(\"Predicted NER:\", \" \".join(all_pred_ner[idx]))\n",
    "\n",
    "# Train the model and evaluate on the test set\n",
    "train_model(model, train_loader, val_loader, test_loader, epochs=20)\n",
    "\n",
    "# Display 5 random samples from the test set\n",
    "display_random_samples(model, test_loader, vocab, ner_tag_to_ix, num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T08:38:44.939156Z",
     "iopub.status.busy": "2025-01-27T08:38:44.938719Z",
     "iopub.status.idle": "2025-01-27T08:41:28.578171Z",
     "shell.execute_reply": "2025-01-27T08:41:28.577363Z",
     "shell.execute_reply.started": "2025-01-27T08:38:44.939106Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 341179 words from FastText binary model.\n",
      "Epoch 1/30: Train Loss = 79217.3216, Val Loss = 3492.5098\n",
      "Epoch 2/30: Train Loss = 22002.7420, Val Loss = 2513.7625\n",
      "Epoch 3/30: Train Loss = 13313.3383, Val Loss = 2305.9815\n",
      "Epoch 4/30: Train Loss = 8987.3853, Val Loss = 2012.5785\n",
      "Epoch 5/30: Train Loss = 6550.1772, Val Loss = 2125.9027\n",
      "Epoch 6/30: Train Loss = 5100.0334, Val Loss = 2082.6652\n",
      "Epoch 7/30: Train Loss = 4124.5737, Val Loss = 2119.9131\n",
      "Early stopping triggered!\n",
      "NER Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-DATE     0.8667    0.7879    0.8254        66\n",
      "       B-LOC     0.9779    0.9721    0.9750      1182\n",
      "       B-NUM     0.3000    0.4000    0.3429        15\n",
      "       B-ORG     0.6774    0.4375    0.5316        48\n",
      "       B-PER     0.9394    0.9118    0.9254        34\n",
      "      B-TIME     0.8750    0.7778    0.8235         9\n",
      "      E-DATE     0.8689    0.8030    0.8346        66\n",
      "       E-LOC     0.9795    0.9687    0.9741      1182\n",
      "       E-NUM     0.3000    0.4000    0.3429        15\n",
      "       E-ORG     0.6857    0.5000    0.5783        48\n",
      "       E-PER     0.9375    0.8824    0.9091        34\n",
      "      E-TIME     0.8750    0.7778    0.8235         9\n",
      "      I-DATE     0.7619    0.8421    0.8000        38\n",
      "       I-LOC     0.9681    0.9642    0.9661       503\n",
      "       I-ORG     0.6957    0.4103    0.5161        39\n",
      "       I-PER     0.0000    0.0000    0.0000         0\n",
      "           O     0.9858    0.9943    0.9900     21324\n",
      "      S-DATE     0.9863    0.8182    0.8944        88\n",
      "       S-LOC     0.7407    0.6452    0.6897       124\n",
      "       S-NUM     0.9712    0.9221    0.9460       475\n",
      "       S-ORG     0.6667    0.4762    0.5556        21\n",
      "       S-PER     0.8049    0.5919    0.6822       223\n",
      "      S-TIME     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.9787     25543\n",
      "   macro avg     0.7332    0.6645    0.6924     25543\n",
      "weighted avg     0.9779    0.9787    0.9779     25543\n",
      "\n",
      "\n",
      "Sample 672:\n",
      "Sentence:     ကျားယုံ သည် မားမား ခေါက်ဆွဲ အား အလွန် ကြိုက် သည် ။\n",
      "True NER:     S-PER O O O O O O O O\n",
      "Predicted NER: S-PER O O O O O O O O\n",
      "\n",
      "Sample 1615:\n",
      "Sentence:     စကား တွေ ဝေ့လည်ကြောင်ပတ် လုပ် မ နေ နဲ့ ဒဲ့ ပြော ပါ ။\n",
      "True NER:     O O O O O O O O O O O\n",
      "Predicted NER: O O O O O O O O O O O\n",
      "\n",
      "Sample 800:\n",
      "Sentence:     ၂၀၁၄ သန်းခေါင်စာရင်း အရ အင်းတန် ကျေးရွာ အုပ်စု တွင် ကျား ၁၂၃၁ ဦး ၊ မ ၁၂၈၃ ဦး ၊ လူဦးရေ စုစုပေါင်း ၂၅၁၄ ဦး နေထိုင် သည် ။\n",
      "True NER:     S-DATE O O B-LOC I-LOC E-LOC O O S-NUM O O O S-NUM O O O O S-NUM O O O O\n",
      "Predicted NER: S-DATE O O B-LOC I-LOC E-LOC O O S-NUM O O O S-NUM O O O O S-NUM O O O O\n",
      "\n",
      "Sample 7:\n",
      "Sentence:     ရေရွေး ( ရှေ့ ) ရွာ သည် ရှမ်း ပြည်နယ် ( တောင် ) ၊ ပအိုဝ်း ကိုယ်ပိုင် အုပ်ချုပ်ခွင့်ရ ဒေသ ၊ ပင်လောင်း မြို့နယ် ၊ ပေါ့အင် ကျေးရွာ အုပ်စု ၌ တည်ရှိ သည် ။\n",
      "True NER:     B-LOC O I-LOC O E-LOC O B-LOC I-LOC O E-LOC O O B-LOC I-LOC I-LOC E-LOC O B-LOC E-LOC O B-LOC I-LOC E-LOC O O O O\n",
      "Predicted NER: B-LOC O I-LOC O E-LOC O B-LOC I-LOC O E-LOC O O B-LOC I-LOC I-LOC E-LOC O B-LOC E-LOC O B-LOC I-LOC E-LOC O O O O\n",
      "\n",
      "Sample 908:\n",
      "Sentence:     ဘာသာစကား သင်ယူ လေ့လာ ရာ မှာ သဒ္ဒါ သာ မက ဝေါဟာရ က လည်း အရေးပါ တယ် ။\n",
      "True NER:     O O O O O O O O O O O O O O\n",
      "Predicted NER: O O O O O O O O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "#embedding300/hiddendim128/batch32 (Single NER)\n",
    "# Load FastText binary model\n",
    "fasttext_bin_file = \"/kaggle/input/glove-100d/cc.my.300.bin\"  # Replace with your .bin file path\n",
    "fasttext_model = load_facebook_model(fasttext_bin_file)\n",
    "\n",
    "# Extract the word vectors\n",
    "fasttext_vectors = fasttext_model.wv\n",
    "print(f\"Loaded {len(fasttext_vectors)} words from FastText binary model.\")\n",
    "\n",
    "# Define Dataset Class (NER-only)\n",
    "class CoNLLDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.sentences, self.ner_tags = self.load_data(file_path)\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        sentences, ner_tags = [], []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            sentence, ner_tag = [], []\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    word, _, ner = line.strip().split(\"\\t\")  # Ignore POS tags\n",
    "                    sentence.append(word)\n",
    "                    ner_tag.append(ner)\n",
    "                else:\n",
    "                    if sentence:\n",
    "                        sentences.append(sentence)\n",
    "                        ner_tags.append(ner_tag)\n",
    "                    sentence, ner_tag = [], []\n",
    "            if sentence:\n",
    "                sentences.append(sentence)\n",
    "                ner_tags.append(ner_tag)\n",
    "        return sentences, ner_tags\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.ner_tags[idx]\n",
    "\n",
    "# Collate function for dynamic padding (NER-only)\n",
    "def collate_fn(batch):\n",
    "    sentences, ner_tags = zip(*batch)\n",
    "    max_len = max(len(s) for s in sentences)\n",
    "\n",
    "    sentence_tensors = []\n",
    "    ner_tensors = []\n",
    "\n",
    "    for s, n in zip(sentences, ner_tags):\n",
    "        padded_sentence = s + [\"<PAD>\"] * (max_len - len(s))\n",
    "        padded_ner = n + [\"<PAD>\"] * (max_len - len(n))\n",
    "\n",
    "        sentence_tensors.append(torch.tensor([vocab.get(word, vocab[\"<UNK>\"]) for word in padded_sentence], dtype=torch.long))\n",
    "        ner_tensors.append(torch.tensor([ner_tag_to_ix[tag] for tag in padded_ner], dtype=torch.long))\n",
    "\n",
    "    return torch.stack(sentence_tensors), torch.stack(ner_tensors)\n",
    "\n",
    "# Define BiLSTM-CRF Model for NER-only\n",
    "class BiLSTMCRF_NER(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_ner_tags, fasttext_embeddings):\n",
    "        super(BiLSTMCRF_NER, self).__init__()\n",
    "        # Initialize embedding layer with FastText embeddings (frozen)\n",
    "        self.embedding = nn.Embedding.from_pretrained(fasttext_embeddings, freeze=False)  # Freeze embeddings\n",
    "        self.dropout = nn.Dropout(0.5)  # Add dropout\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.ner_fc = nn.Linear(hidden_dim * 2, num_ner_tags)\n",
    "        self.ner_crf = CRF(num_ner_tags, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x)\n",
    "        embeddings = self.dropout(embeddings)  # Apply dropout\n",
    "        lstm_out, _ = self.bilstm(embeddings)\n",
    "        lstm_out = self.dropout(lstm_out)  # Apply dropout\n",
    "        ner_logits = self.ner_fc(lstm_out)\n",
    "        return ner_logits\n",
    "\n",
    "    def compute_loss(self, x, ner_tags):\n",
    "        ner_logits = self.forward(x)\n",
    "        ner_loss = -self.ner_crf(ner_logits, ner_tags, mask=(x != vocab[\"<PAD>\"]))\n",
    "        return ner_loss\n",
    "\n",
    "    def decode(self, x):\n",
    "        ner_logits = self.forward(x)\n",
    "        ner_tags = self.ner_crf.decode(ner_logits)\n",
    "        return ner_tags\n",
    "\n",
    "# Paths to pre-split datasets\n",
    "train_file_path = \"/kaggle/input/split-fix-data/train_v5.conll\"\n",
    "val_file_path = \"/kaggle/input/split-fix-data/val_v5.conll\"\n",
    "test_file_path = \"/kaggle/input/split-fix-data/test_v5.conll\"\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = CoNLLDataset(train_file_path)\n",
    "val_dataset = CoNLLDataset(val_file_path)\n",
    "test_dataset = CoNLLDataset(test_file_path)\n",
    "\n",
    "# Create vocabulary and tag-to-index mappings\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "ner_tag_to_ix = {\"<PAD>\": 0}  # Add <PAD> to NER tags\n",
    "\n",
    "# Build vocab and tag mappings\n",
    "for dataset in [train_dataset, val_dataset, test_dataset]:\n",
    "    for sentence, ner_tags in zip(dataset.sentences, dataset.ner_tags):\n",
    "        for word in sentence:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "        for ner_tag in ner_tags:\n",
    "            if ner_tag not in ner_tag_to_ix:\n",
    "                ner_tag_to_ix[ner_tag] = len(ner_tag_to_ix)\n",
    "\n",
    "# Create embedding matrix using FastText (dimension: 300)\n",
    "embedding_dim = 300  # FastText embedding dimension\n",
    "fasttext_embeddings = torch.zeros((len(vocab), embedding_dim))  # Initialize with zeros\n",
    "\n",
    "for word, idx in vocab.items():\n",
    "    if word in fasttext_vectors:\n",
    "        fasttext_embeddings[idx] = torch.tensor(fasttext_vectors[word])  # Use full 300 dimensions\n",
    "    elif word == \"<PAD>\":\n",
    "        fasttext_embeddings[idx] = torch.zeros(embedding_dim)  # Zero vector for padding\n",
    "    else:\n",
    "        fasttext_embeddings[idx] = torch.randn(embedding_dim)  # Random vector for unknown words\n",
    "\n",
    "# Initialize model\n",
    "hidden_dim = 128\n",
    "vocab_size = len(vocab)\n",
    "num_ner_tags = len(ner_tag_to_ix)\n",
    "\n",
    "model = BiLSTMCRF_NER(vocab_size, embedding_dim, hidden_dim, num_ner_tags, fasttext_embeddings).to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # Add weight decay for L2 regularization\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Training loop with early stopping\n",
    "def train_model(model, train_loader, val_loader, test_loader, epochs):\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 3\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for sentences, ner_tags in train_loader:\n",
    "            sentences, ner_tags = sentences.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.compute_loss(sentences, ner_tags)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for sentences, ner_tags in val_loader:\n",
    "                sentences, ner_tags = sentences.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "                val_loss += model.compute_loss(sentences, ner_tags).item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    # Evaluate on test set after training\n",
    "    model.eval()\n",
    "    all_ner_preds, all_ner_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for sentences, ner_tags in test_loader:\n",
    "            sentences, ner_tags = sentences.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "            ner_preds = model.decode(sentences)\n",
    "\n",
    "            # Flatten the predictions and targets, excluding <PAD> tokens\n",
    "            for i in range(len(sentences)):\n",
    "                sentence_length = (sentences[i] != vocab[\"<PAD>\"]).sum().item()  # Length of the actual sentence\n",
    "                all_ner_preds.extend(ner_preds[i][:sentence_length])  # Truncate predictions to sentence length\n",
    "                all_ner_targets.extend(ner_tags[i][:sentence_length].cpu().numpy())  # Truncate targets to sentence length\n",
    "\n",
    "    # Convert predictions and targets to tag names\n",
    "    idx_to_ner = {v: k for k, v in ner_tag_to_ix.items()}\n",
    "\n",
    "    # Filter out padding tokens from predictions and targets\n",
    "    all_ner_preds_filtered = [idx_to_ner[idx] for idx in all_ner_preds]\n",
    "    all_ner_targets_filtered = [idx_to_ner[idx] for idx in all_ner_targets]\n",
    "\n",
    "    # Generate classification report\n",
    "    print(\"NER Classification Report:\")\n",
    "    print(classification_report(all_ner_targets_filtered, all_ner_preds_filtered, zero_division=0, digits=4))\n",
    "\n",
    "# Function to display random sentences with true and predicted NER tags\n",
    "def display_random_samples(model, test_loader, vocab, ner_tag_to_ix, num_samples=5):\n",
    "    model.eval()\n",
    "    idx_to_ner = {v: k for k, v in ner_tag_to_ix.items()}\n",
    "    vocab_inv = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    # Collect all sentences and their true/predicted tags\n",
    "    all_sentences = []\n",
    "    all_true_ner = []\n",
    "    all_pred_ner = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sentences, ner_tags in test_loader:\n",
    "            sentences, ner_tags = sentences.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "            ner_preds = model.decode(sentences)\n",
    "\n",
    "            # Convert indices to words and tags\n",
    "            for i in range(len(sentences)):\n",
    "                sentence_length = (sentences[i] != vocab[\"<PAD>\"]).sum().item()  # Length of the actual sentence\n",
    "                words = [vocab_inv[idx.item()] for idx in sentences[i][:sentence_length]]\n",
    "                true_ner = [idx_to_ner[idx.item()] for idx in ner_tags[i][:sentence_length]]\n",
    "                pred_ner = [idx_to_ner[idx] for idx in ner_preds[i][:sentence_length]]\n",
    "\n",
    "                all_sentences.append(words)\n",
    "                all_true_ner.append(true_ner)\n",
    "                all_pred_ner.append(pred_ner)\n",
    "\n",
    "    # Randomly select `num_samples` sentences\n",
    "    random_indices = random.sample(range(len(all_sentences)), num_samples)\n",
    "    for idx in random_indices:\n",
    "        print(f\"\\nSample {idx + 1}:\")\n",
    "        print(\"Sentence:    \", \" \".join(all_sentences[idx]))\n",
    "        print(\"True NER:    \", \" \".join(all_true_ner[idx]))\n",
    "        print(\"Predicted NER:\", \" \".join(all_pred_ner[idx]))\n",
    "\n",
    "# Train the model and evaluate on the test set\n",
    "train_model(model, train_loader, val_loader, test_loader, epochs=30)\n",
    "\n",
    "# Display 5 random samples from the test set\n",
    "display_random_samples(model, test_loader, vocab, ner_tag_to_ix, num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T04:14:09.229152Z",
     "iopub.status.busy": "2025-01-27T04:14:09.228841Z",
     "iopub.status.idle": "2025-01-27T04:24:11.965981Z",
     "shell.execute_reply": "2025-01-27T04:24:11.965212Z",
     "shell.execute_reply.started": "2025-01-27T04:14:09.229129Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 341179 words from FastText binary model.\n",
      "Epoch 1/30: Train Loss = 113667.3239, Val Loss = 4140.6803\n",
      "Epoch 2/30: Train Loss = 26897.9645, Val Loss = 2721.7103\n",
      "Epoch 3/30: Train Loss = 16614.9794, Val Loss = 2285.2573\n",
      "Epoch 4/30: Train Loss = 12265.3234, Val Loss = 2209.2708\n",
      "Epoch 5/30: Train Loss = 9739.6690, Val Loss = 2204.4113\n",
      "Epoch 6/30: Train Loss = 8198.7894, Val Loss = 2065.8556\n",
      "Epoch 7/30: Train Loss = 7078.0542, Val Loss = 2162.6499\n",
      "Epoch 8/30: Train Loss = 6150.7442, Val Loss = 2250.2693\n",
      "Epoch 9/30: Train Loss = 5564.5062, Val Loss = 2137.7373\n",
      "Epoch 10/30: Train Loss = 4969.9363, Val Loss = 2162.1399\n",
      "Epoch 11/30: Train Loss = 4453.3281, Val Loss = 2257.2618\n",
      "Epoch 12/30: Train Loss = 4076.7058, Val Loss = 2268.3211\n",
      "Epoch 13/30: Train Loss = 3557.5349, Val Loss = 2270.6549\n",
      "Epoch 14/30: Train Loss = 3309.6896, Val Loss = 2425.7447\n",
      "Epoch 15/30: Train Loss = 3134.9647, Val Loss = 2425.7703\n",
      "Epoch 16/30: Train Loss = 2847.1326, Val Loss = 2445.4997\n",
      "Early stopping triggered!\n",
      "POS Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         abb       1.00      0.94      0.97        18\n",
      "         adj       0.92      0.88      0.90       569\n",
      "         adv       0.95      0.83      0.89       356\n",
      "        conj       0.95      0.96      0.95       739\n",
      "          fw       0.69      0.54      0.60        65\n",
      "         int       0.94      0.88      0.91        17\n",
      "           n       0.97      0.99      0.98      7694\n",
      "         num       1.00      0.95      0.97       641\n",
      "        part       0.98      0.98      0.98      4461\n",
      "         ppm       0.99      1.00      0.99      4114\n",
      "        pron       0.98      0.98      0.98       467\n",
      "        punc       1.00      1.00      1.00      2919\n",
      "          sb       1.00      1.00      1.00        13\n",
      "          tn       0.95      0.94      0.95       168\n",
      "           v       0.97      0.96      0.96      3302\n",
      "\n",
      "    accuracy                           0.98     25543\n",
      "   macro avg       0.95      0.92      0.94     25543\n",
      "weighted avg       0.98      0.98      0.98     25543\n",
      "\n",
      "NER Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-DATE       0.91      0.80      0.85        66\n",
      "       B-LOC       0.99      0.96      0.97      1182\n",
      "       B-NUM       0.43      0.20      0.27        15\n",
      "       B-ORG       0.59      0.54      0.57        48\n",
      "       B-PER       0.93      0.82      0.87        34\n",
      "      B-TIME       0.88      0.78      0.82         9\n",
      "      E-DATE       0.95      0.80      0.87        66\n",
      "       E-LOC       0.99      0.96      0.98      1182\n",
      "       E-NUM       0.43      0.20      0.27        15\n",
      "       E-ORG       0.54      0.52      0.53        48\n",
      "       E-PER       0.90      0.82      0.86        34\n",
      "      E-TIME       0.88      0.78      0.82         9\n",
      "      I-DATE       0.82      0.95      0.88        38\n",
      "       I-LOC       0.98      0.97      0.97       503\n",
      "       I-ORG       0.48      0.36      0.41        39\n",
      "           O       0.98      1.00      0.99     21324\n",
      "      S-DATE       0.97      0.84      0.90        88\n",
      "       S-LOC       0.72      0.63      0.67       124\n",
      "       S-NUM       0.96      0.93      0.95       475\n",
      "       S-ORG       0.53      0.48      0.50        21\n",
      "       S-PER       0.86      0.40      0.54       223\n",
      "      S-TIME       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.98     25543\n",
      "   macro avg       0.76      0.67      0.71     25543\n",
      "weighted avg       0.98      0.98      0.98     25543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#embedding300/hiddendim128/batch32(Joint_Model)\n",
    "# Load FastText binary model\n",
    "fasttext_bin_file = \"/kaggle/input/glove-100d/cc.my.300.bin\"  \n",
    "fasttext_model = load_facebook_model(fasttext_bin_file)\n",
    "\n",
    "# Extract the word vectors\n",
    "fasttext_vectors = fasttext_model.wv\n",
    "print(f\"Loaded {len(fasttext_vectors)} words from FastText binary model.\")\n",
    "\n",
    "# Define Dataset Class\n",
    "class CoNLLDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.sentences, self.pos_tags, self.ner_tags = self.load_data(file_path)\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        sentences, pos_tags, ner_tags = [], [], []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            sentence, pos_tag, ner_tag = [], [], []\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    word, pos, ner = line.strip().split(\"\\t\")\n",
    "                    sentence.append(word)\n",
    "                    pos_tag.append(pos)\n",
    "                    ner_tag.append(ner)\n",
    "                else:\n",
    "                    if sentence:\n",
    "                        sentences.append(sentence)\n",
    "                        pos_tags.append(pos_tag)\n",
    "                        ner_tags.append(ner_tag)\n",
    "                    sentence, pos_tag, ner_tag = [], [], []\n",
    "            if sentence:\n",
    "                sentences.append(sentence)\n",
    "                pos_tags.append(pos_tag)\n",
    "                ner_tags.append(ner_tag)\n",
    "        return sentences, pos_tags, ner_tags\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.pos_tags[idx], self.ner_tags[idx]\n",
    "\n",
    "# Collate function for dynamic padding\n",
    "def collate_fn(batch):\n",
    "    sentences, pos_tags, ner_tags = zip(*batch)\n",
    "    max_len = max(len(s) for s in sentences)\n",
    "\n",
    "    sentence_tensors = []\n",
    "    pos_tensors = []\n",
    "    ner_tensors = []\n",
    "\n",
    "    for s, p, n in zip(sentences, pos_tags, ner_tags):\n",
    "        padded_sentence = s + [\"<PAD>\"] * (max_len - len(s))\n",
    "        padded_pos = p + [\"<PAD>\"] * (max_len - len(p))\n",
    "        padded_ner = n + [\"<PAD>\"] * (max_len - len(n))\n",
    "\n",
    "        sentence_tensors.append(torch.tensor([vocab.get(word, vocab[\"<UNK>\"]) for word in padded_sentence], dtype=torch.long))\n",
    "        pos_tensors.append(torch.tensor([pos_tag_to_ix[tag] for tag in padded_pos], dtype=torch.long))\n",
    "        ner_tensors.append(torch.tensor([ner_tag_to_ix[tag] for tag in padded_ner], dtype=torch.long))\n",
    "\n",
    "    return torch.stack(sentence_tensors), torch.stack(pos_tensors), torch.stack(ner_tensors)\n",
    "\n",
    "# Define BiLSTM-CRF Model with Frozen FastText Embeddings\n",
    "class BiLSTMCRF(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_pos_tags, num_ner_tags, fasttext_embeddings):\n",
    "        super(BiLSTMCRF, self).__init__()\n",
    "        # Initialize embedding layer with FastText embeddings (frozen)\n",
    "        self.embedding = nn.Embedding.from_pretrained(fasttext_embeddings, freeze=False)  # Freeze embeddings\n",
    "        self.dropout = nn.Dropout(0.5)  # Add dropout\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.pos_fc = nn.Linear(hidden_dim * 2, num_pos_tags)\n",
    "        self.ner_fc = nn.Linear(hidden_dim * 2, num_ner_tags)\n",
    "        self.pos_crf = CRF(num_pos_tags, batch_first=True)\n",
    "        self.ner_crf = CRF(num_ner_tags, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x)\n",
    "        embeddings = self.dropout(embeddings)  # Apply dropout\n",
    "        lstm_out, _ = self.bilstm(embeddings)\n",
    "        lstm_out = self.dropout(lstm_out)  # Apply dropout\n",
    "        pos_logits = self.pos_fc(lstm_out)\n",
    "        ner_logits = self.ner_fc(lstm_out)\n",
    "        return pos_logits, ner_logits\n",
    "\n",
    "    def compute_loss(self, x, pos_tags, ner_tags, alpha=0.5):\n",
    "        pos_logits, ner_logits = self.forward(x)\n",
    "        pos_loss = -self.pos_crf(pos_logits, pos_tags, mask=(x != vocab[\"<PAD>\"]))\n",
    "        ner_loss = -self.ner_crf(ner_logits, ner_tags, mask=(x != vocab[\"<PAD>\"]))\n",
    "        return alpha * pos_loss + (1 - alpha) * ner_loss\n",
    "\n",
    "    def decode(self, x):\n",
    "        pos_logits, ner_logits = self.forward(x)\n",
    "        pos_tags = self.pos_crf.decode(pos_logits)\n",
    "        ner_tags = self.ner_crf.decode(ner_logits)\n",
    "        return pos_tags, ner_tags\n",
    "\n",
    "# Paths to pre-split datasets\n",
    "train_file_path = \"/kaggle/input/split-fix-data/train_v5.conll\"\n",
    "val_file_path = \"/kaggle/input/split-fix-data/val_v5.conll\"\n",
    "test_file_path = \"/kaggle/input/split-fix-data/test_v5.conll\"\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = CoNLLDataset(train_file_path)\n",
    "val_dataset = CoNLLDataset(val_file_path)\n",
    "test_dataset = CoNLLDataset(test_file_path)\n",
    "\n",
    "# Create vocabulary and tag-to-index mappings\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "pos_tag_to_ix = {\"<PAD>\": 0}  # Add <PAD> to POS tags\n",
    "ner_tag_to_ix = {\"<PAD>\": 0}  # Add <PAD> to NER tags\n",
    "\n",
    "# Build vocab and tag mappings\n",
    "for dataset in [train_dataset, val_dataset, test_dataset]:\n",
    "    for sentence, pos_tags, ner_tags in zip(dataset.sentences, dataset.pos_tags, dataset.ner_tags):\n",
    "        for word in sentence:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "        for pos_tag in pos_tags:\n",
    "            if pos_tag not in pos_tag_to_ix:\n",
    "                pos_tag_to_ix[pos_tag] = len(pos_tag_to_ix)\n",
    "        for ner_tag in ner_tags:\n",
    "            if ner_tag not in ner_tag_to_ix:\n",
    "                ner_tag_to_ix[ner_tag] = len(ner_tag_to_ix)\n",
    "\n",
    "# Create embedding matrix using FastText (dimension: 300)\n",
    "embedding_dim = 300  # FastText embedding dimension\n",
    "fasttext_embeddings = torch.zeros((len(vocab), embedding_dim))  # Initialize with zeros\n",
    "\n",
    "for word, idx in vocab.items():\n",
    "    if word in fasttext_vectors:\n",
    "        fasttext_embeddings[idx] = torch.tensor(fasttext_vectors[word])  # Use full 300 dimensions\n",
    "    elif word == \"<PAD>\":\n",
    "        fasttext_embeddings[idx] = torch.zeros(embedding_dim)  # Zero vector for padding\n",
    "    else:\n",
    "        fasttext_embeddings[idx] = torch.randn(embedding_dim)  # Random vector for unknown words\n",
    "\n",
    "# Initialize model\n",
    "hidden_dim = 128  \n",
    "vocab_size = len(vocab)\n",
    "num_pos_tags = len(pos_tag_to_ix)\n",
    "num_ner_tags = len(ner_tag_to_ix)\n",
    "\n",
    "model = BiLSTMCRF(vocab_size, embedding_dim, hidden_dim, num_pos_tags, num_ner_tags, fasttext_embeddings).to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # Add weight decay for L2 regularization\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Training loop with early stopping\n",
    "def train_model(model, train_loader, val_loader, test_loader, epochs):\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for sentences, pos_tags, ner_tags in train_loader:\n",
    "            sentences, pos_tags, ner_tags = sentences.to(\"cuda\"), pos_tags.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.compute_loss(sentences, pos_tags, ner_tags, alpha=0.5)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for sentences, pos_tags, ner_tags in val_loader:\n",
    "                sentences, pos_tags, ner_tags = sentences.to(\"cuda\"), pos_tags.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "                val_loss += model.compute_loss(sentences, pos_tags, ner_tags).item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    # Evaluate on test set after training\n",
    "    model.eval()\n",
    "    all_pos_preds, all_pos_targets, all_ner_preds, all_ner_targets = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for sentences, pos_tags, ner_tags in test_loader:\n",
    "            sentences, pos_tags, ner_tags = sentences.to(\"cuda\"), pos_tags.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "            pos_preds, ner_preds = model.decode(sentences)\n",
    "\n",
    "            # Flatten the predictions and targets, excluding <PAD> tokens\n",
    "            for i in range(len(sentences)):\n",
    "                sentence_length = (sentences[i] != vocab[\"<PAD>\"]).sum().item()  # Length of the actual sentence\n",
    "                all_pos_preds.extend(pos_preds[i][:sentence_length])  # Truncate predictions to sentence length\n",
    "                all_pos_targets.extend(pos_tags[i][:sentence_length].cpu().numpy())  # Truncate targets to sentence length\n",
    "                all_ner_preds.extend(ner_preds[i][:sentence_length])  # Truncate predictions to sentence length\n",
    "                all_ner_targets.extend(ner_tags[i][:sentence_length].cpu().numpy())  # Truncate targets to sentence length\n",
    "\n",
    "    # Convert predictions and targets to tag names\n",
    "    idx_to_pos = {v: k for k, v in pos_tag_to_ix.items()}\n",
    "    idx_to_ner = {v: k for k, v in ner_tag_to_ix.items()}\n",
    "\n",
    "    # Filter out padding tokens from predictions and targets\n",
    "    all_pos_preds_filtered = [idx_to_pos[idx] for idx in all_pos_preds]\n",
    "    all_pos_targets_filtered = [idx_to_pos[idx] for idx in all_pos_targets]\n",
    "    all_ner_preds_filtered = [idx_to_ner[idx] for idx in all_ner_preds]\n",
    "    all_ner_targets_filtered = [idx_to_ner[idx] for idx in all_ner_targets]\n",
    "\n",
    "    # Generate classification reports\n",
    "    print(\"POS Classification Report:\")\n",
    "    print(classification_report(all_pos_targets_filtered, all_pos_preds_filtered, zero_division=0))\n",
    "\n",
    "    print(\"NER Classification Report:\")\n",
    "    print(classification_report(all_ner_targets_filtered, all_ner_preds_filtered, zero_division=0))\n",
    "\n",
    "# Train the model and evaluate on the test set\n",
    "train_model(model, train_loader, val_loader, test_loader, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T04:47:30.669746Z",
     "iopub.status.busy": "2025-01-27T04:47:30.669407Z",
     "iopub.status.idle": "2025-01-27T04:52:06.169986Z",
     "shell.execute_reply": "2025-01-27T04:52:06.169095Z",
     "shell.execute_reply.started": "2025-01-27T04:47:30.669720Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 341179 words from FastText binary model.\n",
      "Epoch 1/20: Train Loss = 141955.4076, Val Loss = 5151.7805\n",
      "Epoch 2/20: Train Loss = 31547.4353, Val Loss = 3060.4965\n",
      "Epoch 3/20: Train Loss = 19098.5350, Val Loss = 2635.6923\n",
      "Epoch 4/20: Train Loss = 13992.9443, Val Loss = 2305.3816\n",
      "Epoch 5/20: Train Loss = 11195.3423, Val Loss = 2190.1235\n",
      "Epoch 6/20: Train Loss = 9333.1118, Val Loss = 2170.8830\n",
      "Epoch 7/20: Train Loss = 7912.4088, Val Loss = 2151.3330\n",
      "Epoch 8/20: Train Loss = 6958.7443, Val Loss = 2123.2058\n",
      "Epoch 9/20: Train Loss = 6163.5799, Val Loss = 2221.5022\n",
      "Epoch 10/20: Train Loss = 5518.7026, Val Loss = 2302.8981\n",
      "Epoch 11/20: Train Loss = 4931.7257, Val Loss = 2208.8004\n",
      "Early stopping triggered!\n",
      "Total training time: 257.90 seconds\n",
      "POS Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         abb     1.0000    0.9444    0.9714        18\n",
      "         adj     0.8950    0.8840    0.8895       569\n",
      "         adv     0.9866    0.8258    0.8991       356\n",
      "        conj     0.9525    0.9499    0.9512       739\n",
      "          fw     0.8824    0.4615    0.6061        65\n",
      "         int     0.9412    0.9412    0.9412        17\n",
      "           n     0.9707    0.9906    0.9806      7694\n",
      "         num     0.9984    0.9454    0.9712       641\n",
      "        part     0.9854    0.9803    0.9828      4461\n",
      "         ppm     0.9915    0.9961    0.9938      4114\n",
      "        pron     0.9540    0.9764    0.9651       467\n",
      "        punc     1.0000    1.0000    1.0000      2919\n",
      "          sb     1.0000    1.0000    1.0000        13\n",
      "          tn     0.9576    0.9405    0.9489       168\n",
      "           v     0.9698    0.9612    0.9655      3302\n",
      "\n",
      "    accuracy                         0.9780     25543\n",
      "   macro avg     0.9657    0.9198    0.9378     25543\n",
      "weighted avg     0.9779    0.9780    0.9776     25543\n",
      "\n",
      "NER Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-DATE     0.8548    0.8030    0.8281        66\n",
      "       B-LOC     0.9771    0.9755    0.9763      1182\n",
      "       B-NUM     1.0000    0.2667    0.4211        15\n",
      "       B-ORG     0.7429    0.5417    0.6265        48\n",
      "       B-PER     0.8889    0.9412    0.9143        34\n",
      "      B-TIME     0.8889    0.8889    0.8889         9\n",
      "      E-DATE     0.8889    0.8485    0.8682        66\n",
      "       E-LOC     0.9820    0.9712    0.9766      1182\n",
      "       E-NUM     1.0000    0.2667    0.4211        15\n",
      "       E-ORG     0.6667    0.5417    0.5977        48\n",
      "       E-PER     0.8857    0.9118    0.8986        34\n",
      "      E-TIME     0.8889    0.8889    0.8889         9\n",
      "      I-DATE     0.7209    0.8158    0.7654        38\n",
      "       I-LOC     0.9740    0.9682    0.9711       503\n",
      "       I-ORG     0.6000    0.4615    0.5217        39\n",
      "           O     0.9842    0.9953    0.9897     21324\n",
      "      S-DATE     1.0000    0.8409    0.9136        88\n",
      "       S-LOC     0.7895    0.6048    0.6849       124\n",
      "       S-NUM     0.9592    0.9411    0.9501       475\n",
      "       S-ORG     0.7692    0.4762    0.5882        21\n",
      "       S-PER     0.8807    0.4305    0.5783       223\n",
      "      S-TIME     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.9791     25543\n",
      "   macro avg     0.8338    0.6991    0.7395     25543\n",
      "weighted avg     0.9782    0.9791    0.9776     25543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#embedding300/hiddendim256/batch64 (Best REsults_joint_model)\n",
    "\n",
    "# Load FastText binary model\n",
    "fasttext_bin_file = \"/kaggle/input/glove-100d/cc.my.300.bin\"  \n",
    "fasttext_model = load_facebook_model(fasttext_bin_file)\n",
    "\n",
    "# Extract the word vectors\n",
    "fasttext_vectors = fasttext_model.wv\n",
    "print(f\"Loaded {len(fasttext_vectors)} words from FastText binary model.\")\n",
    "\n",
    "# Define Dataset Class\n",
    "class CoNLLDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.sentences, self.pos_tags, self.ner_tags = self.load_data(file_path)\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        sentences, pos_tags, ner_tags = [], [], []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            sentence, pos_tag, ner_tag = [], [], []\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    word, pos, ner = line.strip().split(\"\\t\")\n",
    "                    sentence.append(word)\n",
    "                    pos_tag.append(pos)\n",
    "                    ner_tag.append(ner)\n",
    "                else:\n",
    "                    if sentence:\n",
    "                        sentences.append(sentence)\n",
    "                        pos_tags.append(pos_tag)\n",
    "                        ner_tags.append(ner_tag)\n",
    "                    sentence, pos_tag, ner_tag = [], [], []\n",
    "            if sentence:\n",
    "                sentences.append(sentence)\n",
    "                pos_tags.append(pos_tag)\n",
    "                ner_tags.append(ner_tag)\n",
    "        return sentences, pos_tags, ner_tags\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.pos_tags[idx], self.ner_tags[idx]\n",
    "\n",
    "# Collate function for dynamic padding\n",
    "def collate_fn(batch):\n",
    "    sentences, pos_tags, ner_tags = zip(*batch)\n",
    "    max_len = max(len(s) for s in sentences)\n",
    "\n",
    "    sentence_tensors = []\n",
    "    pos_tensors = []\n",
    "    ner_tensors = []\n",
    "\n",
    "    for s, p, n in zip(sentences, pos_tags, ner_tags):\n",
    "        padded_sentence = s + [\"<PAD>\"] * (max_len - len(s))\n",
    "        padded_pos = p + [\"<PAD>\"] * (max_len - len(p))\n",
    "        padded_ner = n + [\"<PAD>\"] * (max_len - len(n))\n",
    "\n",
    "        sentence_tensors.append(torch.tensor([vocab.get(word, vocab[\"<UNK>\"]) for word in padded_sentence], dtype=torch.long))\n",
    "        pos_tensors.append(torch.tensor([pos_tag_to_ix[tag] for tag in padded_pos], dtype=torch.long))\n",
    "        ner_tensors.append(torch.tensor([ner_tag_to_ix[tag] for tag in padded_ner], dtype=torch.long))\n",
    "\n",
    "    return torch.stack(sentence_tensors), torch.stack(pos_tensors), torch.stack(ner_tensors)\n",
    "\n",
    "# Define BiLSTM-CRF Model with Frozen FastText Embeddings\n",
    "class BiLSTMCRF(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_pos_tags, num_ner_tags, fasttext_embeddings):\n",
    "        super(BiLSTMCRF, self).__init__()\n",
    "        # Initialize embedding layer with FastText embeddings (frozen)\n",
    "        self.embedding = nn.Embedding.from_pretrained(fasttext_embeddings, freeze=False)  # Freeze embeddings\n",
    "        self.dropout = nn.Dropout(0.5)  # Add dropout\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.pos_fc = nn.Linear(hidden_dim * 2, num_pos_tags)\n",
    "        self.ner_fc = nn.Linear(hidden_dim * 2, num_ner_tags)\n",
    "        self.pos_crf = CRF(num_pos_tags, batch_first=True)\n",
    "        self.ner_crf = CRF(num_ner_tags, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x)\n",
    "        embeddings = self.dropout(embeddings)  # Apply dropout\n",
    "        lstm_out, _ = self.bilstm(embeddings)\n",
    "        lstm_out = self.dropout(lstm_out)  # Apply dropout\n",
    "        pos_logits = self.pos_fc(lstm_out)\n",
    "        ner_logits = self.ner_fc(lstm_out)\n",
    "        return pos_logits, ner_logits\n",
    "\n",
    "    def compute_loss(self, x, pos_tags, ner_tags, alpha=0.5):\n",
    "        pos_logits, ner_logits = self.forward(x)\n",
    "        pos_loss = -self.pos_crf(pos_logits, pos_tags, mask=(x != vocab[\"<PAD>\"]))\n",
    "        ner_loss = -self.ner_crf(ner_logits, ner_tags, mask=(x != vocab[\"<PAD>\"]))\n",
    "        return alpha * pos_loss + (1 - alpha) * ner_loss\n",
    "\n",
    "    def decode(self, x):\n",
    "        pos_logits, ner_logits = self.forward(x)\n",
    "        pos_tags = self.pos_crf.decode(pos_logits)\n",
    "        ner_tags = self.ner_crf.decode(ner_logits)\n",
    "        return pos_tags, ner_tags\n",
    "\n",
    "# Paths to pre-split datasets\n",
    "train_file_path = \"/kaggle/input/split-fix-data/train_v5.conll\"\n",
    "val_file_path = \"/kaggle/input/split-fix-data/val_v5.conll\"\n",
    "test_file_path = \"/kaggle/input/split-fix-data/test_v5.conll\"\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = CoNLLDataset(train_file_path)\n",
    "val_dataset = CoNLLDataset(val_file_path)\n",
    "test_dataset = CoNLLDataset(test_file_path)\n",
    "\n",
    "# Create vocabulary and tag-to-index mappings\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "pos_tag_to_ix = {\"<PAD>\": 0}  # Add <PAD> to POS tags\n",
    "ner_tag_to_ix = {\"<PAD>\": 0}  # Add <PAD> to NER tags\n",
    "\n",
    "# Build vocab and tag mappings\n",
    "for dataset in [train_dataset, val_dataset, test_dataset]:\n",
    "    for sentence, pos_tags, ner_tags in zip(dataset.sentences, dataset.pos_tags, dataset.ner_tags):\n",
    "        for word in sentence:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "        for pos_tag in pos_tags:\n",
    "            if pos_tag not in pos_tag_to_ix:\n",
    "                pos_tag_to_ix[pos_tag] = len(pos_tag_to_ix)\n",
    "        for ner_tag in ner_tags:\n",
    "            if ner_tag not in ner_tag_to_ix:\n",
    "                ner_tag_to_ix[ner_tag] = len(ner_tag_to_ix)\n",
    "\n",
    "# Create embedding matrix using FastText (dimension: 300)\n",
    "embedding_dim = 300  # FastText embedding dimension\n",
    "fasttext_embeddings = torch.zeros((len(vocab), embedding_dim))  # Initialize with zeros\n",
    "\n",
    "for word, idx in vocab.items():\n",
    "    if word in fasttext_vectors:\n",
    "        fasttext_embeddings[idx] = torch.tensor(fasttext_vectors[word])  # Use full 300 dimensions\n",
    "    elif word == \"<PAD>\":\n",
    "        fasttext_embeddings[idx] = torch.zeros(embedding_dim)  # Zero vector for padding\n",
    "    else:\n",
    "        fasttext_embeddings[idx] = torch.randn(embedding_dim)  # Random vector for unknown words\n",
    "\n",
    "# Initialize model\n",
    "hidden_dim = 256\n",
    "vocab_size = len(vocab)\n",
    "num_pos_tags = len(pos_tag_to_ix)\n",
    "num_ner_tags = len(ner_tag_to_ix)\n",
    "\n",
    "model = BiLSTMCRF(vocab_size, embedding_dim, hidden_dim, num_pos_tags, num_ner_tags, fasttext_embeddings).to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # Add weight decay for L2 regularization\n",
    "\n",
    "# Create data loaders with batch size 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Training loop with early stopping and training time measurement\n",
    "def train_model(model, train_loader, val_loader, test_loader, epochs):\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 3\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    start_time = time.time() \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for sentences, pos_tags, ner_tags in train_loader:\n",
    "            sentences, pos_tags, ner_tags = sentences.to(\"cuda\"), pos_tags.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.compute_loss(sentences, pos_tags, ner_tags, alpha=0.5)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for sentences, pos_tags, ner_tags in val_loader:\n",
    "                sentences, pos_tags, ner_tags = sentences.to(\"cuda\"), pos_tags.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "                val_loss += model.compute_loss(sentences, pos_tags, ner_tags).item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    end_time = time.time()  # End measuring training time\n",
    "    training_time = end_time - start_time  # Calculate total training time\n",
    "    print(f\"Total training time: {training_time:.2f} seconds\")\n",
    "\n",
    "    # Evaluate on test set after training\n",
    "    model.eval()\n",
    "    all_pos_preds, all_pos_targets, all_ner_preds, all_ner_targets = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for sentences, pos_tags, ner_tags in test_loader:\n",
    "            sentences, pos_tags, ner_tags = sentences.to(\"cuda\"), pos_tags.to(\"cuda\"), ner_tags.to(\"cuda\")\n",
    "            pos_preds, ner_preds = model.decode(sentences)\n",
    "\n",
    "            # Flatten the predictions and targets, excluding <PAD> tokens\n",
    "            for i in range(len(sentences)):\n",
    "                sentence_length = (sentences[i] != vocab[\"<PAD>\"]).sum().item()  # Length of the actual sentence\n",
    "                all_pos_preds.extend(pos_preds[i][:sentence_length])  # Truncate predictions to sentence length\n",
    "                all_pos_targets.extend(pos_tags[i][:sentence_length].cpu().numpy())  # Truncate targets to sentence length\n",
    "                all_ner_preds.extend(ner_preds[i][:sentence_length])  # Truncate predictions to sentence length\n",
    "                all_ner_targets.extend(ner_tags[i][:sentence_length].cpu().numpy())  # Truncate targets to sentence length\n",
    "\n",
    "    # Convert predictions and targets to tag names\n",
    "    idx_to_pos = {v: k for k, v in pos_tag_to_ix.items()}\n",
    "    idx_to_ner = {v: k for k, v in ner_tag_to_ix.items()}\n",
    "\n",
    "    # Filter out padding tokens from predictions and targets\n",
    "    all_pos_preds_filtered = [idx_to_pos[idx] for idx in all_pos_preds]\n",
    "    all_pos_targets_filtered = [idx_to_pos[idx] for idx in all_pos_targets]\n",
    "    all_ner_preds_filtered = [idx_to_ner[idx] for idx in all_ner_preds]\n",
    "    all_ner_targets_filtered = [idx_to_ner[idx] for idx in all_ner_targets]\n",
    "\n",
    "    # Generate classification reports with 4-digit precision\n",
    "    print(\"POS Classification Report:\")\n",
    "    print(classification_report(all_pos_targets_filtered, all_pos_preds_filtered, zero_division=0, digits=4))\n",
    "\n",
    "    print(\"NER Classification Report:\")\n",
    "    print(classification_report(all_ner_targets_filtered, all_ner_preds_filtered, zero_division=0, digits=4))\n",
    "\n",
    "# Train the model and evaluate on the test set\n",
    "train_model(model, train_loader, val_loader, test_loader, epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6515326,
     "sourceId": 10566039,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6521472,
     "sourceId": 10583082,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
